{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = {}\n",
    "id_dict = {}\n",
    "word_count_dict = {}\n",
    "country_set = set()\n",
    "\n",
    "# pulls text IDs, country codes, and document types from the excel sheet,\n",
    "# using it to divide the documents into a dictionary by ID\n",
    "sources_df = pd.read_excel(\"./text/sampleSources.xlsx\", sheet_name=\"texts\")\n",
    "for text_id, (country_code, doc_type), word_count in [(l[0], tuple(l[1].split()), l[2]) for l in sources_df[[\"textID\", \"country|genre\", \"# words\"]].values.tolist()]:\n",
    "    with open(f\"./text/w_{country_code.lower()}_{doc_type.lower()}.txt\", 'r',\n",
    "              encoding=\"utf-8\") as file:\n",
    "        # add each text_id to id_dict\n",
    "        if f\"{country_code}_{doc_type}\" not in id_dict:\n",
    "            id_dict[f\"{country_code}_{doc_type}\"] = [text_id]\n",
    "        else:\n",
    "            id_dict[f\"{country_code}_{doc_type}\"].append(text_id)\n",
    "        # makes country code set\n",
    "        country_set.add(country_code)\n",
    "        # finds correct text_id and adds every line in the document to the dictionary\n",
    "        IS_DOC = False\n",
    "        lines = file.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip().startswith(f\"##{text_id}\"):\n",
    "                IS_DOC = True\n",
    "            elif line.strip().startswith(\"##\"):\n",
    "                IS_DOC = False\n",
    "            if IS_DOC:\n",
    "                if text_id not in doc_dict:\n",
    "                    doc_dict[text_id] = [w.lower() for w in line.split()]\n",
    "                else:\n",
    "                    doc_dict[text_id] += [w.lower() for w in line.split()]\n",
    "        # adds word count to dictionary\n",
    "        word_count_dict[text_id] = word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a counter for every word in the corpus\n",
    "vocab = Counter({})\n",
    "vocab['<UNK>'] = 0\n",
    "for doc in doc_dict.values():\n",
    "    for word in doc:\n",
    "        if word in vocab:\n",
    "            vocab[word] += 1\n",
    "        else:\n",
    "            vocab[word] = 1\n",
    "\n",
    "# make a dictionary of sets for every country and record every word used by each country\n",
    "# also make a word count for every country\n",
    "vocab_sets = {country_code:set() for country_code in country_set}\n",
    "country_word_counts = Counter({country_code:0 for country_code in country_set})\n",
    "for country_code in country_set:\n",
    "    for text_id in id_dict[f\"{country_code}_B\"] + id_dict[f\"{country_code}_G\"]:\n",
    "        for word in doc_dict[text_id]:\n",
    "            vocab_sets[country_code].add(word)\n",
    "        country_word_counts[country_code] += word_count_dict[text_id]\n",
    "\n",
    "# make new vocabulary sets, removing words that appear in less than\n",
    "# 12.5%, 25%, 37.5%, and 50% of the countries datasets respectively\n",
    "vocab_125 = vocab.copy()\n",
    "vocab_25 = vocab.copy()\n",
    "vocab_375 = vocab.copy()\n",
    "vocab_50 = vocab.copy()\n",
    "for word in vocab:\n",
    "    COUNTRY_COUNT = 0\n",
    "    for country_code in country_set:\n",
    "        if word in vocab_sets[country_code]:\n",
    "            COUNTRY_COUNT+=1\n",
    "    if COUNTRY_COUNT / len(country_set) < 0.125:\n",
    "        del vocab_125[word]\n",
    "    if COUNTRY_COUNT / len(country_set) < 0.25:\n",
    "        del vocab_25[word]\n",
    "    if COUNTRY_COUNT / len(country_set) < 0.375:\n",
    "        del vocab_375[word]\n",
    "    if COUNTRY_COUNT / len(country_set) < 0.5:\n",
    "        del vocab_50[word]\n",
    "\n",
    "# Replace any words that appear in less than 12.5% of the countries’ datasets with the <UNK> token\n",
    "doc_dict_125 = copy.deepcopy(doc_dict)\n",
    "for text_id, doc in doc_dict_125.items():\n",
    "    for i, word in enumerate(doc):\n",
    "        if word not in vocab_125:\n",
    "            doc_dict_125[text_id][i] = '<UNK>'\n",
    "            vocab_125['<UNK>'] += 1\n",
    "\n",
    "# Replace any words that appear in less than 25% of the countries’ datasets with the <UNK> token\n",
    "doc_dict_25 = copy.deepcopy(doc_dict)\n",
    "for text_id, doc in doc_dict_25.items():\n",
    "    for i, word in enumerate(doc):\n",
    "        if word not in vocab_25:\n",
    "            doc_dict_25[text_id][i] = '<UNK>'\n",
    "            vocab_25['<UNK>'] += 1\n",
    "\n",
    "# Replace any words that appear in less than 37.5% of the countries’ datasets with the <UNK> token\n",
    "doc_dict_375 = copy.deepcopy(doc_dict)\n",
    "for text_id, doc in doc_dict_375.items():\n",
    "    for i, word in enumerate(doc):\n",
    "        if word not in vocab_375:\n",
    "            doc_dict_375[text_id][i] = '<UNK>'\n",
    "            vocab_375['<UNK>'] += 1\n",
    "\n",
    "# Replace any words that appear in less than 50% of the countries’ datasets with the <UNK> token\n",
    "doc_dict_50 = copy.deepcopy(doc_dict)\n",
    "for text_id, doc in doc_dict_50.items():\n",
    "    for i, word in enumerate(doc):\n",
    "        if word not in vocab_50:\n",
    "            doc_dict_50[text_id][i] = '<UNK>'\n",
    "            vocab_50['<UNK>'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# make a dataframe of one-hot representations of each text in each version of the dataset with their country labels\n",
    "text_ids = []\n",
    "texts = []\n",
    "texts_125 = []\n",
    "texts_25 = []\n",
    "texts_375 = []\n",
    "texts_50 = []\n",
    "country_labels = []\n",
    "\n",
    "for country_code in country_set:\n",
    "    for text_id in id_dict[f\"{country_code}_B\"] + id_dict[f\"{country_code}_G\"]:\n",
    "        text_ids.append(text_ids)\n",
    "        texts.append(\" \".join(doc_dict[text_id]))\n",
    "        texts_125.append(\" \".join(doc_dict_125[text_id]))\n",
    "        texts_25.append(\" \".join(doc_dict_25[text_id]))\n",
    "        texts_375.append(\" \".join(doc_dict_375[text_id]))\n",
    "        texts_50.append(\" \".join(doc_dict_50[text_id]))\n",
    "        country_labels.append(country_code)\n",
    "\n",
    "data = {\n",
    "    'text_id': text_ids,\n",
    "    'texts': texts,\n",
    "    'texts_125': texts_125,\n",
    "    'texts_25': texts_25,\n",
    "    'texts_375': texts_375,\n",
    "    'texts_50': texts_50,\n",
    "    'country_labels': country_labels\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['texts'])\n",
    "X_125 = vectorizer.fit_transform(df['texts_125'])\n",
    "X_25 = vectorizer.fit_transform(df['texts_25'])\n",
    "X_375 = vectorizer.fit_transform(df['texts_375'])\n",
    "X_50 = vectorizer.fit_transform(df['texts_50'])\n",
    "y = df['country_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({'GB': 310, 'US': 309, 'AU': 113, 'CA': 107, 'IE': 82, 'IN': 75, 'NZ': 65, 'PK': 40, 'LK': 36, 'MY': 34, 'PH': 34, 'KE': 33, 'NG': 33, 'ZA': 33, 'JM': 31, 'SG': 31, 'GH': 31, 'HK': 31, 'BD': 31, 'TZ': 26})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the dataset into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=3)\n",
    "print(\"Original class distribution:\", Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(rf_model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][rf_model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][rf_model.classes_[i]] += 1     \n",
    "\n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28\n",
      "LK (4): PK 0.210, GB 0.169, CA 0.134, LK 0.098, HK 0.072, NZ 0.068, US 0.038, GH 0.032, NG 0.030, IE 0.030, AU 0.025, JM 0.025, IN 0.015, TZ 0.010, ZA 0.009, MY 0.008, KE 0.008, BD 0.007, PH 0.007, SG 0.006, \n",
      "ZA (6): AU 0.214, CA 0.144, NG 0.090, GB 0.086, US 0.075, ZA 0.071, GH 0.069, IE 0.045, NZ 0.037, TZ 0.036, HK 0.032, IN 0.020, KE 0.018, JM 0.012, PH 0.011, LK 0.011, MY 0.009, PK 0.008, SG 0.007, BD 0.004, \n",
      "AU (1): AU 0.219, GB 0.168, US 0.146, IE 0.091, IN 0.081, CA 0.057, ZA 0.039, JM 0.037, PH 0.032, NZ 0.023, NG 0.019, HK 0.014, LK 0.013, MY 0.013, TZ 0.011, SG 0.009, BD 0.008, GH 0.008, PK 0.006, KE 0.005, \n",
      "PH (10): AU 0.227, US 0.213, IE 0.141, GB 0.131, CA 0.107, LK 0.036, NG 0.026, IN 0.019, NZ 0.016, PH 0.016, SG 0.010, GH 0.010, HK 0.009, KE 0.008, TZ 0.006, BD 0.006, JM 0.006, MY 0.006, ZA 0.004, PK 0.004, \n",
      "NZ (8): LK 0.179, AU 0.146, US 0.120, GB 0.116, ZA 0.088, CA 0.083, IN 0.053, NZ 0.046, IE 0.038, BD 0.037, JM 0.018, NG 0.012, GH 0.011, SG 0.010, PH 0.010, KE 0.009, PK 0.008, TZ 0.008, HK 0.005, MY 0.004, \n",
      "KE (2): GB 0.368, KE 0.327, AU 0.098, CA 0.092, IE 0.036, US 0.028, MY 0.015, NZ 0.009, TZ 0.006, GH 0.005, PK 0.004, ZA 0.003, IN 0.002, LK 0.002, NG 0.002, BD 0.002, JM 0.001, HK 0.001, SG 0.001, PH 0.000, \n",
      "IN (1): IN 0.214, US 0.168, NZ 0.098, CA 0.094, AU 0.085, GB 0.078, TZ 0.062, IE 0.053, LK 0.039, KE 0.017, PK 0.013, PH 0.013, SG 0.011, NG 0.010, JM 0.010, HK 0.009, MY 0.007, GH 0.007, BD 0.007, ZA 0.006, \n",
      "MY (17): US 0.270, GB 0.203, IN 0.073, AU 0.065, CA 0.052, ZA 0.035, GH 0.035, SG 0.033, IE 0.030, JM 0.026, HK 0.026, NG 0.026, PK 0.026, NZ 0.023, PH 0.018, KE 0.016, MY 0.012, LK 0.011, TZ 0.010, BD 0.008, \n",
      "GH (13): GB 0.239, CA 0.155, AU 0.136, US 0.079, NG 0.073, IN 0.063, PH 0.057, JM 0.041, TZ 0.030, ZA 0.027, KE 0.020, BD 0.015, GH 0.012, LK 0.012, PK 0.009, NZ 0.009, MY 0.008, IE 0.007, SG 0.005, HK 0.003, \n",
      "US (1): US 0.387, GB 0.201, CA 0.073, MY 0.043, AU 0.043, SG 0.041, IE 0.038, NZ 0.038, PH 0.021, IN 0.020, HK 0.013, TZ 0.013, JM 0.012, ZA 0.011, GH 0.011, NG 0.010, PK 0.007, LK 0.007, KE 0.005, BD 0.004, \n",
      "TZ (1): TZ 0.152, US 0.119, GB 0.116, IE 0.084, PH 0.054, GH 0.048, NG 0.048, IN 0.044, JM 0.042, LK 0.041, CA 0.039, NZ 0.039, AU 0.036, ZA 0.033, HK 0.030, SG 0.029, MY 0.022, KE 0.014, BD 0.006, PK 0.005, \n",
      "BD (1): BD 0.329, GB 0.245, IE 0.227, PK 0.112, IN 0.038, US 0.015, SG 0.010, CA 0.006, PH 0.004, NZ 0.003, LK 0.003, NG 0.002, MY 0.002, ZA 0.001, GH 0.001, KE 0.001, AU 0.001, TZ 0.000, JM 0.000, HK 0.000, \n",
      "JM (13): GB 0.388, US 0.137, CA 0.075, IE 0.052, PH 0.044, IN 0.037, ZA 0.034, NZ 0.034, AU 0.033, LK 0.027, SG 0.027, GH 0.021, JM 0.017, PK 0.015, KE 0.013, TZ 0.013, NG 0.010, HK 0.009, MY 0.008, BD 0.006, \n",
      "HK (5): GB 0.263, MY 0.220, US 0.146, NZ 0.100, HK 0.046, AU 0.033, IE 0.032, CA 0.030, IN 0.021, SG 0.020, ZA 0.015, NG 0.012, GH 0.012, PH 0.010, PK 0.009, KE 0.007, JM 0.007, TZ 0.006, BD 0.005, LK 0.005, \n",
      "PK (6): US 0.422, PH 0.194, AU 0.170, GB 0.038, JM 0.030, PK 0.027, MY 0.024, IE 0.020, IN 0.012, LK 0.012, ZA 0.010, GH 0.009, KE 0.007, CA 0.005, TZ 0.004, NG 0.004, NZ 0.004, SG 0.003, BD 0.003, HK 0.001, \n",
      "CA (8): IE 0.213, GB 0.200, US 0.185, IN 0.173, KE 0.091, AU 0.025, MY 0.023, CA 0.018, JM 0.010, ZA 0.009, NZ 0.007, SG 0.007, LK 0.006, PH 0.006, NG 0.005, BD 0.005, GH 0.005, HK 0.004, PK 0.004, TZ 0.003, \n",
      "GB (1): GB 0.442, US 0.193, CA 0.062, AU 0.053, SG 0.044, PK 0.040, IE 0.034, NZ 0.027, IN 0.021, ZA 0.013, PH 0.013, NG 0.008, JM 0.008, MY 0.007, HK 0.007, GH 0.006, LK 0.006, TZ 0.005, KE 0.005, BD 0.004, \n",
      "NG (2): GB 0.300, NG 0.276, AU 0.099, IE 0.079, US 0.061, PK 0.027, HK 0.027, CA 0.024, NZ 0.019, LK 0.018, PH 0.016, IN 0.012, MY 0.009, JM 0.007, GH 0.006, TZ 0.005, KE 0.005, BD 0.004, ZA 0.003, SG 0.002, \n",
      "IE (1): IE 0.307, PH 0.213, US 0.159, HK 0.145, AU 0.061, GB 0.040, JM 0.016, NG 0.013, CA 0.011, LK 0.006, IN 0.004, KE 0.004, PK 0.004, ZA 0.003, TZ 0.003, MY 0.003, NZ 0.003, SG 0.002, GH 0.001, BD 0.001, \n",
      "SG (7): US 0.327, IN 0.108, HK 0.094, CA 0.069, IE 0.068, GB 0.057, SG 0.036, TZ 0.034, LK 0.029, MY 0.028, ZA 0.027, NZ 0.026, NG 0.024, AU 0.018, PH 0.016, JM 0.012, GH 0.010, KE 0.009, PK 0.005, BD 0.003, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression model with class weights balanced\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=500, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][model.classes_[i]] += 1     \n",
    "\n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.29\n",
      "LK (3): US 0.232, GB 0.182, LK 0.136, AU 0.070, PK 0.052, CA 0.046, IN 0.040, HK 0.034, IE 0.030, BD 0.022, TZ 0.022, NZ 0.020, MY 0.020, JM 0.020, PH 0.016, NG 0.016, SG 0.016, ZA 0.010, KE 0.008, GH 0.008, \n",
      "ZA (5): US 0.213, GB 0.209, AU 0.084, CA 0.066, ZA 0.057, NZ 0.057, IE 0.041, KE 0.031, NG 0.031, IN 0.027, TZ 0.026, GH 0.024, HK 0.020, PK 0.020, SG 0.020, LK 0.019, JM 0.019, PH 0.013, BD 0.013, MY 0.010, \n",
      "AU (3): US 0.243, GB 0.241, AU 0.146, CA 0.057, IE 0.054, IN 0.045, NZ 0.035, LK 0.020, HK 0.018, ZA 0.017, MY 0.017, SG 0.016, GH 0.016, KE 0.015, PK 0.014, JM 0.012, PH 0.011, BD 0.009, NG 0.008, TZ 0.005, \n",
      "PH (8): GB 0.270, US 0.268, AU 0.095, NZ 0.053, CA 0.045, IN 0.043, IE 0.037, PH 0.035, HK 0.027, TZ 0.015, PK 0.015, NG 0.015, LK 0.013, ZA 0.013, KE 0.013, JM 0.013, SG 0.013, GH 0.010, MY 0.005, BD 0.005, \n",
      "NZ (4): US 0.203, GB 0.183, AU 0.167, NZ 0.148, CA 0.050, HK 0.047, IN 0.040, IE 0.027, TZ 0.023, MY 0.015, PK 0.012, SG 0.012, PH 0.010, GH 0.010, BD 0.010, JM 0.010, NG 0.010, LK 0.008, KE 0.008, ZA 0.007, \n",
      "KE (5): US 0.247, GB 0.227, AU 0.103, CA 0.073, KE 0.050, IN 0.037, ZA 0.033, PH 0.030, IE 0.030, NG 0.027, NZ 0.023, MY 0.023, JM 0.020, LK 0.017, HK 0.017, TZ 0.013, SG 0.013, BD 0.010, GH 0.003, PK 0.003, \n",
      "IN (3): US 0.263, GB 0.253, IN 0.109, CA 0.058, AU 0.050, IE 0.031, NZ 0.027, ZA 0.022, HK 0.020, PH 0.019, KE 0.018, PK 0.018, SG 0.018, LK 0.017, MY 0.016, BD 0.014, GH 0.014, JM 0.013, TZ 0.011, NG 0.009, \n",
      "MY (18): US 0.373, GB 0.320, CA 0.053, AU 0.050, IN 0.043, NZ 0.027, IE 0.023, LK 0.013, PH 0.013, KE 0.013, NG 0.013, ZA 0.010, TZ 0.010, JM 0.010, BD 0.007, HK 0.007, SG 0.007, MY 0.003, PK 0.003, GH 0.000, \n",
      "GH (5): US 0.250, GB 0.210, CA 0.077, IN 0.070, GH 0.057, NZ 0.043, ZA 0.037, AU 0.037, KE 0.033, IE 0.033, LK 0.030, HK 0.030, NG 0.027, BD 0.017, PK 0.017, MY 0.013, PH 0.010, JM 0.007, SG 0.003, TZ 0.000, \n",
      "US (1): US 0.287, GB 0.269, CA 0.080, AU 0.059, IE 0.040, IN 0.037, NZ 0.033, PK 0.021, JM 0.019, MY 0.018, KE 0.017, BD 0.017, PH 0.016, ZA 0.016, LK 0.013, NG 0.012, SG 0.012, GH 0.012, HK 0.012, TZ 0.010, \n",
      "TZ (6): US 0.274, GB 0.256, AU 0.090, CA 0.054, GH 0.046, TZ 0.046, ZA 0.042, IE 0.034, KE 0.026, IN 0.024, PH 0.020, MY 0.020, JM 0.020, NZ 0.014, LK 0.012, PK 0.006, NG 0.006, HK 0.004, SG 0.004, BD 0.002, \n",
      "BD (3): US 0.188, GB 0.180, BD 0.130, CA 0.080, NZ 0.070, IN 0.062, PK 0.048, AU 0.045, IE 0.028, LK 0.025, ZA 0.020, JM 0.020, SG 0.020, MY 0.015, TZ 0.015, PH 0.013, KE 0.013, NG 0.013, HK 0.010, GH 0.007, \n",
      "JM (9): GB 0.283, US 0.240, IE 0.118, CA 0.073, AU 0.053, NZ 0.033, IN 0.028, ZA 0.023, JM 0.022, SG 0.022, PK 0.020, KE 0.015, MY 0.015, PH 0.013, GH 0.013, TZ 0.010, HK 0.010, NG 0.007, LK 0.005, BD 0.000, \n",
      "HK (3): GB 0.237, US 0.225, HK 0.120, AU 0.068, CA 0.060, NZ 0.048, IN 0.033, IE 0.028, SG 0.028, LK 0.025, PH 0.022, ZA 0.020, KE 0.020, BD 0.015, MY 0.015, JM 0.013, GH 0.007, TZ 0.007, PK 0.005, NG 0.005, \n",
      "PK (14): GB 0.294, US 0.266, CA 0.082, AU 0.048, IE 0.038, PH 0.030, NZ 0.028, JM 0.028, IN 0.028, SG 0.020, ZA 0.018, LK 0.018, MY 0.018, PK 0.018, KE 0.016, GH 0.012, TZ 0.012, HK 0.012, BD 0.010, NG 0.004, \n",
      "CA (4): US 0.272, GB 0.243, AU 0.074, CA 0.066, NZ 0.041, IE 0.041, IN 0.032, PK 0.029, KE 0.022, JM 0.022, NG 0.018, SG 0.018, ZA 0.017, PH 0.017, MY 0.017, TZ 0.017, BD 0.014, HK 0.014, LK 0.013, GH 0.013, \n",
      "GB (1): GB 0.306, US 0.276, CA 0.067, AU 0.062, IE 0.059, IN 0.036, NZ 0.031, PK 0.023, KE 0.016, NG 0.016, PH 0.015, SG 0.014, ZA 0.013, BD 0.012, MY 0.011, JM 0.010, HK 0.010, LK 0.009, TZ 0.009, GH 0.006, \n",
      "NG (2): GB 0.227, NG 0.185, US 0.172, AU 0.072, CA 0.062, NZ 0.048, IE 0.037, IN 0.030, HK 0.028, GH 0.023, TZ 0.020, ZA 0.015, KE 0.015, BD 0.013, PK 0.013, PH 0.010, JM 0.010, MY 0.007, SG 0.007, LK 0.005, \n",
      "IE (3): GB 0.188, US 0.173, IE 0.150, CA 0.073, AU 0.070, IN 0.057, NZ 0.033, PK 0.032, ZA 0.023, HK 0.023, KE 0.023, PH 0.022, MY 0.020, SG 0.018, TZ 0.018, BD 0.017, NG 0.017, LK 0.015, JM 0.015, GH 0.012, \n",
      "SG (3): GB 0.243, US 0.199, SG 0.070, CA 0.060, IN 0.056, AU 0.053, MY 0.040, NZ 0.036, IE 0.036, PH 0.034, LK 0.026, HK 0.026, PK 0.021, JM 0.020, TZ 0.017, ZA 0.016, BD 0.014, KE 0.013, GH 0.013, NG 0.009, \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Classifier with class weights balanced\n",
    "rf_model = RandomForestClassifier(random_state=3, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(rf_model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][rf_model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][rf_model.classes_[i]] += 1     \n",
    "\n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.30\n",
      "LK (5): CA 0.473, GB 0.218, US 0.109, AU 0.036, LK 0.023, IE 0.021, HK 0.014, KE 0.014, IN 0.013, MY 0.013, NZ 0.011, NG 0.010, TZ 0.009, ZA 0.008, SG 0.006, PH 0.005, JM 0.005, PK 0.005, BD 0.005, GH 0.004, \n",
      "ZA (10): CA 0.306, AU 0.180, US 0.179, GB 0.154, IE 0.025, KE 0.019, NZ 0.019, IN 0.014, PH 0.014, ZA 0.013, TZ 0.013, HK 0.011, MY 0.010, BD 0.010, JM 0.010, GH 0.006, NG 0.005, SG 0.004, PK 0.004, LK 0.003, \n",
      "AU (1): AU 0.338, US 0.182, GB 0.168, CA 0.111, IE 0.023, NZ 0.018, HK 0.017, KE 0.016, TZ 0.015, JM 0.013, BD 0.013, MY 0.012, PH 0.012, IN 0.012, ZA 0.011, NG 0.009, LK 0.008, SG 0.008, PK 0.008, GH 0.006, \n",
      "PH (11): GB 0.333, CA 0.302, US 0.145, AU 0.075, KE 0.017, IN 0.015, TZ 0.013, NZ 0.012, JM 0.011, NG 0.009, PH 0.009, HK 0.009, BD 0.008, ZA 0.008, IE 0.007, SG 0.007, MY 0.006, PK 0.005, LK 0.004, GH 0.003, \n",
      "NZ (5): GB 0.350, CA 0.239, AU 0.182, US 0.071, NZ 0.061, JM 0.023, IE 0.016, ZA 0.010, HK 0.010, IN 0.006, TZ 0.006, PH 0.005, BD 0.005, LK 0.004, KE 0.003, MY 0.002, PK 0.002, NG 0.002, GH 0.002, SG 0.001, \n",
      "KE (5): GB 0.343, CA 0.292, AU 0.248, US 0.061, KE 0.029, IE 0.008, MY 0.006, NG 0.005, NZ 0.002, BD 0.002, IN 0.001, TZ 0.001, PH 0.001, LK 0.000, ZA 0.000, HK 0.000, PK 0.000, JM 0.000, SG 0.000, GH 0.000, \n",
      "IN (4): US 0.322, CA 0.224, GB 0.128, IN 0.073, AU 0.071, NZ 0.037, HK 0.017, IE 0.014, KE 0.013, MY 0.012, LK 0.011, SG 0.011, TZ 0.010, NG 0.010, ZA 0.009, PH 0.009, JM 0.009, BD 0.008, PK 0.006, GH 0.005, \n",
      "MY (14): US 0.221, GB 0.131, CA 0.095, NZ 0.071, HK 0.071, AU 0.067, IE 0.048, KE 0.048, TZ 0.036, IN 0.033, BD 0.023, LK 0.022, GH 0.022, MY 0.020, ZA 0.020, PH 0.017, JM 0.016, NG 0.015, PK 0.014, SG 0.013, \n",
      "GH (20): GB 0.491, CA 0.206, US 0.097, AU 0.035, IN 0.024, KE 0.020, MY 0.015, HK 0.014, TZ 0.011, NZ 0.011, NG 0.011, JM 0.010, IE 0.009, PK 0.008, ZA 0.007, SG 0.007, PH 0.007, BD 0.006, LK 0.006, GH 0.005, \n",
      "US (1): US 0.508, GB 0.191, CA 0.137, AU 0.045, NZ 0.035, KE 0.012, IN 0.009, HK 0.008, TZ 0.007, PH 0.006, IE 0.006, MY 0.006, JM 0.005, LK 0.005, NG 0.005, PK 0.004, BD 0.004, ZA 0.003, SG 0.003, GH 0.001, \n",
      "TZ (5): US 0.213, AU 0.184, GB 0.153, CA 0.136, TZ 0.078, IE 0.060, NG 0.044, PH 0.019, ZA 0.017, IN 0.015, JM 0.014, NZ 0.011, BD 0.009, KE 0.008, HK 0.008, LK 0.008, GH 0.008, MY 0.007, SG 0.005, PK 0.003, \n",
      "BD (3): US 0.500, GB 0.258, BD 0.149, CA 0.087, AU 0.005, IN 0.000, NZ 0.000, IE 0.000, PH 0.000, TZ 0.000, LK 0.000, PK 0.000, HK 0.000, KE 0.000, ZA 0.000, MY 0.000, GH 0.000, NG 0.000, JM 0.000, SG 0.000, \n",
      "JM (15): GB 0.580, CA 0.099, US 0.070, AU 0.065, ZA 0.027, PH 0.024, IE 0.015, TZ 0.013, IN 0.012, SG 0.012, GH 0.012, NZ 0.011, BD 0.010, KE 0.010, JM 0.009, MY 0.008, LK 0.008, NG 0.006, HK 0.005, PK 0.004, \n",
      "HK (3): US 0.403, GB 0.271, HK 0.082, CA 0.078, AU 0.033, NZ 0.021, IE 0.017, MY 0.015, KE 0.015, TZ 0.010, ZA 0.010, PH 0.007, LK 0.006, JM 0.006, IN 0.006, NG 0.005, BD 0.005, PK 0.004, SG 0.004, GH 0.002, \n",
      "PK (14): CA 0.367, GB 0.307, US 0.197, AU 0.077, JM 0.010, KE 0.009, NZ 0.008, HK 0.005, PH 0.004, MY 0.003, IE 0.002, BD 0.002, IN 0.002, PK 0.001, SG 0.001, TZ 0.001, LK 0.001, NG 0.001, ZA 0.001, GH 0.000, \n",
      "CA (1): CA 0.318, US 0.311, GB 0.169, AU 0.110, IN 0.009, JM 0.008, SG 0.008, KE 0.007, IE 0.006, PH 0.006, HK 0.005, ZA 0.005, TZ 0.005, MY 0.005, NZ 0.005, LK 0.005, BD 0.005, PK 0.004, GH 0.004, NG 0.004, \n",
      "GB (1): GB 0.462, US 0.211, CA 0.130, AU 0.037, IE 0.019, HK 0.014, IN 0.014, NZ 0.013, KE 0.013, TZ 0.010, JM 0.010, PH 0.010, ZA 0.009, MY 0.008, BD 0.008, LK 0.008, NG 0.007, SG 0.006, PK 0.006, GH 0.005, \n",
      "NG (2): CA 0.274, NG 0.251, GB 0.167, IE 0.105, AU 0.081, US 0.077, NZ 0.023, HK 0.008, IN 0.003, KE 0.002, ZA 0.002, JM 0.002, PH 0.002, MY 0.001, TZ 0.001, LK 0.001, BD 0.001, PK 0.000, SG 0.000, GH 0.000, \n",
      "IE (3): US 0.339, GB 0.325, IE 0.146, CA 0.104, HK 0.038, NZ 0.030, AU 0.006, KE 0.004, NG 0.002, MY 0.002, TZ 0.001, JM 0.001, LK 0.001, ZA 0.001, IN 0.001, PK 0.001, BD 0.000, PH 0.000, SG 0.000, GH 0.000, \n",
      "SG (4): GB 0.284, CA 0.228, US 0.217, SG 0.091, IN 0.081, AU 0.043, JM 0.012, LK 0.007, KE 0.006, HK 0.005, ZA 0.004, MY 0.003, PH 0.003, PK 0.003, TZ 0.002, NZ 0.002, NG 0.002, GH 0.002, BD 0.002, IE 0.002, \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Multi-layer Perceptron Model\n",
    "mlp_model = MLPClassifier(random_state=3)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(mlp_model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][mlp_model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][mlp_model.classes_[i]] += 1  \n",
    "   \n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled class distribution: Counter({'AU': 26, 'BD': 26, 'CA': 26, 'GB': 26, 'GH': 26, 'HK': 26, 'IE': 26, 'IN': 26, 'JM': 26, 'KE': 26, 'LK': 26, 'MY': 26, 'NG': 26, 'NZ': 26, 'PH': 26, 'PK': 26, 'SG': 26, 'TZ': 26, 'US': 26, 'ZA': 26})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Even when class weights are balanced, the bigger countries are heavily favored\n",
    "# so we can try undersampling the dataset so everything is equal\n",
    "rus = RandomUnderSampler(random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=3)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "print(\"Resampled class distribution:\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09\n",
      "LK (5): PK 0.212, NZ 0.125, JM 0.085, HK 0.085, LK 0.061, US 0.055, GH 0.050, NG 0.048, SG 0.044, ZA 0.040, MY 0.037, AU 0.035, IN 0.024, TZ 0.022, PH 0.018, CA 0.017, IE 0.015, BD 0.011, KE 0.009, GB 0.009, \n",
      "ZA (4): GB 0.232, IN 0.154, AU 0.154, ZA 0.066, GH 0.052, US 0.041, TZ 0.041, HK 0.039, NZ 0.039, IE 0.036, JM 0.023, NG 0.020, CA 0.018, MY 0.017, SG 0.017, PH 0.016, KE 0.014, LK 0.012, BD 0.004, PK 0.004, \n",
      "AU (8): US 0.117, IE 0.095, JM 0.095, ZA 0.063, NG 0.061, IN 0.053, HK 0.051, AU 0.050, GB 0.043, KE 0.042, TZ 0.041, LK 0.040, NZ 0.039, BD 0.039, GH 0.033, SG 0.032, PH 0.031, MY 0.030, CA 0.022, PK 0.022, \n",
      "PH (4): LK 0.136, IE 0.121, US 0.104, PH 0.100, NG 0.092, CA 0.080, NZ 0.053, AU 0.040, HK 0.033, GH 0.032, GB 0.030, SG 0.030, IN 0.028, JM 0.027, TZ 0.025, MY 0.019, ZA 0.019, KE 0.016, BD 0.011, PK 0.005, \n",
      "NZ (6): LK 0.190, ZA 0.171, AU 0.108, IE 0.074, US 0.072, NZ 0.053, NG 0.046, JM 0.043, SG 0.042, BD 0.035, TZ 0.025, PH 0.022, CA 0.022, MY 0.019, IN 0.017, KE 0.017, HK 0.013, GB 0.012, GH 0.012, PK 0.008, \n",
      "KE (4): AU 0.235, JM 0.173, IE 0.166, KE 0.089, MY 0.059, ZA 0.054, GH 0.038, NZ 0.036, TZ 0.023, BD 0.022, CA 0.021, PK 0.019, US 0.016, GB 0.013, SG 0.010, NG 0.008, IN 0.007, HK 0.005, LK 0.003, PH 0.002, \n",
      "IN (6): NZ 0.152, US 0.110, KE 0.094, LK 0.092, TZ 0.091, IN 0.077, NG 0.043, JM 0.042, SG 0.042, CA 0.038, PK 0.036, AU 0.035, HK 0.029, IE 0.025, ZA 0.024, MY 0.019, PH 0.019, GB 0.014, GH 0.010, BD 0.009, \n",
      "MY (11): US 0.144, SG 0.087, PH 0.087, JM 0.079, IN 0.070, ZA 0.070, AU 0.070, NG 0.064, HK 0.054, CA 0.048, MY 0.030, GB 0.029, IE 0.028, NZ 0.027, GH 0.025, KE 0.022, LK 0.021, TZ 0.019, PK 0.017, BD 0.010, \n",
      "GH (5): TZ 0.253, NG 0.175, ZA 0.084, AU 0.076, GH 0.058, JM 0.057, PH 0.044, GB 0.041, KE 0.036, NZ 0.031, IE 0.024, BD 0.022, MY 0.019, US 0.018, IN 0.015, SG 0.011, LK 0.011, CA 0.009, HK 0.008, PK 0.007, \n",
      "US (1): US 0.093, AU 0.091, MY 0.088, CA 0.068, NZ 0.065, HK 0.061, ZA 0.060, TZ 0.060, SG 0.059, GB 0.048, GH 0.047, JM 0.044, PH 0.044, IE 0.044, IN 0.033, NG 0.030, LK 0.024, KE 0.016, PK 0.014, BD 0.010, \n",
      "TZ (1): TZ 0.174, IE 0.106, JM 0.071, LK 0.065, PH 0.061, ZA 0.060, SG 0.058, NG 0.057, KE 0.053, HK 0.043, IN 0.041, CA 0.039, MY 0.038, GH 0.030, NZ 0.026, AU 0.023, US 0.023, GB 0.018, BD 0.010, PK 0.004, \n",
      "BD (1): BD 0.421, IN 0.237, PK 0.220, PH 0.062, LK 0.014, MY 0.013, SG 0.011, IE 0.009, ZA 0.005, KE 0.002, GH 0.001, TZ 0.001, NZ 0.001, CA 0.001, US 0.001, NG 0.001, GB 0.000, HK 0.000, JM 0.000, AU 0.000, \n",
      "JM (4): PH 0.105, US 0.100, NZ 0.081, JM 0.073, HK 0.072, SG 0.068, IE 0.064, IN 0.061, ZA 0.053, AU 0.050, GB 0.050, TZ 0.037, LK 0.037, GH 0.032, KE 0.027, NG 0.027, CA 0.019, MY 0.018, PK 0.014, BD 0.011, \n",
      "HK (4): MY 0.200, CA 0.099, ZA 0.093, HK 0.092, NZ 0.086, US 0.066, AU 0.062, SG 0.058, NG 0.043, GB 0.038, IE 0.037, JM 0.022, BD 0.017, GH 0.015, TZ 0.015, IN 0.015, PH 0.014, KE 0.011, LK 0.010, PK 0.006, \n",
      "PK (6): PH 0.282, US 0.170, AU 0.082, MY 0.069, JM 0.056, PK 0.051, CA 0.042, ZA 0.036, IE 0.035, HK 0.028, TZ 0.024, SG 0.022, NG 0.019, GB 0.015, IN 0.015, NZ 0.013, KE 0.011, LK 0.011, GH 0.011, BD 0.006, \n",
      "CA (5): PK 0.167, IN 0.141, MY 0.125, US 0.109, CA 0.100, ZA 0.057, HK 0.033, JM 0.032, PH 0.029, LK 0.026, GB 0.025, AU 0.023, SG 0.021, KE 0.020, IE 0.020, NG 0.018, TZ 0.015, BD 0.014, NZ 0.014, GH 0.011, \n",
      "GB (16): US 0.131, CA 0.115, AU 0.093, SG 0.089, PH 0.076, NZ 0.050, PK 0.048, IE 0.048, MY 0.042, JM 0.038, ZA 0.037, NG 0.034, HK 0.033, KE 0.032, TZ 0.031, GB 0.030, IN 0.026, BD 0.016, LK 0.015, GH 0.014, \n",
      "NG (1): NG 0.264, AU 0.151, NZ 0.098, IE 0.086, HK 0.060, PK 0.059, TZ 0.040, JM 0.035, KE 0.034, GH 0.028, US 0.024, PH 0.023, MY 0.021, ZA 0.017, LK 0.017, GB 0.014, BD 0.012, SG 0.008, CA 0.005, IN 0.004, \n",
      "IE (7): PH 0.337, LK 0.207, HK 0.162, AU 0.058, JM 0.055, NG 0.049, IE 0.036, TZ 0.015, BD 0.013, KE 0.012, NZ 0.011, MY 0.008, SG 0.007, US 0.007, ZA 0.006, GH 0.006, PK 0.005, GB 0.003, IN 0.003, CA 0.002, \n",
      "SG (2): IE 0.118, SG 0.114, NZ 0.102, ZA 0.076, US 0.070, TZ 0.063, MY 0.063, NG 0.061, CA 0.046, KE 0.042, PH 0.037, HK 0.037, LK 0.033, GH 0.030, IN 0.028, JM 0.024, GB 0.020, AU 0.015, PK 0.011, BD 0.009, \n"
     ]
    }
   ],
   "source": [
    "# Trains a logistic regression model with undersampling\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][model.classes_[i]] += 1  \n",
    "\n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16\n",
      "LK (1): LK 0.108, US 0.080, PK 0.068, IE 0.064, JM 0.056, CA 0.054, NG 0.052, ZA 0.050, MY 0.050, NZ 0.048, HK 0.044, IN 0.042, TZ 0.042, GB 0.042, SG 0.040, AU 0.036, KE 0.036, PH 0.034, GH 0.028, BD 0.026, \n",
      "ZA (7): IE 0.071, KE 0.069, GH 0.067, TZ 0.061, PH 0.060, NG 0.056, ZA 0.053, NZ 0.053, BD 0.050, JM 0.050, US 0.049, HK 0.047, CA 0.046, IN 0.044, SG 0.041, LK 0.040, GB 0.040, AU 0.037, PK 0.036, MY 0.030, \n",
      "AU (7): US 0.095, NZ 0.062, JM 0.062, ZA 0.057, PH 0.055, SG 0.054, AU 0.054, IE 0.053, GB 0.052, MY 0.048, CA 0.046, HK 0.046, NG 0.045, KE 0.045, LK 0.044, IN 0.041, BD 0.041, GH 0.036, TZ 0.033, PK 0.029, \n",
      "PH (1): PH 0.085, AU 0.080, JM 0.080, NZ 0.075, IE 0.072, CA 0.070, HK 0.065, NG 0.062, SG 0.055, IN 0.052, LK 0.048, US 0.043, GH 0.037, GB 0.037, ZA 0.035, MY 0.035, TZ 0.025, BD 0.018, KE 0.015, PK 0.010, \n",
      "NZ (1): NZ 0.093, AU 0.085, HK 0.067, US 0.063, JM 0.063, ZA 0.057, GB 0.057, SG 0.057, CA 0.055, IN 0.047, LK 0.045, IE 0.045, PH 0.043, NG 0.043, GH 0.038, KE 0.032, TZ 0.032, PK 0.028, BD 0.025, MY 0.025, \n",
      "KE (6): IE 0.097, CA 0.070, AU 0.063, PH 0.063, IN 0.060, KE 0.060, JM 0.057, MY 0.053, HK 0.053, BD 0.050, NZ 0.050, GH 0.047, US 0.043, LK 0.040, GB 0.040, TZ 0.037, PK 0.037, NG 0.033, SG 0.027, ZA 0.020, \n",
      "IN (2): US 0.106, IN 0.082, JM 0.059, PH 0.058, MY 0.058, LK 0.052, CA 0.052, NZ 0.051, SG 0.051, ZA 0.050, TZ 0.046, AU 0.041, HK 0.041, IE 0.041, NG 0.041, GB 0.039, PK 0.036, KE 0.035, GH 0.033, BD 0.028, \n",
      "MY (14): US 0.183, PH 0.100, ZA 0.070, HK 0.060, JM 0.057, NZ 0.057, NG 0.050, CA 0.047, AU 0.043, TZ 0.040, SG 0.040, IE 0.037, LK 0.037, MY 0.037, IN 0.033, BD 0.030, GH 0.027, GB 0.023, KE 0.020, PK 0.010, \n",
      "GH (1): GH 0.080, MY 0.073, PH 0.057, KE 0.057, AU 0.057, CA 0.053, ZA 0.053, IN 0.050, NG 0.050, JM 0.050, HK 0.050, LK 0.047, NZ 0.047, US 0.047, TZ 0.043, BD 0.043, GB 0.040, IE 0.040, SG 0.033, PK 0.030, \n",
      "US (2): PH 0.062, US 0.062, NZ 0.057, CA 0.056, MY 0.054, AU 0.052, JM 0.052, NG 0.051, TZ 0.050, ZA 0.050, IE 0.050, GH 0.049, HK 0.049, SG 0.049, IN 0.048, GB 0.047, KE 0.046, PK 0.041, LK 0.040, BD 0.037, \n",
      "TZ (1): TZ 0.078, ZA 0.072, US 0.072, IE 0.070, PH 0.064, AU 0.058, NZ 0.056, GH 0.056, JM 0.056, IN 0.050, MY 0.050, HK 0.048, LK 0.046, NG 0.046, SG 0.044, KE 0.044, CA 0.038, BD 0.022, GB 0.016, PK 0.014, \n",
      "BD (1): BD 0.173, PK 0.068, IN 0.062, GB 0.055, KE 0.050, MY 0.050, PH 0.048, TZ 0.048, SG 0.048, NZ 0.045, LK 0.045, JM 0.045, NG 0.045, CA 0.043, ZA 0.037, AU 0.035, HK 0.035, GH 0.025, US 0.025, IE 0.020, \n",
      "JM (13): IE 0.092, PH 0.080, US 0.077, GB 0.068, NG 0.060, IN 0.057, SG 0.057, KE 0.053, ZA 0.052, GH 0.050, TZ 0.050, LK 0.045, JM 0.045, MY 0.042, HK 0.040, CA 0.038, AU 0.030, NZ 0.028, PK 0.020, BD 0.015, \n",
      "HK (1): HK 0.085, AU 0.072, KE 0.070, PH 0.060, US 0.058, GB 0.053, LK 0.050, ZA 0.050, MY 0.050, NG 0.050, TZ 0.050, SG 0.050, IE 0.048, NZ 0.048, IN 0.048, JM 0.042, GH 0.040, CA 0.030, PK 0.025, BD 0.022, \n",
      "PK (20): PH 0.078, NZ 0.066, TZ 0.064, GB 0.058, IN 0.056, CA 0.056, LK 0.054, MY 0.050, HK 0.050, SG 0.050, NG 0.050, IE 0.050, ZA 0.048, GH 0.048, JM 0.048, BD 0.044, AU 0.036, KE 0.036, US 0.034, PK 0.024, \n",
      "CA (10): SG 0.075, IN 0.066, PH 0.064, US 0.062, ZA 0.059, NZ 0.058, AU 0.057, PK 0.054, JM 0.052, CA 0.052, KE 0.050, MY 0.050, GB 0.043, LK 0.041, HK 0.041, NG 0.039, TZ 0.037, BD 0.036, IE 0.034, GH 0.030, \n",
      "GB (15): US 0.101, PH 0.066, JM 0.062, SG 0.062, IE 0.056, MY 0.054, IN 0.054, NZ 0.053, ZA 0.051, LK 0.051, NG 0.048, CA 0.048, HK 0.045, AU 0.042, GB 0.041, TZ 0.039, KE 0.036, PK 0.035, GH 0.031, BD 0.025, \n",
      "NG (1): NG 0.150, PH 0.068, KE 0.068, CA 0.060, AU 0.058, JM 0.058, HK 0.057, IE 0.053, GH 0.050, SG 0.048, BD 0.045, ZA 0.043, MY 0.043, TZ 0.040, NZ 0.038, LK 0.030, US 0.028, GB 0.025, PK 0.025, IN 0.018, \n",
      "IE (4): KE 0.067, ZA 0.065, IN 0.063, IE 0.063, TZ 0.062, GH 0.055, HK 0.053, AU 0.053, GB 0.052, NZ 0.052, SG 0.052, PK 0.050, LK 0.045, PH 0.045, NG 0.045, CA 0.040, US 0.037, BD 0.037, JM 0.035, MY 0.030, \n",
      "SG (1): SG 0.067, PH 0.064, JM 0.061, MY 0.059, NZ 0.054, IE 0.054, ZA 0.054, TZ 0.054, NG 0.053, GH 0.050, IN 0.049, HK 0.049, LK 0.047, US 0.043, GB 0.043, CA 0.043, KE 0.041, BD 0.039, PK 0.039, AU 0.037, \n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier with undersampling\n",
    "rf_model = RandomForestClassifier(random_state=3)\n",
    "rf_model.fit(X_resampled, y_resampled)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(rf_model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][rf_model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][rf_model.classes_[i]] += 1   \n",
    "  \n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.15\n",
      "LK (3): HK 0.402, JM 0.217, LK 0.056, NG 0.054, US 0.053, TZ 0.037, IN 0.026, MY 0.025, SG 0.023, KE 0.015, PH 0.015, GB 0.014, CA 0.012, IE 0.011, NZ 0.008, AU 0.008, GH 0.007, ZA 0.007, BD 0.006, PK 0.004, \n",
      "ZA (16): HK 0.210, JM 0.185, TZ 0.137, KE 0.067, NZ 0.052, IN 0.043, PH 0.038, GB 0.032, SG 0.027, AU 0.027, US 0.026, NG 0.024, BD 0.023, IE 0.022, CA 0.022, ZA 0.019, GH 0.016, MY 0.015, LK 0.009, PK 0.006, \n",
      "AU (13): US 0.198, TZ 0.179, JM 0.141, HK 0.074, PH 0.050, IE 0.042, NG 0.039, SG 0.036, IN 0.029, MY 0.029, KE 0.026, CA 0.026, AU 0.023, NZ 0.022, GB 0.020, BD 0.017, LK 0.016, ZA 0.014, GH 0.011, PK 0.008, \n",
      "PH (1): PH 0.189, US 0.115, JM 0.115, IN 0.099, NG 0.086, TZ 0.085, HK 0.046, SG 0.043, KE 0.034, MY 0.031, CA 0.030, GB 0.030, LK 0.018, NZ 0.017, IE 0.014, BD 0.012, AU 0.012, ZA 0.011, GH 0.007, PK 0.006, \n",
      "NZ (2): HK 0.187, NZ 0.131, TZ 0.092, US 0.072, JM 0.067, NG 0.061, ZA 0.053, AU 0.052, PH 0.040, KE 0.030, LK 0.030, IE 0.029, SG 0.027, CA 0.025, BD 0.025, IN 0.023, GB 0.022, MY 0.020, GH 0.011, PK 0.003, \n",
      "KE (2): SG 0.332, KE 0.225, TZ 0.067, IE 0.056, US 0.048, NZ 0.036, GB 0.035, NG 0.032, AU 0.031, JM 0.025, HK 0.023, BD 0.022, CA 0.021, IN 0.019, MY 0.010, PH 0.008, LK 0.005, GH 0.003, ZA 0.001, PK 0.001, \n",
      "IN (4): US 0.189, TZ 0.131, HK 0.123, IN 0.083, NG 0.074, JM 0.069, NZ 0.061, SG 0.042, PH 0.031, MY 0.029, KE 0.027, CA 0.022, LK 0.021, GB 0.017, AU 0.017, IE 0.015, BD 0.014, ZA 0.012, GH 0.011, PK 0.010, \n",
      "MY (13): HK 0.155, TZ 0.093, JM 0.073, US 0.066, IN 0.060, NG 0.052, KE 0.049, NZ 0.048, PH 0.041, AU 0.040, LK 0.038, GH 0.036, MY 0.034, SG 0.034, IE 0.033, ZA 0.033, CA 0.032, BD 0.032, GB 0.032, PK 0.018, \n",
      "GH (3): TZ 0.404, US 0.107, GH 0.103, IN 0.044, JM 0.044, NG 0.043, PH 0.037, HK 0.030, KE 0.029, GB 0.021, SG 0.018, BD 0.018, CA 0.017, AU 0.017, NZ 0.015, MY 0.015, IE 0.014, LK 0.009, ZA 0.008, PK 0.008, \n",
      "US (1): US 0.205, JM 0.143, TZ 0.117, HK 0.088, IN 0.066, NZ 0.064, SG 0.061, CA 0.045, NG 0.038, PH 0.038, MY 0.028, AU 0.023, KE 0.022, GB 0.014, LK 0.014, BD 0.012, IE 0.010, ZA 0.006, GH 0.005, PK 0.003, \n",
      "TZ (1): TZ 0.254, NG 0.130, US 0.105, JM 0.086, PH 0.074, HK 0.050, IE 0.046, SG 0.035, IN 0.034, KE 0.032, GB 0.023, AU 0.022, GH 0.022, NZ 0.017, LK 0.017, BD 0.016, MY 0.015, ZA 0.010, CA 0.009, PK 0.004, \n",
      "BD (2): PH 0.193, BD 0.182, TZ 0.176, IN 0.168, HK 0.102, US 0.076, CA 0.043, NG 0.030, JM 0.011, GB 0.004, MY 0.004, NZ 0.003, AU 0.002, SG 0.002, ZA 0.001, KE 0.001, LK 0.000, IE 0.000, PK 0.000, GH 0.000, \n",
      "JM (3): US 0.188, PH 0.127, JM 0.118, TZ 0.090, KE 0.057, HK 0.053, NG 0.051, SG 0.046, IN 0.043, MY 0.032, IE 0.032, GB 0.027, NZ 0.025, LK 0.025, CA 0.023, GH 0.018, AU 0.018, BD 0.014, ZA 0.007, PK 0.007, \n",
      "HK (2): US 0.297, HK 0.224, TZ 0.067, JM 0.064, CA 0.058, SG 0.049, IN 0.033, MY 0.028, IE 0.025, NG 0.024, PH 0.021, NZ 0.017, KE 0.017, AU 0.016, LK 0.016, ZA 0.011, BD 0.011, GH 0.010, GB 0.008, PK 0.003, \n",
      "PK (18): JM 0.220, PH 0.187, US 0.173, TZ 0.088, NG 0.063, SG 0.052, IN 0.046, HK 0.040, MY 0.030, KE 0.029, CA 0.026, GB 0.020, NZ 0.010, AU 0.005, BD 0.004, LK 0.003, IE 0.002, PK 0.001, ZA 0.001, GH 0.001, \n",
      "CA (11): HK 0.276, US 0.236, JM 0.101, TZ 0.099, IN 0.053, MY 0.039, PH 0.029, SG 0.026, AU 0.017, NG 0.015, CA 0.015, IE 0.013, KE 0.013, NZ 0.013, GH 0.012, GB 0.011, LK 0.010, BD 0.008, ZA 0.008, PK 0.005, \n",
      "GB (11): US 0.248, TZ 0.126, NG 0.076, SG 0.067, HK 0.057, KE 0.053, JM 0.049, PH 0.043, CA 0.038, IN 0.037, GB 0.036, NZ 0.031, MY 0.026, IE 0.023, AU 0.020, LK 0.018, BD 0.016, ZA 0.016, GH 0.011, PK 0.009, \n",
      "NG (1): NG 0.347, JM 0.164, TZ 0.122, HK 0.119, AU 0.043, IN 0.032, PH 0.026, NZ 0.025, IE 0.020, US 0.018, KE 0.017, SG 0.013, BD 0.011, GB 0.010, MY 0.009, LK 0.007, CA 0.007, ZA 0.005, GH 0.004, PK 0.002, \n",
      "IE (5): US 0.313, HK 0.227, PH 0.219, NG 0.080, IE 0.048, TZ 0.043, JM 0.014, LK 0.013, NZ 0.010, KE 0.007, MY 0.004, IN 0.004, SG 0.003, GB 0.003, AU 0.003, BD 0.002, CA 0.002, ZA 0.002, GH 0.001, PK 0.001, \n",
      "SG (2): US 0.228, SG 0.165, HK 0.165, JM 0.159, TZ 0.070, IN 0.065, PH 0.042, NG 0.027, LK 0.023, MY 0.010, KE 0.009, GB 0.008, IE 0.006, GH 0.005, NZ 0.004, ZA 0.004, CA 0.004, AU 0.003, BD 0.003, PK 0.001, \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Multi-layer Perceptron Model with undersampling\n",
    "mlp_model = MLPClassifier(random_state=3)\n",
    "mlp_model.fit(X_resampled, y_resampled)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(mlp_model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][mlp_model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][mlp_model.classes_[i]] += 1     \n",
    "\n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'TZ': 2.855769230769231, 'GH': 2.3951612903225805, 'BD': 2.3951612903225805, 'JM': 2.3951612903225805, 'HK': 2.3951612903225805, 'SG': 2.3951612903225805, 'ZA': 2.25, 'KE': 2.25, 'NG': 2.25, 'PH': 2.1838235294117645, 'MY': 2.1838235294117645, 'LK': 2.0625, 'PK': 1.85625, 'NZ': 1.1423076923076922, 'IN': 0.99, 'IE': 0.9054878048780488, 'CA': 0.6939252336448598, 'AU': 0.6570796460176991, 'US': 0.24029126213592233, 'GB': 0.23951612903225805})\n"
     ]
    }
   ],
   "source": [
    "# Calculate weights based on document number (default \"balanced\" weights)\n",
    "total_docs = sum(Counter(y_train).values())\n",
    "weights_dict_doc = Counter({country_code:0 for country_code in country_set})\n",
    "\n",
    "for country_code in weights_dict_doc:\n",
    "    weights_dict_doc[country_code] = total_docs/(len(country_set) * Counter(y_train)[country_code])\n",
    "\n",
    "print(weights_dict_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'PH': 3.910410016719866, 'JM': 3.8743401980347127, 'GH': 3.3747532220509626, 'ZA': 2.622619654416637, 'BD': 2.469495572460464, 'KE': 2.4500928549320253, 'SG': 2.444912093133761, 'NG': 2.262075749582344, 'MY': 2.1723032593092966, 'HK': 2.0897236211518155, 'TZ': 2.0851083014203797, 'NZ': 1.4886779214767236, 'LK': 1.3360817179730204, 'IN': 0.9784297599239363, 'IE': 0.9313634718074034, 'PK': 0.7469376719338613, 'CA': 0.6877617376775271, 'AU': 0.4746887497866569, 'GB': 0.3223559981455493, 'US': 0.23762114120382663})\n"
     ]
    }
   ],
   "source": [
    "# Calculate weights based on word count\n",
    "total_words = sum(country_word_counts.values())\n",
    "weights_dict_word = Counter({country_code:0 for country_code in country_set})\n",
    "\n",
    "for country_code in weights_dict_word:\n",
    "    weights_dict_word[country_code] = total_words/(len(country_set) * country_word_counts[country_code])\n",
    "\n",
    "print(weights_dict_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28\n",
      "LK (3): GB 0.232, US 0.198, LK 0.118, AU 0.056, IN 0.050, CA 0.050, NZ 0.044, PK 0.038, HK 0.036, BD 0.024, IE 0.024, PH 0.024, JM 0.022, SG 0.016, GH 0.014, KE 0.012, TZ 0.012, NG 0.012, MY 0.010, ZA 0.008, \n",
      "ZA (7): US 0.234, GB 0.229, AU 0.070, NZ 0.059, CA 0.056, IE 0.053, ZA 0.043, IN 0.033, KE 0.030, TZ 0.026, GH 0.023, NG 0.023, HK 0.020, PH 0.017, PK 0.017, LK 0.016, BD 0.016, SG 0.014, MY 0.013, JM 0.010, \n",
      "AU (3): GB 0.268, US 0.248, AU 0.113, IN 0.058, CA 0.053, IE 0.044, NZ 0.039, LK 0.021, ZA 0.021, MY 0.016, KE 0.016, PH 0.015, PK 0.014, HK 0.013, BD 0.012, SG 0.012, GH 0.011, NG 0.010, TZ 0.009, JM 0.006, \n",
      "PH (8): US 0.280, GB 0.270, AU 0.075, CA 0.065, NZ 0.048, IE 0.030, IN 0.028, PH 0.025, MY 0.025, ZA 0.022, BD 0.022, HK 0.020, KE 0.018, JM 0.015, NG 0.015, LK 0.013, GH 0.010, SG 0.010, TZ 0.005, PK 0.005, \n",
      "NZ (3): US 0.222, GB 0.207, NZ 0.133, AU 0.122, CA 0.053, HK 0.033, IE 0.030, IN 0.025, PH 0.020, SG 0.018, LK 0.018, GH 0.017, BD 0.015, PK 0.015, MY 0.013, ZA 0.012, KE 0.012, TZ 0.012, NG 0.012, JM 0.012, \n",
      "KE (6): GB 0.260, US 0.193, AU 0.120, CA 0.090, IE 0.043, KE 0.040, IN 0.033, PH 0.027, NZ 0.027, MY 0.027, BD 0.023, PK 0.023, LK 0.017, ZA 0.013, JM 0.013, NG 0.013, SG 0.013, GH 0.010, TZ 0.007, HK 0.007, \n",
      "IN (3): GB 0.267, US 0.249, IN 0.094, AU 0.056, CA 0.040, IE 0.040, NZ 0.038, LK 0.031, MY 0.025, BD 0.021, JM 0.020, PH 0.017, GH 0.017, HK 0.016, ZA 0.014, SG 0.014, PK 0.013, KE 0.011, NG 0.011, TZ 0.006, \n",
      "MY (17): US 0.353, GB 0.297, CA 0.050, AU 0.047, IE 0.043, NZ 0.040, IN 0.033, HK 0.027, ZA 0.020, TZ 0.013, LK 0.010, PH 0.010, KE 0.010, PK 0.010, NG 0.010, SG 0.010, MY 0.007, JM 0.007, GH 0.003, BD 0.000, \n",
      "GH (3): GB 0.220, US 0.213, GH 0.087, AU 0.067, IN 0.063, CA 0.057, NZ 0.040, IE 0.037, KE 0.030, NG 0.030, HK 0.023, LK 0.020, PK 0.020, ZA 0.017, TZ 0.017, PH 0.013, BD 0.013, JM 0.013, MY 0.010, SG 0.010, \n",
      "US (2): GB 0.270, US 0.269, AU 0.071, CA 0.066, NZ 0.043, IN 0.040, IE 0.036, PH 0.021, PK 0.021, MY 0.019, ZA 0.019, HK 0.019, LK 0.017, SG 0.016, KE 0.015, GH 0.015, BD 0.012, NG 0.012, JM 0.012, TZ 0.008, \n",
      "TZ (6): US 0.244, GB 0.214, AU 0.092, CA 0.060, ZA 0.048, TZ 0.046, IE 0.042, GH 0.036, IN 0.034, NZ 0.028, NG 0.028, JM 0.024, HK 0.022, LK 0.016, KE 0.016, MY 0.014, PH 0.012, PK 0.012, BD 0.010, SG 0.002, \n",
      "BD (3): GB 0.180, US 0.165, BD 0.158, CA 0.077, NZ 0.068, IN 0.055, AU 0.045, PK 0.040, IE 0.038, LK 0.028, SG 0.025, NG 0.022, KE 0.020, MY 0.018, PH 0.015, TZ 0.013, ZA 0.010, JM 0.010, HK 0.010, GH 0.005, \n",
      "JM (11): GB 0.297, US 0.250, IE 0.128, CA 0.053, AU 0.043, NZ 0.037, ZA 0.030, LK 0.028, TZ 0.023, IN 0.022, JM 0.020, PH 0.013, PK 0.013, KE 0.010, BD 0.010, SG 0.010, NG 0.007, GH 0.005, MY 0.003, HK 0.000, \n",
      "HK (3): US 0.290, GB 0.210, HK 0.087, AU 0.065, CA 0.043, IE 0.042, NZ 0.037, IN 0.035, GH 0.035, SG 0.025, LK 0.023, KE 0.022, PH 0.020, BD 0.017, JM 0.013, ZA 0.010, PK 0.010, MY 0.007, TZ 0.007, NG 0.000, \n",
      "PK (12): GB 0.264, US 0.262, CA 0.072, AU 0.062, NZ 0.048, IN 0.038, IE 0.032, PH 0.030, KE 0.024, MY 0.024, JM 0.024, PK 0.022, HK 0.016, LK 0.014, BD 0.014, ZA 0.014, NG 0.012, SG 0.012, GH 0.010, TZ 0.006, \n",
      "CA (4): GB 0.257, US 0.245, AU 0.080, CA 0.051, IE 0.039, IN 0.034, PK 0.031, NZ 0.029, HK 0.028, JM 0.026, GH 0.024, PH 0.023, MY 0.023, LK 0.017, KE 0.017, TZ 0.017, NG 0.017, ZA 0.015, SG 0.014, BD 0.013, \n",
      "GB (1): GB 0.317, US 0.284, AU 0.061, CA 0.054, IE 0.051, NZ 0.036, IN 0.033, PK 0.017, KE 0.016, HK 0.014, MY 0.014, NG 0.014, PH 0.014, GH 0.014, SG 0.011, LK 0.010, TZ 0.010, BD 0.010, ZA 0.009, JM 0.009, \n",
      "NG (3): US 0.230, GB 0.182, NG 0.182, AU 0.085, CA 0.043, IE 0.040, NZ 0.038, KE 0.030, PH 0.022, GH 0.022, TZ 0.018, JM 0.018, HK 0.018, SG 0.018, IN 0.017, LK 0.015, PK 0.013, MY 0.005, ZA 0.003, BD 0.003, \n",
      "IE (3): GB 0.225, US 0.195, IE 0.120, AU 0.080, CA 0.063, NZ 0.035, IN 0.035, GH 0.033, PH 0.027, KE 0.023, ZA 0.022, MY 0.022, NG 0.020, SG 0.020, JM 0.018, TZ 0.015, HK 0.015, PK 0.015, LK 0.010, BD 0.007, \n",
      "SG (6): GB 0.246, US 0.227, AU 0.074, CA 0.069, IN 0.063, SG 0.051, PH 0.041, NZ 0.041, IE 0.036, LK 0.021, MY 0.017, GH 0.017, JM 0.017, HK 0.016, ZA 0.014, KE 0.010, TZ 0.010, BD 0.010, NG 0.010, PK 0.009, \n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier with class weights balanced by word count\n",
    "rf_model = RandomForestClassifier(random_state=3, class_weight=weights_dict_word)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(rf_model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][rf_model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][rf_model.classes_[i]] += 1     \n",
    "\n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like the weight balanced RF and MLP model without undersampling had the best performance so we will use them for the 25%...\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_25, y, test_size=0.1, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.27\n",
      "LK (3): GB 0.188, US 0.176, LK 0.164, AU 0.076, CA 0.052, IE 0.048, NZ 0.044, PK 0.034, IN 0.032, HK 0.032, TZ 0.024, SG 0.022, NG 0.018, JM 0.016, BD 0.014, ZA 0.014, KE 0.014, PH 0.012, GH 0.012, MY 0.008, \n",
      "ZA (6): US 0.224, GB 0.169, AU 0.086, NZ 0.074, CA 0.064, ZA 0.054, IN 0.041, IE 0.036, JM 0.030, NG 0.030, KE 0.030, GH 0.029, BD 0.023, PH 0.019, LK 0.017, PK 0.017, TZ 0.016, HK 0.016, MY 0.013, SG 0.013, \n",
      "AU (3): US 0.244, GB 0.225, AU 0.157, IE 0.058, IN 0.052, CA 0.046, NZ 0.033, LK 0.024, MY 0.024, KE 0.018, ZA 0.016, SG 0.016, HK 0.014, BD 0.013, PH 0.012, GH 0.011, NG 0.011, PK 0.010, JM 0.008, TZ 0.007, \n",
      "PH (12): US 0.268, GB 0.253, AU 0.125, CA 0.062, NZ 0.052, IN 0.045, IE 0.028, GH 0.023, JM 0.022, HK 0.022, SG 0.022, PH 0.015, KE 0.015, PK 0.015, NG 0.010, ZA 0.007, MY 0.005, TZ 0.005, BD 0.005, LK 0.000, \n",
      "NZ (4): AU 0.188, US 0.168, GB 0.152, NZ 0.145, HK 0.053, CA 0.047, IN 0.038, IE 0.033, GH 0.025, LK 0.022, ZA 0.017, TZ 0.017, BD 0.017, PH 0.013, KE 0.013, PK 0.013, NG 0.013, SG 0.010, MY 0.008, JM 0.007, \n",
      "KE (7): GB 0.220, US 0.187, AU 0.113, IE 0.070, CA 0.067, NZ 0.057, KE 0.057, IN 0.043, BD 0.037, NG 0.023, MY 0.020, PK 0.020, PH 0.017, LK 0.013, GH 0.013, JM 0.013, TZ 0.010, SG 0.010, HK 0.007, ZA 0.003, \n",
      "IN (3): US 0.268, GB 0.236, IN 0.105, CA 0.060, AU 0.058, LK 0.029, IE 0.025, ZA 0.023, NZ 0.023, SG 0.021, PH 0.020, MY 0.019, KE 0.018, GH 0.018, PK 0.018, BD 0.016, HK 0.012, JM 0.011, NG 0.011, TZ 0.009, \n",
      "MY (19): GB 0.367, US 0.303, CA 0.047, HK 0.037, IE 0.037, AU 0.033, NZ 0.033, ZA 0.023, IN 0.023, JM 0.020, PH 0.013, KE 0.013, PK 0.013, NG 0.010, SG 0.010, LK 0.007, TZ 0.007, BD 0.003, MY 0.000, GH 0.000, \n",
      "GH (9): US 0.237, GB 0.203, AU 0.067, HK 0.060, CA 0.057, KE 0.047, IN 0.047, PK 0.033, GH 0.030, TZ 0.027, NG 0.027, IE 0.027, NZ 0.023, BD 0.023, ZA 0.020, LK 0.017, PH 0.017, MY 0.017, JM 0.013, SG 0.010, \n",
      "US (1): US 0.274, GB 0.242, CA 0.071, AU 0.058, IN 0.048, NZ 0.037, IE 0.032, ZA 0.025, PK 0.023, MY 0.022, NG 0.022, PH 0.018, HK 0.018, SG 0.018, KE 0.017, LK 0.017, GH 0.017, TZ 0.014, BD 0.013, JM 0.013, \n",
      "TZ (8): GB 0.224, US 0.210, ZA 0.080, IE 0.062, CA 0.058, AU 0.054, GH 0.054, TZ 0.042, NG 0.030, JM 0.028, IN 0.026, PH 0.022, KE 0.018, BD 0.018, NZ 0.016, HK 0.016, MY 0.012, LK 0.010, PK 0.010, SG 0.010, \n",
      "BD (3): US 0.155, GB 0.155, BD 0.128, CA 0.092, IN 0.077, NZ 0.075, PK 0.060, AU 0.045, LK 0.033, JM 0.030, IE 0.025, SG 0.025, NG 0.022, TZ 0.015, MY 0.015, PH 0.013, GH 0.013, ZA 0.007, KE 0.007, HK 0.007, \n",
      "JM (6): US 0.247, GB 0.227, IE 0.105, CA 0.080, AU 0.050, JM 0.048, SG 0.037, IN 0.035, NZ 0.028, PH 0.025, ZA 0.020, HK 0.020, NG 0.018, KE 0.015, MY 0.013, GH 0.010, TZ 0.010, PK 0.007, LK 0.003, BD 0.003, \n",
      "HK (3): GB 0.198, US 0.193, HK 0.110, CA 0.065, AU 0.058, SG 0.045, IN 0.040, ZA 0.035, IE 0.035, NZ 0.033, MY 0.033, GH 0.030, KE 0.028, LK 0.020, PH 0.020, BD 0.015, TZ 0.015, JM 0.013, PK 0.010, NG 0.007, \n",
      "PK (15): GB 0.244, US 0.226, CA 0.076, AU 0.066, NZ 0.042, PH 0.038, IN 0.036, JM 0.030, LK 0.026, MY 0.026, NG 0.026, IE 0.026, SG 0.024, HK 0.022, PK 0.018, KE 0.018, BD 0.018, ZA 0.014, GH 0.012, TZ 0.012, \n",
      "CA (3): US 0.245, GB 0.206, CA 0.078, AU 0.070, IE 0.050, IN 0.047, PK 0.046, NZ 0.031, GH 0.029, NG 0.025, ZA 0.023, PH 0.021, KE 0.021, MY 0.020, JM 0.019, HK 0.015, SG 0.015, BD 0.014, LK 0.014, TZ 0.011, \n",
      "GB (2): US 0.286, GB 0.267, CA 0.068, AU 0.066, IE 0.052, IN 0.042, NZ 0.032, PK 0.022, MY 0.018, PH 0.017, GH 0.016, KE 0.015, ZA 0.015, JM 0.014, TZ 0.014, LK 0.012, NG 0.012, HK 0.011, SG 0.011, BD 0.009, \n",
      "NG (3): GB 0.190, US 0.163, NG 0.143, AU 0.065, CA 0.045, NZ 0.045, IN 0.043, IE 0.037, JM 0.035, HK 0.033, KE 0.028, GH 0.028, BD 0.028, SG 0.023, LK 0.022, ZA 0.022, MY 0.018, PH 0.015, PK 0.013, TZ 0.007, \n",
      "IE (3): GB 0.192, US 0.160, IE 0.138, AU 0.085, CA 0.065, IN 0.048, NG 0.030, NZ 0.028, JM 0.027, PK 0.027, HK 0.025, ZA 0.022, GH 0.022, TZ 0.022, SG 0.022, PH 0.020, MY 0.020, KE 0.018, LK 0.015, BD 0.015, \n",
      "SG (5): US 0.203, GB 0.203, AU 0.076, IN 0.069, SG 0.059, CA 0.051, LK 0.046, NZ 0.043, MY 0.036, ZA 0.027, IE 0.026, PH 0.026, HK 0.020, KE 0.019, TZ 0.019, PK 0.019, GH 0.017, JM 0.017, NG 0.014, BD 0.013, \n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier with class weights balanced\n",
    "rf_model = RandomForestClassifier(random_state=3, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(rf_model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][rf_model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][rf_model.classes_[i]] += 1     \n",
    "\n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25\n",
      "LK (8): GB 0.459, CA 0.158, IE 0.115, US 0.052, NG 0.042, AU 0.035, JM 0.027, LK 0.024, MY 0.017, IN 0.014, HK 0.012, SG 0.010, BD 0.008, TZ 0.008, ZA 0.005, GH 0.004, NZ 0.003, PH 0.003, KE 0.003, PK 0.002, \n",
      "ZA (4): US 0.233, GB 0.219, IN 0.129, ZA 0.059, IE 0.057, AU 0.055, CA 0.039, NZ 0.039, PH 0.033, JM 0.026, HK 0.021, MY 0.017, NG 0.014, GH 0.011, SG 0.009, BD 0.009, LK 0.008, TZ 0.008, PK 0.006, KE 0.006, \n",
      "AU (1): AU 0.272, GB 0.261, US 0.215, IE 0.054, CA 0.040, NG 0.020, MY 0.014, ZA 0.013, IN 0.013, TZ 0.011, HK 0.011, SG 0.010, PH 0.010, KE 0.010, JM 0.010, BD 0.009, LK 0.007, NZ 0.007, PK 0.006, GH 0.006, \n",
      "PH (13): GB 0.277, AU 0.253, US 0.213, CA 0.127, IE 0.030, NG 0.019, HK 0.011, IN 0.010, JM 0.008, LK 0.008, BD 0.007, SG 0.007, PH 0.005, ZA 0.005, KE 0.005, MY 0.004, NZ 0.004, GH 0.004, TZ 0.002, PK 0.001, \n",
      "NZ (13): GB 0.331, US 0.263, AU 0.223, CA 0.097, JM 0.015, LK 0.011, IE 0.011, TZ 0.006, HK 0.006, SG 0.006, NG 0.005, PH 0.005, NZ 0.005, MY 0.003, PK 0.003, BD 0.003, ZA 0.002, IN 0.002, KE 0.002, GH 0.001, \n",
      "KE (12): GB 0.663, AU 0.252, US 0.069, CA 0.012, IE 0.003, NG 0.000, IN 0.000, LK 0.000, ZA 0.000, MY 0.000, JM 0.000, KE 0.000, NZ 0.000, SG 0.000, BD 0.000, HK 0.000, PH 0.000, PK 0.000, GH 0.000, TZ 0.000, \n",
      "IN (3): US 0.385, GB 0.221, IN 0.056, NG 0.047, AU 0.040, NZ 0.037, IE 0.035, CA 0.033, JM 0.023, SG 0.020, HK 0.017, TZ 0.016, BD 0.014, LK 0.013, MY 0.011, KE 0.010, ZA 0.006, PH 0.005, PK 0.005, GH 0.005, \n",
      "MY (16): US 0.272, NG 0.075, GB 0.067, TZ 0.062, NZ 0.054, AU 0.053, CA 0.050, HK 0.048, GH 0.046, JM 0.034, KE 0.032, ZA 0.031, IE 0.029, IN 0.027, SG 0.026, MY 0.024, PK 0.021, BD 0.016, PH 0.016, LK 0.015, \n",
      "GH (9): US 0.361, GB 0.271, AU 0.110, JM 0.047, BD 0.041, IE 0.023, PK 0.017, IN 0.016, GH 0.014, MY 0.014, NG 0.013, TZ 0.013, CA 0.012, PH 0.011, SG 0.011, KE 0.008, ZA 0.006, LK 0.005, HK 0.004, NZ 0.002, \n",
      "US (1): US 0.448, GB 0.223, CA 0.063, IE 0.050, AU 0.044, SG 0.042, IN 0.027, NG 0.016, JM 0.014, MY 0.012, NZ 0.011, PK 0.008, PH 0.008, TZ 0.006, LK 0.006, BD 0.006, KE 0.006, ZA 0.004, HK 0.003, GH 0.003, \n",
      "TZ (5): US 0.345, GB 0.211, JM 0.098, IE 0.058, TZ 0.054, AU 0.053, NG 0.050, ZA 0.033, GH 0.019, BD 0.015, IN 0.014, CA 0.014, MY 0.009, LK 0.009, PH 0.007, KE 0.004, SG 0.003, PK 0.001, NZ 0.001, HK 0.001, \n",
      "BD (3): US 0.263, GB 0.263, BD 0.221, IN 0.206, PK 0.032, NG 0.009, MY 0.002, IE 0.001, JM 0.001, CA 0.000, NZ 0.000, AU 0.000, SG 0.000, HK 0.000, ZA 0.000, TZ 0.000, LK 0.000, GH 0.000, KE 0.000, PH 0.000, \n",
      "JM (6): GB 0.351, AU 0.228, US 0.076, IE 0.059, CA 0.058, JM 0.039, ZA 0.037, NG 0.028, PH 0.018, SG 0.016, MY 0.015, LK 0.013, TZ 0.013, BD 0.011, PK 0.011, IN 0.007, GH 0.006, NZ 0.005, HK 0.005, KE 0.003, \n",
      "HK (2): US 0.636, HK 0.120, GB 0.041, AU 0.029, IE 0.020, NG 0.019, CA 0.016, SG 0.016, IN 0.016, TZ 0.015, JM 0.012, MY 0.012, NZ 0.011, BD 0.010, PK 0.007, ZA 0.007, LK 0.005, GH 0.003, PH 0.003, KE 0.003, \n",
      "PK (5): GB 0.318, JM 0.200, US 0.140, CA 0.116, PK 0.066, IN 0.046, AU 0.031, NZ 0.017, BD 0.013, IE 0.010, SG 0.009, MY 0.007, NG 0.005, KE 0.005, TZ 0.005, ZA 0.004, PH 0.003, LK 0.002, HK 0.001, GH 0.001, \n",
      "CA (5): US 0.223, GB 0.218, NZ 0.172, AU 0.139, CA 0.112, IN 0.023, IE 0.020, JM 0.019, NG 0.010, SG 0.010, ZA 0.010, MY 0.008, GH 0.006, BD 0.006, PH 0.005, TZ 0.005, KE 0.005, HK 0.004, PK 0.004, LK 0.003, \n",
      "GB (1): GB 0.429, US 0.261, CA 0.065, PK 0.041, IE 0.036, AU 0.032, NZ 0.017, SG 0.014, JM 0.012, IN 0.012, NG 0.010, MY 0.009, HK 0.009, TZ 0.009, ZA 0.009, BD 0.008, LK 0.008, GH 0.008, KE 0.007, PH 0.007, \n",
      "NG (2): GB 0.405, NG 0.254, AU 0.222, US 0.065, CA 0.033, JM 0.005, SG 0.005, NZ 0.005, IE 0.003, IN 0.001, BD 0.001, HK 0.000, LK 0.000, PH 0.000, ZA 0.000, MY 0.000, TZ 0.000, KE 0.000, GH 0.000, PK 0.000, \n",
      "IE (3): US 0.473, GB 0.324, IE 0.183, HK 0.008, CA 0.004, AU 0.004, NG 0.003, SG 0.001, LK 0.000, JM 0.000, IN 0.000, MY 0.000, ZA 0.000, NZ 0.000, TZ 0.000, KE 0.000, BD 0.000, PK 0.000, PH 0.000, GH 0.000, \n",
      "SG (3): US 0.327, GB 0.195, SG 0.113, ZA 0.099, IN 0.053, JM 0.044, IE 0.029, AU 0.025, LK 0.022, CA 0.021, MY 0.016, BD 0.012, NZ 0.012, NG 0.011, PK 0.006, HK 0.005, TZ 0.004, PH 0.003, GH 0.002, KE 0.001, \n"
     ]
    }
   ],
   "source": [
    "# Multi-layer Perceptron Model removing words that appear in less than 25% of countries\n",
    "mlp_model = MLPClassifier(random_state=3)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(mlp_model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][mlp_model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][mlp_model.classes_[i]] += 1  \n",
    "   \n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF and MLP model without undersampling for the 50% dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_50, y, test_size=0.1, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22\n",
      "LK (11): US 0.230, GB 0.180, AU 0.088, CA 0.060, PK 0.056, HK 0.052, IE 0.044, IN 0.042, NZ 0.038, SG 0.032, LK 0.028, MY 0.020, TZ 0.020, BD 0.020, PH 0.020, JM 0.018, KE 0.016, NG 0.016, ZA 0.012, GH 0.008, \n",
      "ZA (6): US 0.177, GB 0.170, AU 0.097, NZ 0.080, GH 0.056, ZA 0.050, CA 0.046, KE 0.036, IN 0.034, IE 0.034, LK 0.029, JM 0.029, NG 0.027, PK 0.024, BD 0.024, TZ 0.023, PH 0.021, MY 0.017, HK 0.014, SG 0.011, \n",
      "AU (3): US 0.225, GB 0.223, AU 0.144, IE 0.058, IN 0.054, CA 0.054, NZ 0.035, LK 0.023, SG 0.021, ZA 0.021, JM 0.019, BD 0.018, NG 0.016, KE 0.016, PH 0.015, GH 0.014, MY 0.013, PK 0.012, HK 0.011, TZ 0.010, \n",
      "PH (18): US 0.258, GB 0.222, AU 0.092, CA 0.085, NZ 0.055, IE 0.042, IN 0.038, HK 0.037, MY 0.025, NG 0.018, SG 0.017, GH 0.015, TZ 0.015, LK 0.013, ZA 0.013, KE 0.013, PK 0.013, PH 0.010, BD 0.010, JM 0.010, \n",
      "NZ (4): US 0.170, GB 0.153, AU 0.148, NZ 0.128, IE 0.048, CA 0.043, HK 0.042, IN 0.032, PK 0.028, MY 0.027, ZA 0.025, JM 0.023, TZ 0.023, LK 0.022, NG 0.018, KE 0.018, GH 0.018, BD 0.017, PH 0.010, SG 0.005, \n",
      "KE (17): US 0.250, GB 0.230, CA 0.100, AU 0.050, MY 0.037, NZ 0.033, IN 0.033, LK 0.030, IE 0.030, BD 0.027, NG 0.027, ZA 0.023, TZ 0.023, PK 0.023, JM 0.020, PH 0.013, KE 0.013, GH 0.013, SG 0.013, HK 0.010, \n",
      "IN (3): GB 0.248, US 0.241, IN 0.116, CA 0.059, AU 0.047, IE 0.040, LK 0.039, NZ 0.021, MY 0.021, KE 0.020, PH 0.019, ZA 0.019, PK 0.019, BD 0.016, SG 0.015, GH 0.013, NG 0.013, HK 0.012, TZ 0.011, JM 0.011, \n",
      "MY (17): US 0.333, GB 0.243, AU 0.057, HK 0.047, IE 0.047, IN 0.043, CA 0.043, JM 0.033, ZA 0.027, PH 0.023, NG 0.020, SG 0.020, GH 0.013, PK 0.013, NZ 0.010, BD 0.010, MY 0.007, TZ 0.007, KE 0.003, LK 0.000, \n",
      "GH (10): US 0.237, GB 0.210, AU 0.067, IN 0.057, CA 0.050, LK 0.047, HK 0.043, NG 0.043, IE 0.037, GH 0.033, NZ 0.027, MY 0.020, TZ 0.020, PK 0.020, ZA 0.017, JM 0.017, SG 0.017, PH 0.013, KE 0.013, BD 0.013, \n",
      "US (1): US 0.253, GB 0.240, AU 0.072, CA 0.064, IE 0.044, IN 0.040, NZ 0.026, LK 0.026, KE 0.025, MY 0.023, JM 0.023, PK 0.022, NG 0.022, GH 0.021, PH 0.019, ZA 0.017, BD 0.016, HK 0.016, SG 0.016, TZ 0.015, \n",
      "TZ (8): GB 0.200, US 0.188, GH 0.068, ZA 0.066, AU 0.060, CA 0.052, IE 0.050, TZ 0.040, JM 0.040, IN 0.034, KE 0.030, BD 0.024, NG 0.024, LK 0.022, PH 0.020, NZ 0.020, PK 0.020, MY 0.014, HK 0.014, SG 0.014, \n",
      "BD (6): GB 0.180, US 0.158, CA 0.087, IN 0.085, AU 0.068, BD 0.062, NZ 0.058, PK 0.040, ZA 0.030, IE 0.030, PH 0.028, TZ 0.025, JM 0.025, NG 0.025, SG 0.025, LK 0.025, MY 0.017, GH 0.015, KE 0.013, HK 0.005, \n",
      "JM (17): GB 0.225, US 0.203, IE 0.155, CA 0.080, AU 0.068, GH 0.038, SG 0.028, IN 0.028, PH 0.025, NG 0.025, NZ 0.020, HK 0.018, ZA 0.017, KE 0.017, TZ 0.015, LK 0.013, JM 0.013, MY 0.010, PK 0.005, BD 0.000, \n",
      "HK (3): GB 0.225, US 0.175, HK 0.105, CA 0.060, LK 0.052, AU 0.048, IN 0.045, KE 0.042, ZA 0.035, IE 0.035, GH 0.025, BD 0.025, NG 0.022, NZ 0.020, TZ 0.020, PH 0.015, MY 0.013, JM 0.013, PK 0.013, SG 0.013, \n",
      "PK (15): GB 0.232, US 0.218, AU 0.070, CA 0.070, IE 0.056, PH 0.038, IN 0.038, NG 0.034, LK 0.030, HK 0.026, NZ 0.024, MY 0.024, KE 0.022, JM 0.022, PK 0.020, TZ 0.018, ZA 0.018, BD 0.018, GH 0.012, SG 0.010, \n",
      "CA (3): US 0.241, GB 0.212, CA 0.072, AU 0.072, PK 0.052, IE 0.043, IN 0.039, KE 0.033, NZ 0.026, JM 0.023, SG 0.023, MY 0.022, BD 0.022, HK 0.021, NG 0.021, PH 0.021, ZA 0.018, LK 0.018, GH 0.011, TZ 0.010, \n",
      "GB (1): GB 0.266, US 0.259, CA 0.072, IE 0.060, AU 0.058, IN 0.039, NZ 0.035, PK 0.021, KE 0.020, LK 0.020, MY 0.020, PH 0.019, NG 0.018, SG 0.016, BD 0.015, GH 0.015, JM 0.014, ZA 0.013, HK 0.013, TZ 0.009, \n",
      "NG (10): US 0.200, GB 0.190, AU 0.077, IE 0.068, CA 0.062, GH 0.042, IN 0.040, NZ 0.037, JM 0.037, NG 0.035, ZA 0.025, HK 0.025, PH 0.023, SG 0.023, LK 0.022, KE 0.022, TZ 0.022, PK 0.020, BD 0.018, MY 0.010, \n",
      "IE (3): US 0.177, GB 0.167, IE 0.085, AU 0.082, CA 0.053, TZ 0.047, GH 0.045, IN 0.040, LK 0.037, NZ 0.035, JM 0.033, PH 0.027, KE 0.027, BD 0.027, PK 0.027, NG 0.025, SG 0.022, ZA 0.017, MY 0.017, HK 0.013, \n",
      "SG (4): GB 0.201, US 0.199, AU 0.071, SG 0.066, CA 0.064, IN 0.060, NZ 0.044, LK 0.040, IE 0.036, MY 0.029, PH 0.027, JM 0.026, NG 0.021, ZA 0.021, TZ 0.019, HK 0.017, GH 0.016, BD 0.014, PK 0.014, KE 0.014, \n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier with class weights balanced\n",
    "rf_model = RandomForestClassifier(random_state=3, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(rf_model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][rf_model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][rf_model.classes_[i]] += 1     \n",
    "\n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.23\n",
      "LK (7): CA 0.259, GB 0.247, PK 0.208, IN 0.116, AU 0.035, US 0.030, LK 0.027, HK 0.013, NG 0.010, NZ 0.009, IE 0.009, TZ 0.008, ZA 0.008, SG 0.005, JM 0.005, KE 0.003, BD 0.003, MY 0.002, GH 0.002, PH 0.001, \n",
      "ZA (6): GB 0.307, IE 0.136, US 0.107, CA 0.090, GH 0.075, ZA 0.064, AU 0.037, IN 0.032, KE 0.023, PH 0.022, TZ 0.022, NZ 0.017, HK 0.016, PK 0.011, NG 0.010, JM 0.009, MY 0.008, BD 0.005, SG 0.004, LK 0.003, \n",
      "AU (3): US 0.316, GB 0.277, AU 0.177, IE 0.038, CA 0.035, NG 0.022, IN 0.022, ZA 0.013, HK 0.012, NZ 0.010, LK 0.010, PH 0.009, SG 0.009, MY 0.008, BD 0.008, GH 0.008, JM 0.007, PK 0.007, TZ 0.006, KE 0.005, \n",
      "PH (7): US 0.363, GB 0.235, CA 0.124, AU 0.106, IE 0.049, IN 0.028, PH 0.014, PK 0.012, NZ 0.009, NG 0.009, GH 0.008, KE 0.008, SG 0.007, BD 0.007, HK 0.007, ZA 0.003, TZ 0.003, JM 0.003, MY 0.003, LK 0.002, \n",
      "NZ (5): GB 0.392, US 0.222, AU 0.097, IE 0.051, NZ 0.049, CA 0.045, BD 0.043, IN 0.023, ZA 0.018, LK 0.015, PK 0.013, HK 0.005, JM 0.004, NG 0.004, GH 0.004, MY 0.003, TZ 0.003, PH 0.002, KE 0.002, SG 0.002, \n",
      "KE (6): GB 0.575, AU 0.323, IE 0.077, CA 0.010, US 0.005, KE 0.003, NZ 0.002, IN 0.001, LK 0.001, GH 0.001, ZA 0.001, PH 0.000, JM 0.000, PK 0.000, NG 0.000, BD 0.000, TZ 0.000, MY 0.000, HK 0.000, SG 0.000, \n",
      "IN (2): US 0.359, IN 0.162, IE 0.107, NZ 0.091, GB 0.064, ZA 0.043, CA 0.036, TZ 0.021, NG 0.017, MY 0.015, AU 0.013, GH 0.011, PK 0.010, SG 0.009, PH 0.009, HK 0.007, JM 0.007, LK 0.007, KE 0.005, BD 0.005, \n",
      "MY (17): GB 0.193, US 0.176, IN 0.094, HK 0.092, CA 0.069, SG 0.062, GH 0.054, IE 0.053, PK 0.050, KE 0.030, NG 0.023, ZA 0.021, NZ 0.019, TZ 0.014, LK 0.013, AU 0.010, MY 0.008, BD 0.007, PH 0.007, JM 0.004, \n",
      "GH (10): GB 0.354, US 0.257, CA 0.129, IN 0.098, BD 0.035, TZ 0.020, AU 0.018, PK 0.016, KE 0.013, GH 0.008, NZ 0.008, MY 0.008, IE 0.007, ZA 0.006, JM 0.006, PH 0.005, LK 0.004, NG 0.003, SG 0.003, HK 0.003, \n",
      "US (1): US 0.467, GB 0.209, IE 0.066, CA 0.065, NZ 0.053, AU 0.023, IN 0.018, JM 0.016, MY 0.014, SG 0.013, PK 0.010, NG 0.008, PH 0.007, ZA 0.006, LK 0.006, TZ 0.005, BD 0.005, GH 0.004, HK 0.003, KE 0.002, \n",
      "TZ (11): US 0.362, GB 0.122, NG 0.106, IE 0.092, GH 0.042, JM 0.040, CA 0.037, AU 0.034, ZA 0.034, IN 0.027, TZ 0.022, SG 0.017, KE 0.015, LK 0.015, PH 0.010, MY 0.008, NZ 0.007, HK 0.003, PK 0.003, BD 0.002, \n",
      "BD (4): US 0.484, GB 0.288, IN 0.198, BD 0.021, PK 0.004, IE 0.004, ZA 0.001, CA 0.001, PH 0.001, NZ 0.000, LK 0.000, NG 0.000, TZ 0.000, AU 0.000, JM 0.000, SG 0.000, KE 0.000, MY 0.000, GH 0.000, HK 0.000, \n",
      "JM (17): GB 0.345, AU 0.162, ZA 0.153, IE 0.117, CA 0.054, US 0.034, NZ 0.021, LK 0.019, IN 0.016, PH 0.013, TZ 0.009, GH 0.009, KE 0.009, SG 0.008, PK 0.008, NG 0.007, JM 0.007, MY 0.004, BD 0.003, HK 0.003, \n",
      "HK (2): CA 0.216, HK 0.207, US 0.147, GB 0.094, NZ 0.066, SG 0.037, AU 0.035, MY 0.029, TZ 0.029, IN 0.024, PK 0.022, ZA 0.018, IE 0.015, LK 0.014, NG 0.013, BD 0.009, PH 0.008, JM 0.008, GH 0.007, KE 0.002, \n",
      "PK (6): US 0.335, GB 0.250, JM 0.176, NZ 0.081, AU 0.038, PK 0.033, CA 0.023, PH 0.012, IE 0.010, LK 0.009, IN 0.009, BD 0.007, ZA 0.004, SG 0.002, MY 0.002, NG 0.002, TZ 0.001, HK 0.001, KE 0.001, GH 0.001, \n",
      "CA (3): GB 0.386, US 0.192, CA 0.116, IN 0.113, NZ 0.095, AU 0.026, PH 0.010, ZA 0.008, IE 0.007, LK 0.006, JM 0.006, MY 0.005, GH 0.005, TZ 0.004, NG 0.004, PK 0.004, SG 0.003, KE 0.003, BD 0.003, HK 0.003, \n",
      "GB (1): GB 0.382, US 0.272, CA 0.103, IE 0.054, IN 0.036, AU 0.034, PK 0.013, SG 0.013, NZ 0.011, ZA 0.010, LK 0.010, MY 0.010, TZ 0.009, BD 0.009, HK 0.007, NG 0.006, KE 0.006, JM 0.005, PH 0.005, GH 0.005, \n",
      "NG (10): US 0.354, GB 0.329, AU 0.143, NZ 0.068, IN 0.025, CA 0.022, IE 0.013, TZ 0.012, HK 0.006, NG 0.005, SG 0.005, GH 0.003, PK 0.003, BD 0.002, MY 0.002, LK 0.002, PH 0.002, JM 0.002, ZA 0.001, KE 0.001, \n",
      "IE (3): US 0.390, GB 0.172, IE 0.170, IN 0.155, CA 0.068, HK 0.016, NG 0.011, AU 0.007, NZ 0.003, SG 0.002, TZ 0.001, GH 0.001, PK 0.001, ZA 0.000, MY 0.000, PH 0.000, JM 0.000, LK 0.000, KE 0.000, BD 0.000, \n",
      "SG (2): US 0.218, SG 0.148, JM 0.128, LK 0.111, ZA 0.102, GB 0.093, IN 0.082, CA 0.029, IE 0.016, TZ 0.016, PK 0.014, NZ 0.012, NG 0.011, AU 0.006, PH 0.004, BD 0.003, KE 0.003, HK 0.002, MY 0.002, GH 0.001, \n"
     ]
    }
   ],
   "source": [
    "# Multi-layer Perceptron Model removing words that appear in less than 50% of countries\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_50, y, test_size=0.1, random_state=3)\n",
    "mlp_model = MLPClassifier(random_state=3)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "for test_num, probabilities in enumerate(mlp_model.predict_proba(X_test)):\n",
    "    for i, probability in enumerate(probabilities):\n",
    "        most_similar_country[y_test.tolist()[test_num]][mlp_model.classes_[i]] += probability\n",
    "        times_added[y_test.tolist()[test_num]][mlp_model.classes_[i]] += 1  \n",
    "\n",
    "for country in most_similar_country:\n",
    "    print(f\"{country}\",end=\" \")\n",
    "    for cur_country in most_similar_country[country]:\n",
    "        if times_added[country][cur_country] != 0:\n",
    "            most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "        else: \n",
    "            most_similar_country[country][cur_country] = 0\n",
    "    for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "        if country_code==country:\n",
    "            print(f\"({i+1}):\", end=\" \")\n",
    "    for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "        print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
