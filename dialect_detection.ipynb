{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = {}\n",
    "id_dict = {}\n",
    "word_count_dict = {}\n",
    "country_set = set()\n",
    "\n",
    "# pulls text IDs, country codes, document types, and word counts from the excel sheet,\n",
    "# using it to divide the documents into a dictionary by ID\n",
    "sources_df = pd.read_excel(\"./text/sampleSources.xlsx\", sheet_name=\"texts\")\n",
    "for text_id, (country_code, doc_type), word_count in [(l[0], tuple(l[1].split()), l[2]) for l in sources_df[[\"textID\", \"country|genre\", \"# words\"]].values.tolist()]:\n",
    "    with open(f\"./text/w_{country_code.lower()}_{doc_type.lower()}.txt\", 'r',\n",
    "              encoding=\"utf-8\") as file:\n",
    "        # add each text_id to id_dict\n",
    "        if f\"{country_code}_{doc_type}\" not in id_dict:\n",
    "            id_dict[f\"{country_code}_{doc_type}\"] = [text_id]\n",
    "        else:\n",
    "            id_dict[f\"{country_code}_{doc_type}\"].append(text_id)\n",
    "        # makes country code set\n",
    "        country_set.add(country_code)\n",
    "        # finds correct text_id and adds every line in the document to the dictionary\n",
    "        IS_DOC = False\n",
    "        lines = file.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip().startswith(f\"##{text_id}\"):\n",
    "                IS_DOC = True\n",
    "            elif line.strip().startswith(\"##\"):\n",
    "                IS_DOC = False\n",
    "            if IS_DOC:\n",
    "                if text_id not in doc_dict:\n",
    "                    doc_dict[text_id] = [w.lower() for w in line.split()]\n",
    "                else:\n",
    "                    doc_dict[text_id] += [w.lower() for w in line.split()]\n",
    "        # adds word count to dictionary\n",
    "        word_count_dict[text_id] = word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73745\n",
      "5412\n"
     ]
    }
   ],
   "source": [
    "# make a counter for every word in the corpus\n",
    "vocab = Counter({})\n",
    "vocab['<UNK>'] = 0\n",
    "for doc in doc_dict.values():\n",
    "    for word in doc:\n",
    "        if word in vocab:\n",
    "            vocab[word] += 1\n",
    "        else:\n",
    "            vocab[word] = 1\n",
    "\n",
    "# make a dictionary of sets for every country and record every word used by each country\n",
    "# also make a word count for every country\n",
    "vocab_sets = {country_code:set() for country_code in country_set}\n",
    "country_word_counts = Counter({country_code:0 for country_code in country_set})\n",
    "for country_code in country_set:\n",
    "    for text_id in id_dict[f\"{country_code}_B\"] + id_dict[f\"{country_code}_G\"]:\n",
    "        for word in doc_dict[text_id]:\n",
    "            vocab_sets[country_code].add(word)\n",
    "        country_word_counts[country_code] += word_count_dict[text_id]\n",
    "\n",
    "# make new vocabulary sets, removing words that appear in less than\n",
    "# 12.5%, 25%, 37.5%, and 50% of the countries datasets respectively\n",
    "vocab_125 = vocab.copy()\n",
    "vocab_25 = vocab.copy()\n",
    "vocab_375 = vocab.copy()\n",
    "vocab_50 = vocab.copy()\n",
    "for word in vocab:\n",
    "    COUNTRY_COUNT = 0\n",
    "    for country_code in country_set:\n",
    "        if word in vocab_sets[country_code]:\n",
    "            COUNTRY_COUNT+=1\n",
    "    if COUNTRY_COUNT / len(country_set) < 0.125:\n",
    "        del vocab_125[word]\n",
    "    if COUNTRY_COUNT / len(country_set) < 0.25:\n",
    "        del vocab_25[word]\n",
    "    if COUNTRY_COUNT / len(country_set) < 0.375:\n",
    "        del vocab_375[word]\n",
    "    if COUNTRY_COUNT / len(country_set) < 0.5:\n",
    "        del vocab_50[word]\n",
    "\n",
    "# Replace any words that appear in less than 12.5% of the countries’ datasets with the <UNK> token\n",
    "doc_dict_125 = copy.deepcopy(doc_dict)\n",
    "for text_id, doc in doc_dict_125.items():\n",
    "    for i, word in enumerate(doc):\n",
    "        if word not in vocab_125:\n",
    "            doc_dict_125[text_id][i] = '<UNK>'\n",
    "            vocab_125['<UNK>'] += 1\n",
    "\n",
    "# Replace any words that appear in less than 25% of the countries’ datasets with the <UNK> token\n",
    "doc_dict_25 = copy.deepcopy(doc_dict)\n",
    "for text_id, doc in doc_dict_25.items():\n",
    "    for i, word in enumerate(doc):\n",
    "        if word not in vocab_25:\n",
    "            doc_dict_25[text_id][i] = '<UNK>'\n",
    "            vocab_25['<UNK>'] += 1\n",
    "\n",
    "# Replace any words that appear in less than 37.5% of the countries’ datasets with the <UNK> token\n",
    "doc_dict_375 = copy.deepcopy(doc_dict)\n",
    "for text_id, doc in doc_dict_375.items():\n",
    "    for i, word in enumerate(doc):\n",
    "        if word not in vocab_375:\n",
    "            doc_dict_375[text_id][i] = '<UNK>'\n",
    "            vocab_375['<UNK>'] += 1\n",
    "\n",
    "# Replace any words that appear in less than 50% of the countries’ datasets with the <UNK> token\n",
    "doc_dict_50 = copy.deepcopy(doc_dict)\n",
    "for text_id, doc in doc_dict_50.items():\n",
    "    for i, word in enumerate(doc):\n",
    "        if word not in vocab_50:\n",
    "            doc_dict_50[text_id][i] = '<UNK>'\n",
    "            vocab_50['<UNK>'] += 1\n",
    "\n",
    "print(len(vocab))\n",
    "print(len(vocab_50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# make a dataframe of one-hot representations of each text in each version of the dataset with their country labels\n",
    "text_ids = []\n",
    "texts = []\n",
    "texts_125 = []\n",
    "texts_25 = []\n",
    "texts_375 = []\n",
    "texts_50 = []\n",
    "country_labels = []\n",
    "\n",
    "for country_code in country_set:\n",
    "    for text_id in id_dict[f\"{country_code}_B\"] + id_dict[f\"{country_code}_G\"]:\n",
    "        text_ids.append(text_ids)\n",
    "        texts.append(\" \".join(doc_dict[text_id]))\n",
    "        texts_125.append(\" \".join(doc_dict_125[text_id]))\n",
    "        texts_25.append(\" \".join(doc_dict_25[text_id]))\n",
    "        texts_375.append(\" \".join(doc_dict_375[text_id]))\n",
    "        texts_50.append(\" \".join(doc_dict_50[text_id]))\n",
    "        country_labels.append(country_code)\n",
    "\n",
    "data = {\n",
    "    'text_id': text_ids,\n",
    "    'texts': texts,\n",
    "    'texts_125': texts_125,\n",
    "    'texts_25': texts_25,\n",
    "    'texts_375': texts_375,\n",
    "    'texts_50': texts_50,\n",
    "    'country_labels': country_labels\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['texts'])\n",
    "X_125 = vectorizer.fit_transform(df['texts_125'])\n",
    "X_25 = vectorizer.fit_transform(df['texts_25'])\n",
    "X_375 = vectorizer.fit_transform(df['texts_375'])\n",
    "X_50 = vectorizer.fit_transform(df['texts_50'])\n",
    "y = df['country_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({'US': 310, 'GB': 309, 'AU': 117, 'CA': 103, 'IE': 77, 'IN': 76, 'NZ': 64, 'LK': 39, 'PK': 37, 'PH': 36, 'MY': 35, 'SG': 35, 'ZA': 34, 'BD': 33, 'KE': 32, 'GH': 31, 'HK': 31, 'NG': 30, 'JM': 30, 'TZ': 26})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the dataset into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=3)\n",
    "print(\"Original class distribution:\", Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_breakdown(model):\n",
    "    \"\"\"\n",
    "    Prints the average probabilities for each country label on a given country's documents,\n",
    "    highlighting where the country itself ranks in paranthesis.\n",
    "    \"Average tries estimate\" is the estimated average number of tries needed to identify a country correctly.\n",
    "    \"\"\"\n",
    "    most_similar_country = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "    times_added = {c:Counter({c:0 for c in country_set}) for c in country_set}\n",
    "    for test_num, probabilities in enumerate(model.predict_proba(X_test)):\n",
    "        for i, probability in enumerate(probabilities):\n",
    "            most_similar_country[y_test.tolist()[test_num]][model.classes_[i]] += probability\n",
    "            times_added[y_test.tolist()[test_num]][model.classes_[i]] += 1     \n",
    "    total_tries = 0\n",
    "    for country in most_similar_country:\n",
    "        print(f\"{country}\",end=\" \")\n",
    "        for cur_country in most_similar_country[country]:\n",
    "            if times_added[country][cur_country] != 0:\n",
    "                most_similar_country[country][cur_country] = most_similar_country[country][cur_country] / times_added[country][cur_country]\n",
    "            else: \n",
    "                most_similar_country[country][cur_country] = 0\n",
    "        for i, (country_code,prob) in enumerate(most_similar_country[country].most_common(20)):\n",
    "            if country_code==country:\n",
    "                print(f\"({i+1}):\", end=\" \")\n",
    "                total_tries+=(i+1)\n",
    "                break\n",
    "        for cur_country, prob in most_similar_country[country].most_common(20):\n",
    "            print(cur_country, f\"{prob:.3f}\", end=\", \")\n",
    "        print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_tries(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Takes the model and testing data.\n",
    "    Matches each country label to its probability\n",
    "    Sorts each probability distribution for highest probability\n",
    "    records the number of iterations needed to get to the right label\n",
    "\n",
    "    \"\"\"\n",
    "    country_try_count = {c:0 for c in country_set}\n",
    "    country_test_count = {c:0 for c in country_set}\n",
    "    df = pd.DataFrame(model.predict_proba(X_test).tolist(), columns=model.classes_.tolist())\n",
    "    for i in range(len(df)):\n",
    "        for try_count, (country_code, _) in enumerate(sorted(df.iloc[i].to_dict().items(), key=lambda item: item[1], reverse=True)):\n",
    "            if country_code == y_test.tolist()[i]:\n",
    "                country_try_count[country_code] += try_count+1\n",
    "                country_test_count[country_code] += 1\n",
    "                break\n",
    "    country_try_count = {country:country_try_count[country]/country_test_count[country] for country in country_try_count}\n",
    "    \n",
    "    for country, try_count in sorted(country_try_count.items(), key=lambda item: item[1]):\n",
    "        print(country, f\"{try_count:.1f} tries\", f\"({country_test_count[country]} tests)\")\n",
    "\n",
    "    print(\"Average number of tries:\", sum(country_try_count.values())/len(country_try_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28\n",
      "KE (10): US 0.435, GB 0.230, CA 0.116, GH 0.070, JM 0.054, AU 0.027, NG 0.025, NZ 0.008, IE 0.008, KE 0.005, LK 0.005, PK 0.003, ZA 0.003, BD 0.003, MY 0.002, TZ 0.002, PH 0.002, HK 0.001, IN 0.001, SG 0.001, \n",
      "NG (2): GB 0.200, NG 0.157, CA 0.115, KE 0.100, NZ 0.097, TZ 0.078, GH 0.061, US 0.043, PK 0.041, IE 0.019, AU 0.017, IN 0.015, ZA 0.013, HK 0.012, PH 0.009, JM 0.008, SG 0.007, LK 0.004, MY 0.003, BD 0.002, \n",
      "ZA (5): GB 0.175, CA 0.163, NG 0.146, AU 0.124, ZA 0.122, IE 0.067, US 0.060, MY 0.041, NZ 0.033, PK 0.012, BD 0.010, TZ 0.009, SG 0.008, LK 0.008, IN 0.006, GH 0.006, HK 0.004, JM 0.003, KE 0.003, PH 0.001, \n",
      "IE (1): IE 0.322, GB 0.163, US 0.142, PH 0.098, LK 0.072, IN 0.039, TZ 0.033, CA 0.033, ZA 0.028, AU 0.018, NZ 0.009, JM 0.008, PK 0.006, KE 0.005, HK 0.005, BD 0.004, NG 0.004, GH 0.004, SG 0.004, MY 0.003, \n",
      "HK (1): HK 0.304, GB 0.161, JM 0.148, MY 0.147, AU 0.105, CA 0.048, US 0.033, SG 0.013, NZ 0.007, PH 0.007, IE 0.007, LK 0.006, IN 0.005, NG 0.004, KE 0.002, TZ 0.002, BD 0.001, PK 0.001, ZA 0.000, GH 0.000, \n",
      "GB (1): GB 0.351, US 0.171, CA 0.105, AU 0.077, PH 0.057, LK 0.045, IE 0.045, IN 0.027, NZ 0.021, TZ 0.018, ZA 0.015, JM 0.012, MY 0.009, PK 0.008, NG 0.008, SG 0.008, HK 0.007, GH 0.006, KE 0.005, BD 0.005, \n",
      "AU (1): AU 0.261, GB 0.233, US 0.142, JM 0.085, ZA 0.057, CA 0.048, TZ 0.033, IN 0.027, NZ 0.019, IE 0.016, PH 0.015, GH 0.012, NG 0.009, LK 0.009, SG 0.009, MY 0.006, KE 0.005, HK 0.005, PK 0.005, BD 0.003, \n",
      "NZ (1): NZ 0.251, GB 0.230, AU 0.139, IN 0.130, US 0.085, ZA 0.029, PH 0.027, CA 0.024, IE 0.015, PK 0.011, TZ 0.010, KE 0.010, HK 0.009, GH 0.006, MY 0.006, LK 0.006, JM 0.005, SG 0.004, BD 0.003, NG 0.003, \n",
      "SG (8): GB 0.326, US 0.191, CA 0.125, IN 0.065, AU 0.056, PH 0.042, ZA 0.027, SG 0.023, IE 0.023, LK 0.021, JM 0.019, NG 0.013, MY 0.013, NZ 0.012, BD 0.010, TZ 0.009, PK 0.008, GH 0.007, HK 0.007, KE 0.005, \n",
      "BD (3): GB 0.710, IN 0.110, BD 0.048, US 0.038, KE 0.028, PH 0.026, NZ 0.016, CA 0.010, IE 0.006, GH 0.003, AU 0.001, SG 0.001, JM 0.001, LK 0.001, HK 0.001, TZ 0.000, ZA 0.000, MY 0.000, PK 0.000, NG 0.000, \n",
      "TZ (18): GB 0.274, US 0.243, IN 0.109, AU 0.090, PH 0.056, ZA 0.036, NZ 0.028, CA 0.028, JM 0.022, KE 0.022, IE 0.018, GH 0.016, NG 0.013, LK 0.011, HK 0.010, BD 0.009, MY 0.007, TZ 0.006, SG 0.003, PK 0.002, \n",
      "IN (2): GB 0.259, IN 0.119, KE 0.112, PH 0.107, IE 0.096, US 0.076, LK 0.058, CA 0.047, JM 0.039, AU 0.026, TZ 0.022, MY 0.009, GH 0.007, ZA 0.007, NZ 0.005, BD 0.003, NG 0.002, HK 0.002, SG 0.002, PK 0.002, \n",
      "JM (5): AU 0.189, GB 0.176, ZA 0.138, NZ 0.121, JM 0.078, US 0.073, KE 0.047, PK 0.027, LK 0.026, IE 0.023, NG 0.022, CA 0.022, GH 0.011, TZ 0.007, HK 0.007, IN 0.007, BD 0.007, PH 0.006, MY 0.006, SG 0.006, \n",
      "GH (1): GH 0.193, IN 0.178, SG 0.139, KE 0.120, PK 0.065, US 0.064, IE 0.063, GB 0.055, ZA 0.038, TZ 0.024, JM 0.016, HK 0.013, CA 0.009, AU 0.007, NG 0.005, LK 0.005, NZ 0.002, MY 0.002, BD 0.001, PH 0.001, \n",
      "US (1): US 0.369, GB 0.238, AU 0.087, CA 0.059, IE 0.050, LK 0.049, NZ 0.034, JM 0.029, IN 0.019, PK 0.013, PH 0.009, BD 0.007, SG 0.007, MY 0.007, HK 0.007, ZA 0.005, NG 0.004, GH 0.003, KE 0.003, TZ 0.001, \n",
      "PH (19): GB 0.776, CA 0.124, KE 0.050, NG 0.012, SG 0.010, IN 0.005, JM 0.004, HK 0.003, MY 0.003, AU 0.003, GH 0.002, TZ 0.002, NZ 0.001, LK 0.001, US 0.001, BD 0.001, ZA 0.000, IE 0.000, PH 0.000, PK 0.000, \n",
      "CA (3): US 0.258, GB 0.190, CA 0.179, LK 0.068, PH 0.067, ZA 0.054, AU 0.046, IN 0.026, IE 0.025, BD 0.018, HK 0.015, SG 0.011, MY 0.008, JM 0.007, KE 0.006, GH 0.006, NZ 0.005, PK 0.005, NG 0.004, TZ 0.003, \n",
      "LK (1): LK 0.601, AU 0.092, PH 0.056, US 0.053, IE 0.027, GB 0.025, NG 0.024, TZ 0.024, GH 0.022, IN 0.017, NZ 0.014, MY 0.010, ZA 0.009, SG 0.007, PK 0.006, HK 0.004, BD 0.004, CA 0.002, JM 0.002, KE 0.001, \n",
      "MY (15): US 0.364, GB 0.340, CA 0.070, NG 0.044, AU 0.042, SG 0.031, IN 0.022, ZA 0.016, LK 0.012, PH 0.010, IE 0.010, HK 0.008, PK 0.006, NZ 0.005, MY 0.005, KE 0.004, JM 0.004, TZ 0.004, BD 0.003, GH 0.001, \n",
      "PK (4): US 0.239, IN 0.176, AU 0.136, PK 0.136, NG 0.118, IE 0.042, MY 0.035, PH 0.034, GB 0.028, CA 0.023, GH 0.013, HK 0.005, NZ 0.004, LK 0.003, JM 0.002, BD 0.002, SG 0.002, TZ 0.001, ZA 0.001, KE 0.000, \n",
      "\n",
      "LK 1.0 tries (2 tests)\n",
      "GB 2.3 tries (30 tests)\n",
      "US 3.1 tries (28 tests)\n",
      "IE 3.3 tries (11 tests)\n",
      "NZ 3.9 tries (7 tests)\n",
      "CA 4.1 tries (14 tests)\n",
      "AU 4.3 tries (13 tests)\n",
      "BD 4.5 tries (2 tests)\n",
      "GH 4.7 tries (3 tests)\n",
      "ZA 7.3 tries (6 tests)\n",
      "HK 7.5 tries (4 tests)\n",
      "PK 8.8 tries (8 tests)\n",
      "IN 9.2 tries (9 tests)\n",
      "NG 9.3 tries (7 tests)\n",
      "SG 10.0 tries (3 tests)\n",
      "JM 10.4 tries (5 tests)\n",
      "KE 13.5 tries (4 tests)\n",
      "TZ 13.6 tries (5 tests)\n",
      "MY 14.0 tries (2 tests)\n",
      "PH 17.0 tries (2 tests)\n",
      "Average number of tries: 7.582036852036852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression model with class weights balanced\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=500, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(model)\n",
    "average_tries(model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35\n",
      "KE (3): GB 0.255, US 0.215, KE 0.080, CA 0.062, AU 0.052, IE 0.050, BD 0.028, TZ 0.028, IN 0.025, ZA 0.022, GH 0.022, PK 0.022, JM 0.020, LK 0.020, HK 0.018, NZ 0.018, MY 0.018, NG 0.017, SG 0.015, PH 0.013, \n",
      "NG (3): GB 0.207, US 0.183, NG 0.161, CA 0.077, AU 0.049, IN 0.049, HK 0.039, IE 0.036, NZ 0.029, KE 0.027, PK 0.023, ZA 0.021, TZ 0.021, LK 0.017, MY 0.017, SG 0.016, GH 0.011, PH 0.009, BD 0.006, JM 0.003, \n",
      "ZA (8): US 0.272, GB 0.213, CA 0.078, AU 0.062, IN 0.057, IE 0.038, NZ 0.037, ZA 0.032, PH 0.027, HK 0.025, SG 0.023, LK 0.020, KE 0.018, GH 0.018, NG 0.017, MY 0.015, TZ 0.013, PK 0.013, BD 0.012, JM 0.010, \n",
      "IE (2): GB 0.233, IE 0.207, US 0.205, AU 0.065, CA 0.045, IN 0.039, ZA 0.027, NZ 0.025, SG 0.024, HK 0.016, BD 0.015, TZ 0.015, MY 0.015, LK 0.014, PK 0.011, KE 0.010, PH 0.009, JM 0.009, GH 0.009, NG 0.007, \n",
      "HK (4): US 0.227, GB 0.220, CA 0.083, HK 0.068, AU 0.068, IE 0.048, NZ 0.033, PK 0.033, SG 0.028, JM 0.028, BD 0.025, IN 0.025, TZ 0.020, PH 0.020, MY 0.020, GH 0.015, ZA 0.015, KE 0.010, LK 0.010, NG 0.007, \n",
      "GB (1): GB 0.297, US 0.288, AU 0.072, CA 0.064, IE 0.044, NZ 0.037, IN 0.031, PK 0.016, ZA 0.016, SG 0.016, PH 0.015, JM 0.015, KE 0.014, MY 0.014, GH 0.012, TZ 0.011, LK 0.011, BD 0.011, NG 0.010, HK 0.007, \n",
      "AU (3): GB 0.231, US 0.213, AU 0.149, CA 0.055, NZ 0.045, IE 0.042, IN 0.028, BD 0.028, PK 0.028, ZA 0.020, JM 0.020, KE 0.019, PH 0.018, HK 0.017, NG 0.016, SG 0.016, GH 0.015, LK 0.014, MY 0.013, TZ 0.013, \n",
      "NZ (1): NZ 0.221, GB 0.216, US 0.181, AU 0.099, IE 0.053, CA 0.051, IN 0.026, KE 0.016, BD 0.016, HK 0.014, PH 0.013, LK 0.013, ZA 0.013, NG 0.011, SG 0.011, PK 0.011, TZ 0.011, JM 0.010, MY 0.007, GH 0.006, \n",
      "SG (7): US 0.383, GB 0.333, CA 0.067, AU 0.053, IE 0.033, JM 0.023, SG 0.020, HK 0.017, BD 0.013, IN 0.013, LK 0.010, NG 0.007, ZA 0.007, MY 0.007, PK 0.007, TZ 0.003, PH 0.003, KE 0.000, NZ 0.000, GH 0.000, \n",
      "BD (2): GB 0.290, BD 0.155, US 0.145, AU 0.055, IN 0.055, CA 0.050, NZ 0.045, IE 0.030, SG 0.020, GH 0.020, PH 0.020, MY 0.020, KE 0.015, NG 0.015, TZ 0.015, LK 0.015, PK 0.015, ZA 0.010, HK 0.010, JM 0.000, \n",
      "TZ (4): GB 0.220, US 0.192, ZA 0.074, TZ 0.074, AU 0.054, KE 0.050, IE 0.044, GH 0.044, CA 0.044, IN 0.034, NZ 0.026, NG 0.024, LK 0.024, BD 0.020, PH 0.018, SG 0.018, HK 0.014, MY 0.012, PK 0.008, JM 0.006, \n",
      "IN (3): US 0.237, GB 0.229, IN 0.096, AU 0.067, CA 0.051, BD 0.036, LK 0.034, IE 0.033, NZ 0.032, PK 0.029, MY 0.026, JM 0.023, PH 0.019, HK 0.018, SG 0.017, GH 0.016, KE 0.014, ZA 0.013, TZ 0.008, NG 0.003, \n",
      "JM (3): US 0.256, GB 0.228, JM 0.072, CA 0.064, AU 0.054, NZ 0.050, HK 0.046, IE 0.040, ZA 0.034, IN 0.030, PH 0.024, NG 0.022, KE 0.020, GH 0.012, MY 0.012, SG 0.010, TZ 0.010, LK 0.010, BD 0.004, PK 0.002, \n",
      "GH (2): US 0.193, GH 0.153, GB 0.147, KE 0.083, AU 0.063, IE 0.050, HK 0.043, TZ 0.040, ZA 0.033, LK 0.030, PH 0.030, MY 0.023, NZ 0.020, JM 0.017, CA 0.017, SG 0.013, IN 0.013, NG 0.010, BD 0.010, PK 0.010, \n",
      "US (1): US 0.272, GB 0.226, CA 0.081, AU 0.059, IN 0.040, IE 0.038, NZ 0.031, PK 0.028, BD 0.023, PH 0.021, LK 0.021, TZ 0.020, ZA 0.019, NG 0.019, KE 0.018, SG 0.018, MY 0.018, JM 0.017, GH 0.017, HK 0.016, \n",
      "PH (18): GB 0.315, US 0.210, AU 0.115, IN 0.080, CA 0.050, NZ 0.040, ZA 0.025, TZ 0.025, KE 0.020, NG 0.020, MY 0.020, HK 0.015, LK 0.015, IE 0.010, SG 0.010, JM 0.010, GH 0.010, PH 0.005, PK 0.005, BD 0.000, \n",
      "CA (3): US 0.276, GB 0.251, CA 0.116, AU 0.072, IE 0.037, IN 0.031, NZ 0.028, SG 0.026, PH 0.021, JM 0.020, HK 0.019, ZA 0.018, GH 0.016, PK 0.013, KE 0.012, BD 0.012, LK 0.010, MY 0.009, NG 0.008, TZ 0.006, \n",
      "LK (1): LK 0.320, US 0.155, GB 0.120, IN 0.090, AU 0.065, BD 0.060, PK 0.045, NZ 0.020, CA 0.020, MY 0.020, NG 0.015, ZA 0.015, PH 0.015, IE 0.010, HK 0.010, JM 0.010, KE 0.005, TZ 0.005, SG 0.000, GH 0.000, \n",
      "MY (17): US 0.315, GB 0.220, IN 0.060, IE 0.050, AU 0.050, CA 0.045, ZA 0.040, JM 0.035, NZ 0.035, PH 0.025, NG 0.020, KE 0.015, HK 0.015, SG 0.015, TZ 0.015, GH 0.015, MY 0.010, PK 0.010, BD 0.005, LK 0.005, \n",
      "PK (3): GB 0.220, US 0.154, PK 0.146, CA 0.073, AU 0.050, IN 0.049, BD 0.043, IE 0.030, NZ 0.029, KE 0.026, SG 0.025, LK 0.024, TZ 0.024, HK 0.021, MY 0.020, ZA 0.016, NG 0.015, JM 0.014, PH 0.014, GH 0.009, \n",
      "\n",
      "LK 1.0 tries (2 tests)\n",
      "US 1.5 tries (28 tests)\n",
      "GB 1.5 tries (30 tests)\n",
      "AU 2.8 tries (13 tests)\n",
      "IE 3.0 tries (11 tests)\n",
      "CA 3.3 tries (14 tests)\n",
      "GH 3.7 tries (3 tests)\n",
      "NZ 4.7 tries (7 tests)\n",
      "PK 5.4 tries (8 tests)\n",
      "IN 6.7 tries (9 tests)\n",
      "TZ 6.8 tries (5 tests)\n",
      "KE 7.5 tries (4 tests)\n",
      "BD 8.0 tries (2 tests)\n",
      "NG 8.4 tries (7 tests)\n",
      "JM 8.6 tries (5 tests)\n",
      "HK 8.8 tries (4 tests)\n",
      "SG 12.3 tries (3 tests)\n",
      "ZA 12.7 tries (6 tests)\n",
      "MY 15.0 tries (2 tests)\n",
      "PH 17.5 tries (2 tests)\n",
      "Average number of tries: 6.954473443223444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Classifier with class weights balanced\n",
    "rf_model = RandomForestClassifier(random_state=3, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(rf_model)\n",
    "average_tries(rf_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.32\n",
      "KE (8): US 0.514, GB 0.437, CA 0.031, AU 0.008, IN 0.003, NZ 0.002, IE 0.002, KE 0.001, BD 0.001, TZ 0.000, JM 0.000, PK 0.000, PH 0.000, MY 0.000, ZA 0.000, HK 0.000, NG 0.000, GH 0.000, SG 0.000, LK 0.000, \n",
      "NG (17): GB 0.485, CA 0.202, US 0.173, IN 0.038, NZ 0.014, AU 0.014, JM 0.012, PH 0.007, TZ 0.007, BD 0.006, MY 0.006, HK 0.006, KE 0.005, LK 0.005, ZA 0.005, IE 0.004, NG 0.004, SG 0.003, PK 0.003, GH 0.001, \n",
      "ZA (5): US 0.334, CA 0.232, AU 0.157, GB 0.132, ZA 0.040, IE 0.022, NZ 0.014, BD 0.013, IN 0.012, MY 0.009, PH 0.008, HK 0.007, TZ 0.007, JM 0.005, KE 0.004, PK 0.002, LK 0.001, NG 0.000, SG 0.000, GH 0.000, \n",
      "IE (1): IE 0.345, GB 0.249, US 0.195, CA 0.054, BD 0.019, AU 0.018, PH 0.018, TZ 0.017, IN 0.015, NZ 0.015, LK 0.010, JM 0.009, HK 0.007, PK 0.006, KE 0.006, MY 0.004, ZA 0.003, GH 0.003, SG 0.003, NG 0.002, \n",
      "HK (2): US 0.343, HK 0.279, IN 0.132, AU 0.068, GB 0.053, CA 0.035, PH 0.022, SG 0.013, JM 0.010, MY 0.009, KE 0.008, NZ 0.006, BD 0.005, TZ 0.005, IE 0.004, LK 0.003, PK 0.002, ZA 0.002, NG 0.001, GH 0.001, \n",
      "GB (1): GB 0.447, US 0.198, CA 0.072, AU 0.071, IE 0.036, IN 0.027, PH 0.024, TZ 0.015, NZ 0.014, JM 0.013, PK 0.012, BD 0.011, ZA 0.010, MY 0.010, HK 0.010, KE 0.009, LK 0.007, NG 0.007, SG 0.005, GH 0.003, \n",
      "AU (1): AU 0.346, US 0.293, GB 0.246, CA 0.052, NZ 0.016, JM 0.009, PK 0.006, PH 0.005, IE 0.005, BD 0.004, TZ 0.004, KE 0.004, IN 0.003, ZA 0.001, HK 0.001, SG 0.001, MY 0.001, LK 0.001, NG 0.000, GH 0.000, \n",
      "NZ (4): GB 0.367, US 0.187, AU 0.132, NZ 0.106, IN 0.071, CA 0.027, TZ 0.020, PH 0.015, BD 0.014, KE 0.011, JM 0.010, IE 0.010, ZA 0.008, HK 0.005, SG 0.004, PK 0.004, MY 0.003, NG 0.002, GH 0.002, LK 0.001, \n",
      "SG (15): CA 0.255, AU 0.123, JM 0.085, GB 0.083, US 0.081, IN 0.042, PH 0.037, BD 0.036, HK 0.030, TZ 0.030, MY 0.027, IE 0.024, KE 0.022, NZ 0.021, SG 0.021, ZA 0.021, LK 0.019, PK 0.018, NG 0.014, GH 0.010, \n",
      "BD (4): GB 0.908, US 0.048, CA 0.021, BD 0.009, IN 0.006, AU 0.002, PH 0.002, MY 0.002, JM 0.001, PK 0.001, IE 0.001, TZ 0.000, ZA 0.000, KE 0.000, HK 0.000, NZ 0.000, NG 0.000, SG 0.000, LK 0.000, GH 0.000, \n",
      "TZ (5): GB 0.491, US 0.412, IN 0.027, AU 0.020, TZ 0.016, IE 0.012, PH 0.008, NZ 0.003, HK 0.003, BD 0.002, JM 0.002, CA 0.002, PK 0.001, KE 0.001, ZA 0.001, MY 0.000, GH 0.000, SG 0.000, NG 0.000, LK 0.000, \n",
      "IN (3): GB 0.322, US 0.240, IN 0.126, CA 0.120, JM 0.047, IE 0.044, AU 0.040, PH 0.012, NZ 0.011, BD 0.011, HK 0.007, TZ 0.006, MY 0.004, PK 0.004, LK 0.002, ZA 0.001, KE 0.001, GH 0.001, SG 0.000, NG 0.000, \n",
      "JM (6): CA 0.290, GB 0.202, AU 0.175, NZ 0.082, PK 0.062, JM 0.061, US 0.040, BD 0.023, ZA 0.014, HK 0.010, IN 0.008, TZ 0.006, NG 0.005, IE 0.005, MY 0.005, PH 0.004, GH 0.003, KE 0.003, LK 0.002, SG 0.001, \n",
      "GH (4): CA 0.334, IN 0.282, GB 0.239, GH 0.069, AU 0.037, US 0.015, JM 0.006, TZ 0.004, KE 0.003, PK 0.002, IE 0.002, BD 0.002, NZ 0.001, HK 0.001, ZA 0.001, NG 0.000, SG 0.000, LK 0.000, PH 0.000, MY 0.000, \n",
      "US (1): US 0.462, GB 0.237, AU 0.112, CA 0.087, IN 0.011, JM 0.011, PH 0.010, NZ 0.008, IE 0.008, HK 0.008, MY 0.007, BD 0.007, KE 0.006, TZ 0.005, SG 0.004, ZA 0.004, LK 0.004, PK 0.003, NG 0.003, GH 0.002, \n",
      "PH (6): GB 0.946, IN 0.016, US 0.015, CA 0.011, AU 0.002, PH 0.002, NZ 0.001, KE 0.001, TZ 0.001, JM 0.001, NG 0.001, PK 0.001, BD 0.000, ZA 0.000, IE 0.000, SG 0.000, MY 0.000, HK 0.000, GH 0.000, LK 0.000, \n",
      "CA (2): US 0.384, CA 0.305, GB 0.202, AU 0.058, JM 0.008, PH 0.007, IE 0.007, NZ 0.006, IN 0.005, BD 0.004, HK 0.003, MY 0.002, ZA 0.002, KE 0.002, PK 0.002, TZ 0.001, SG 0.001, LK 0.001, NG 0.001, GH 0.000, \n",
      "LK (1): LK 0.281, US 0.274, AU 0.256, BD 0.062, GB 0.034, TZ 0.034, PH 0.027, CA 0.007, NZ 0.005, IN 0.005, JM 0.004, PK 0.003, IE 0.002, ZA 0.002, HK 0.001, MY 0.001, GH 0.001, SG 0.000, NG 0.000, KE 0.000, \n",
      "MY (5): US 0.562, GB 0.297, IN 0.030, JM 0.016, MY 0.016, CA 0.013, AU 0.013, ZA 0.008, PK 0.007, PH 0.007, SG 0.006, HK 0.005, KE 0.005, BD 0.004, NG 0.003, TZ 0.003, IE 0.002, NZ 0.002, LK 0.001, GH 0.001, \n",
      "PK (2): US 0.338, PK 0.236, IN 0.129, AU 0.114, CA 0.059, GB 0.037, ZA 0.013, PH 0.012, IE 0.011, BD 0.009, MY 0.008, JM 0.006, NZ 0.006, HK 0.006, TZ 0.004, LK 0.004, KE 0.002, SG 0.002, GH 0.001, NG 0.001, \n",
      "\n",
      "GB 2.0 tries (30 tests)\n",
      "US 2.3 tries (28 tests)\n",
      "CA 2.7 tries (14 tests)\n",
      "IE 3.2 tries (11 tests)\n",
      "AU 3.2 tries (13 tests)\n",
      "NZ 3.6 tries (7 tests)\n",
      "TZ 3.8 tries (5 tests)\n",
      "LK 4.5 tries (2 tests)\n",
      "PH 5.0 tries (2 tests)\n",
      "MY 5.0 tries (2 tests)\n",
      "JM 5.2 tries (5 tests)\n",
      "HK 7.0 tries (4 tests)\n",
      "BD 7.0 tries (2 tests)\n",
      "PK 7.9 tries (8 tests)\n",
      "ZA 8.8 tries (6 tests)\n",
      "GH 10.3 tries (3 tests)\n",
      "IN 11.0 tries (9 tests)\n",
      "KE 11.8 tries (4 tests)\n",
      "NG 12.1 tries (7 tests)\n",
      "SG 15.0 tries (3 tests)\n",
      "Average number of tries: 6.570926989676989\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Multi-layer Perceptron Model\n",
    "mlp_model = MLPClassifier(random_state=3)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(mlp_model)\n",
    "average_tries(mlp_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled class distribution: Counter({'AU': 26, 'BD': 26, 'CA': 26, 'GB': 26, 'GH': 26, 'HK': 26, 'IE': 26, 'IN': 26, 'JM': 26, 'KE': 26, 'LK': 26, 'MY': 26, 'NG': 26, 'NZ': 26, 'PH': 26, 'PK': 26, 'SG': 26, 'TZ': 26, 'US': 26, 'ZA': 26})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Even when class weights are balanced, the bigger countries are heavily favored\n",
    "# so we can try undersampling the dataset so everything is equal\n",
    "rus = RandomUnderSampler(random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=3)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "print(\"Resampled class distribution:\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.15\n",
      "KE (8): CA 0.264, IE 0.108, GH 0.096, NG 0.081, US 0.072, JM 0.068, AU 0.066, KE 0.040, ZA 0.039, PK 0.031, NZ 0.022, BD 0.021, TZ 0.020, MY 0.018, LK 0.017, PH 0.013, GB 0.010, IN 0.006, SG 0.005, HK 0.005, \n",
      "NG (1): NG 0.171, NZ 0.139, KE 0.137, CA 0.115, PK 0.095, GB 0.067, TZ 0.065, AU 0.050, HK 0.023, IN 0.022, ZA 0.019, PH 0.019, JM 0.017, GH 0.017, US 0.014, SG 0.009, IE 0.008, MY 0.006, BD 0.006, LK 0.003, \n",
      "ZA (4): NG 0.195, MY 0.147, TZ 0.083, ZA 0.080, IE 0.061, SG 0.059, US 0.056, GB 0.042, LK 0.034, KE 0.034, HK 0.031, NZ 0.030, GH 0.026, CA 0.026, JM 0.022, AU 0.022, IN 0.016, PK 0.014, BD 0.013, PH 0.010, \n",
      "IE (2): ZA 0.159, IE 0.147, LK 0.120, PH 0.117, TZ 0.079, GB 0.061, BD 0.048, CA 0.047, NZ 0.029, NG 0.028, JM 0.027, PK 0.023, US 0.022, IN 0.021, GH 0.015, AU 0.014, HK 0.013, SG 0.011, KE 0.011, MY 0.010, \n",
      "HK (1): HK 0.305, MY 0.208, JM 0.171, GB 0.094, US 0.063, NZ 0.050, SG 0.028, AU 0.025, PH 0.015, CA 0.014, NG 0.007, KE 0.004, IN 0.004, TZ 0.004, PK 0.002, BD 0.002, ZA 0.001, IE 0.001, LK 0.001, GH 0.001, \n",
      "GB (1): GB 0.121, PH 0.089, ZA 0.078, US 0.075, CA 0.073, AU 0.061, TZ 0.053, LK 0.051, IN 0.050, NG 0.042, PK 0.041, IE 0.040, KE 0.038, GH 0.033, JM 0.032, NZ 0.031, MY 0.027, SG 0.026, HK 0.021, BD 0.016, \n",
      "AU (1): AU 0.192, US 0.116, GB 0.099, JM 0.086, TZ 0.074, ZA 0.073, NG 0.051, BD 0.040, PH 0.037, NZ 0.036, HK 0.033, MY 0.031, CA 0.024, SG 0.024, PK 0.023, GH 0.016, IE 0.015, KE 0.012, LK 0.011, IN 0.009, \n",
      "NZ (6): PH 0.170, ZA 0.133, IN 0.116, TZ 0.102, GB 0.071, NZ 0.071, AU 0.059, CA 0.042, KE 0.040, PK 0.031, HK 0.031, GH 0.024, IE 0.017, JM 0.017, US 0.016, SG 0.016, MY 0.013, NG 0.012, BD 0.010, LK 0.010, \n",
      "SG (2): PH 0.115, SG 0.104, ZA 0.103, GB 0.093, JM 0.067, NZ 0.063, IN 0.063, AU 0.055, US 0.044, LK 0.037, MY 0.035, NG 0.034, CA 0.032, IE 0.031, PK 0.026, TZ 0.026, BD 0.022, GH 0.021, HK 0.016, KE 0.012, \n",
      "BD (3): NZ 0.264, IN 0.178, BD 0.156, CA 0.108, GB 0.072, US 0.052, PH 0.047, KE 0.029, GH 0.026, LK 0.014, TZ 0.010, JM 0.009, IE 0.009, AU 0.007, SG 0.006, HK 0.003, MY 0.003, ZA 0.003, PK 0.002, NG 0.001, \n",
      "TZ (16): AU 0.110, KE 0.096, GB 0.083, CA 0.081, GH 0.069, ZA 0.067, NZ 0.064, IE 0.060, IN 0.048, PH 0.048, HK 0.045, BD 0.044, MY 0.043, JM 0.037, NG 0.030, TZ 0.026, LK 0.021, US 0.012, SG 0.008, PK 0.007, \n",
      "IN (1): IN 0.139, KE 0.118, PH 0.105, GH 0.086, TZ 0.080, ZA 0.072, IE 0.070, GB 0.070, CA 0.052, BD 0.041, LK 0.032, MY 0.030, JM 0.029, NZ 0.016, PK 0.013, SG 0.011, NG 0.010, US 0.009, AU 0.009, HK 0.009, \n",
      "JM (3): ZA 0.219, IE 0.138, JM 0.110, AU 0.081, NZ 0.074, NG 0.058, PK 0.048, GB 0.041, KE 0.038, HK 0.036, GH 0.027, LK 0.024, TZ 0.021, IN 0.018, PH 0.016, BD 0.014, CA 0.012, MY 0.009, US 0.008, SG 0.007, \n",
      "GH (1): GH 0.261, KE 0.249, CA 0.159, PK 0.072, TZ 0.053, JM 0.039, IE 0.025, AU 0.022, ZA 0.022, NG 0.014, HK 0.013, SG 0.012, GB 0.011, US 0.010, BD 0.008, LK 0.008, MY 0.006, NZ 0.005, IN 0.005, PH 0.004, \n",
      "US (8): NZ 0.085, GB 0.081, CA 0.078, ZA 0.075, NG 0.068, AU 0.065, HK 0.062, US 0.059, BD 0.058, LK 0.058, JM 0.050, IE 0.035, GH 0.035, PH 0.033, IN 0.031, PK 0.029, KE 0.028, MY 0.027, TZ 0.023, SG 0.019, \n",
      "PH (8): JM 0.243, TZ 0.099, NG 0.095, KE 0.077, US 0.072, GH 0.066, NZ 0.065, PH 0.061, IN 0.044, MY 0.038, SG 0.034, AU 0.024, GB 0.018, HK 0.013, CA 0.011, ZA 0.011, BD 0.011, LK 0.010, PK 0.003, IE 0.003, \n",
      "CA (1): CA 0.133, ZA 0.123, US 0.121, PH 0.100, LK 0.092, GH 0.068, SG 0.048, JM 0.038, IE 0.035, IN 0.032, AU 0.032, NZ 0.029, BD 0.026, GB 0.023, NG 0.022, MY 0.018, HK 0.017, KE 0.017, TZ 0.016, PK 0.011, \n",
      "LK (1): LK 0.432, GB 0.191, HK 0.073, NG 0.047, GH 0.042, IE 0.037, JM 0.032, TZ 0.027, NZ 0.020, AU 0.019, PH 0.018, CA 0.013, PK 0.013, ZA 0.010, MY 0.009, SG 0.006, IN 0.005, BD 0.003, KE 0.002, US 0.001, \n",
      "MY (15): NG 0.177, GB 0.094, IN 0.085, ZA 0.080, NZ 0.072, CA 0.070, SG 0.051, HK 0.044, US 0.043, PH 0.040, IE 0.036, LK 0.035, PK 0.032, AU 0.031, MY 0.029, KE 0.028, TZ 0.019, BD 0.018, JM 0.011, GH 0.004, \n",
      "PK (1): PK 0.192, MY 0.172, NG 0.112, TZ 0.084, PH 0.063, GH 0.053, CA 0.052, BD 0.045, HK 0.034, US 0.032, LK 0.029, NZ 0.029, IE 0.025, SG 0.021, AU 0.018, GB 0.012, JM 0.011, IN 0.006, ZA 0.006, KE 0.004, \n",
      "\n",
      "LK 1.5 tries (2 tests)\n",
      "BD 2.0 tries (2 tests)\n",
      "GH 4.7 tries (3 tests)\n",
      "IE 4.7 tries (11 tests)\n",
      "ZA 5.2 tries (6 tests)\n",
      "SG 5.3 tries (3 tests)\n",
      "AU 5.4 tries (13 tests)\n",
      "NZ 6.4 tries (7 tests)\n",
      "HK 6.5 tries (4 tests)\n",
      "CA 6.5 tries (14 tests)\n",
      "GB 7.3 tries (30 tests)\n",
      "US 7.5 tries (28 tests)\n",
      "PK 8.0 tries (8 tests)\n",
      "JM 8.4 tries (5 tests)\n",
      "IN 9.4 tries (9 tests)\n",
      "NG 9.9 tries (7 tests)\n",
      "TZ 10.0 tries (5 tests)\n",
      "PH 11.0 tries (2 tests)\n",
      "MY 12.0 tries (2 tests)\n",
      "KE 13.0 tries (4 tests)\n",
      "Average number of tries: 7.231983294483294\n"
     ]
    }
   ],
   "source": [
    "# Trains a logistic regression model with undersampling\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(model)\n",
    "average_tries(model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22\n",
      "KE (1): KE 0.103, US 0.073, NG 0.068, TZ 0.060, JM 0.060, PH 0.058, CA 0.055, ZA 0.053, SG 0.053, IE 0.052, GH 0.050, BD 0.048, NZ 0.040, IN 0.040, AU 0.038, GB 0.035, LK 0.035, HK 0.033, MY 0.028, PK 0.022, \n",
      "NG (1): NG 0.131, GH 0.060, US 0.056, JM 0.053, HK 0.053, MY 0.053, ZA 0.050, IE 0.049, SG 0.049, CA 0.049, NZ 0.047, BD 0.047, KE 0.043, GB 0.043, PK 0.041, TZ 0.040, IN 0.037, AU 0.036, PH 0.036, LK 0.029, \n",
      "ZA (8): NZ 0.070, SG 0.063, PH 0.063, AU 0.058, HK 0.057, IE 0.055, KE 0.053, ZA 0.053, US 0.053, IN 0.052, GH 0.048, CA 0.048, TZ 0.047, LK 0.047, GB 0.045, BD 0.042, PK 0.040, MY 0.038, JM 0.035, NG 0.032, \n",
      "IE (1): IE 0.108, AU 0.068, ZA 0.068, JM 0.061, CA 0.057, NG 0.054, GB 0.054, IN 0.047, KE 0.046, PH 0.046, LK 0.046, US 0.045, TZ 0.044, PK 0.042, NZ 0.038, BD 0.038, GH 0.037, HK 0.035, MY 0.035, SG 0.031, \n",
      "HK (1): HK 0.100, PH 0.085, PK 0.062, AU 0.060, TZ 0.060, IN 0.060, NZ 0.055, SG 0.055, BD 0.053, CA 0.052, GH 0.050, IE 0.045, KE 0.040, US 0.040, GB 0.038, ZA 0.035, NG 0.033, LK 0.028, MY 0.028, JM 0.023, \n",
      "GB (4): PH 0.082, US 0.071, ZA 0.065, GB 0.062, CA 0.054, AU 0.053, IE 0.052, JM 0.052, TZ 0.052, SG 0.048, IN 0.047, MY 0.044, NG 0.043, KE 0.042, PK 0.042, NZ 0.041, HK 0.039, GH 0.038, BD 0.038, LK 0.034, \n",
      "AU (1): AU 0.084, GB 0.060, CA 0.059, KE 0.057, US 0.057, PH 0.055, BD 0.055, HK 0.052, TZ 0.052, JM 0.050, MY 0.048, NG 0.045, SG 0.045, IE 0.044, NZ 0.044, PK 0.044, GH 0.041, ZA 0.038, LK 0.036, IN 0.035, \n",
      "NZ (1): NZ 0.093, PH 0.069, ZA 0.067, GB 0.066, SG 0.060, IN 0.056, US 0.053, HK 0.051, AU 0.051, TZ 0.050, CA 0.047, JM 0.046, KE 0.044, IE 0.040, GH 0.039, MY 0.039, NG 0.034, PK 0.034, BD 0.031, LK 0.030, \n",
      "SG (6): ZA 0.107, PH 0.097, JM 0.077, GB 0.070, US 0.070, SG 0.067, IN 0.067, MY 0.053, AU 0.050, NG 0.047, PK 0.047, HK 0.043, LK 0.037, CA 0.033, IE 0.030, KE 0.027, TZ 0.027, GH 0.027, BD 0.023, NZ 0.003, \n",
      "BD (1): BD 0.125, GB 0.080, PH 0.080, NZ 0.060, ZA 0.055, US 0.055, GH 0.050, AU 0.045, KE 0.045, HK 0.045, SG 0.045, TZ 0.040, CA 0.040, PK 0.040, NG 0.035, IN 0.035, JM 0.035, LK 0.035, MY 0.035, IE 0.020, \n",
      "TZ (1): TZ 0.112, ZA 0.078, NG 0.076, KE 0.064, IE 0.052, NZ 0.050, GH 0.050, IN 0.048, MY 0.048, CA 0.046, BD 0.042, JM 0.042, LK 0.042, GB 0.040, AU 0.040, US 0.040, PK 0.036, SG 0.032, HK 0.032, PH 0.030, \n",
      "IN (1): IN 0.067, BD 0.064, US 0.063, HK 0.060, GB 0.060, LK 0.059, NZ 0.057, CA 0.054, KE 0.052, SG 0.050, AU 0.049, IE 0.048, PK 0.047, PH 0.043, NG 0.042, ZA 0.040, MY 0.039, JM 0.038, TZ 0.034, GH 0.033, \n",
      "JM (1): JM 0.088, HK 0.072, ZA 0.070, KE 0.068, IE 0.066, NG 0.058, PH 0.054, MY 0.054, TZ 0.052, GB 0.050, US 0.050, NZ 0.048, IN 0.046, SG 0.044, GH 0.044, LK 0.034, PK 0.034, BD 0.026, CA 0.022, AU 0.020, \n",
      "GH (1): GH 0.127, TZ 0.103, PH 0.060, GB 0.057, JM 0.053, CA 0.053, HK 0.050, KE 0.050, LK 0.047, MY 0.047, PK 0.047, ZA 0.043, IE 0.043, AU 0.037, SG 0.037, US 0.037, NG 0.033, NZ 0.033, IN 0.027, BD 0.017, \n",
      "US (1): US 0.062, IN 0.061, PH 0.058, CA 0.058, ZA 0.053, JM 0.053, AU 0.053, GB 0.053, NZ 0.051, PK 0.050, KE 0.049, IE 0.048, SG 0.048, GH 0.047, TZ 0.047, HK 0.045, BD 0.045, NG 0.043, LK 0.040, MY 0.039, \n",
      "PH (2): GB 0.085, PH 0.070, AU 0.065, JM 0.065, US 0.065, TZ 0.060, BD 0.060, SG 0.055, ZA 0.050, IE 0.050, GH 0.045, CA 0.045, KE 0.040, NZ 0.040, LK 0.040, PK 0.040, HK 0.035, NG 0.030, IN 0.030, MY 0.030, \n",
      "CA (3): KE 0.066, HK 0.064, CA 0.061, US 0.059, NZ 0.058, JM 0.056, SG 0.054, GB 0.053, PH 0.051, ZA 0.050, AU 0.049, IE 0.045, GH 0.044, MY 0.044, BD 0.044, TZ 0.044, LK 0.042, IN 0.041, NG 0.041, PK 0.034, \n",
      "LK (1): LK 0.275, PK 0.075, IN 0.065, BD 0.060, PH 0.050, CA 0.045, TZ 0.045, US 0.045, IE 0.040, GB 0.040, NZ 0.035, AU 0.030, SG 0.030, KE 0.030, MY 0.030, ZA 0.025, GH 0.025, NG 0.020, JM 0.020, HK 0.015, \n",
      "MY (8): NG 0.100, ZA 0.075, PH 0.075, TZ 0.065, HK 0.060, US 0.060, JM 0.060, MY 0.060, IE 0.055, GH 0.050, AU 0.045, KE 0.040, NZ 0.040, CA 0.040, GB 0.030, SG 0.030, BD 0.030, IN 0.030, LK 0.030, PK 0.025, \n",
      "PK (1): PK 0.128, HK 0.069, SG 0.064, IN 0.058, NZ 0.050, PH 0.050, AU 0.050, NG 0.049, KE 0.046, IE 0.046, MY 0.044, TZ 0.044, GB 0.043, ZA 0.042, GH 0.041, LK 0.039, JM 0.038, BD 0.036, CA 0.035, US 0.030, \n",
      "\n",
      "LK 1.0 tries (2 tests)\n",
      "TZ 2.4 tries (5 tests)\n",
      "GH 3.3 tries (3 tests)\n",
      "IE 3.9 tries (11 tests)\n",
      "KE 4.5 tries (4 tests)\n",
      "AU 4.5 tries (13 tests)\n",
      "NZ 5.4 tries (7 tests)\n",
      "HK 5.8 tries (4 tests)\n",
      "NG 5.9 tries (7 tests)\n",
      "PH 6.0 tries (2 tests)\n",
      "MY 6.0 tries (2 tests)\n",
      "JM 6.2 tries (5 tests)\n",
      "SG 6.3 tries (3 tests)\n",
      "BD 7.5 tries (2 tests)\n",
      "PK 7.5 tries (8 tests)\n",
      "GB 7.6 tries (30 tests)\n",
      "US 7.9 tries (28 tests)\n",
      "CA 8.6 tries (14 tests)\n",
      "ZA 9.3 tries (6 tests)\n",
      "IN 10.4 tries (9 tests)\n",
      "Average number of tries: 6.006266511266512\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier with undersampling\n",
    "rf_model = RandomForestClassifier(random_state=3)\n",
    "rf_model.fit(X_resampled, y_resampled)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(rf_model)\n",
    "average_tries(rf_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.21\n",
      "KE (9): PH 0.277, TZ 0.222, BD 0.061, CA 0.050, AU 0.046, GH 0.039, US 0.038, NZ 0.037, KE 0.037, IN 0.035, JM 0.034, GB 0.031, HK 0.024, NG 0.020, LK 0.013, MY 0.013, ZA 0.008, SG 0.008, IE 0.006, PK 0.002, \n",
      "NG (1): NG 0.232, TZ 0.193, GH 0.135, PH 0.108, JM 0.060, IN 0.047, HK 0.032, NZ 0.032, KE 0.024, CA 0.019, GB 0.019, SG 0.016, US 0.015, MY 0.012, AU 0.012, IE 0.011, BD 0.011, LK 0.009, ZA 0.009, PK 0.003, \n",
      "ZA (13): TZ 0.421, IN 0.120, HK 0.071, PH 0.064, US 0.047, MY 0.043, CA 0.042, NZ 0.036, SG 0.032, GB 0.023, JM 0.022, LK 0.017, ZA 0.016, KE 0.015, AU 0.011, BD 0.008, GH 0.005, IE 0.003, NG 0.003, PK 0.001, \n",
      "IE (5): PH 0.169, HK 0.124, TZ 0.121, LK 0.119, IE 0.087, IN 0.055, ZA 0.052, JM 0.039, GB 0.038, CA 0.033, BD 0.031, GH 0.023, US 0.021, NZ 0.018, KE 0.015, SG 0.013, AU 0.013, NG 0.012, MY 0.011, PK 0.007, \n",
      "HK (1): HK 0.505, US 0.161, PH 0.068, TZ 0.055, MY 0.027, SG 0.024, JM 0.023, GB 0.020, CA 0.019, KE 0.016, NZ 0.014, IE 0.013, IN 0.012, AU 0.011, BD 0.010, LK 0.008, ZA 0.006, NG 0.004, GH 0.003, PK 0.002, \n",
      "GB (3): TZ 0.148, PH 0.119, GB 0.115, CA 0.081, IN 0.081, AU 0.057, HK 0.056, NZ 0.048, US 0.043, JM 0.037, LK 0.031, MY 0.030, ZA 0.026, KE 0.025, BD 0.023, IE 0.021, GH 0.019, SG 0.016, NG 0.016, PK 0.008, \n",
      "AU (8): PH 0.154, TZ 0.135, US 0.112, NZ 0.112, GB 0.068, JM 0.064, HK 0.054, AU 0.044, IN 0.040, CA 0.039, GH 0.033, MY 0.027, KE 0.025, BD 0.024, SG 0.018, ZA 0.018, NG 0.012, LK 0.009, IE 0.009, PK 0.002, \n",
      "NZ (2): PH 0.155, NZ 0.137, TZ 0.114, IN 0.077, CA 0.074, GB 0.067, GH 0.060, US 0.044, JM 0.042, HK 0.041, ZA 0.031, SG 0.030, KE 0.028, AU 0.022, BD 0.021, MY 0.015, IE 0.013, LK 0.011, NG 0.010, PK 0.007, \n",
      "SG (4): JM 0.119, TZ 0.107, PH 0.073, SG 0.073, IN 0.061, HK 0.061, AU 0.059, ZA 0.048, GB 0.047, BD 0.045, GH 0.038, US 0.036, MY 0.035, LK 0.034, KE 0.031, NZ 0.030, IE 0.028, CA 0.028, NG 0.028, PK 0.019, \n",
      "BD (7): NZ 0.468, US 0.141, TZ 0.082, PH 0.080, GB 0.054, CA 0.052, BD 0.031, HK 0.021, IN 0.017, MY 0.012, KE 0.011, LK 0.007, JM 0.006, GH 0.005, IE 0.005, AU 0.004, ZA 0.003, SG 0.001, PK 0.000, NG 0.000, \n",
      "TZ (1): TZ 0.450, IN 0.171, PH 0.060, HK 0.057, GH 0.046, GB 0.043, AU 0.025, JM 0.024, KE 0.020, NG 0.016, BD 0.015, CA 0.013, US 0.013, NZ 0.013, LK 0.011, ZA 0.009, MY 0.006, SG 0.005, IE 0.002, PK 0.001, \n",
      "IN (2): TZ 0.256, IN 0.193, PH 0.145, HK 0.137, GB 0.048, CA 0.037, US 0.028, IE 0.024, JM 0.023, BD 0.022, LK 0.019, MY 0.016, GH 0.012, NZ 0.010, KE 0.006, ZA 0.006, SG 0.005, NG 0.004, AU 0.004, PK 0.003, \n",
      "JM (4): TZ 0.176, HK 0.128, ZA 0.118, JM 0.094, NZ 0.077, GH 0.061, GB 0.043, IE 0.040, PH 0.037, SG 0.035, PK 0.033, IN 0.030, US 0.026, CA 0.022, KE 0.021, LK 0.017, BD 0.017, AU 0.011, MY 0.009, NG 0.006, \n",
      "GH (1): GH 0.381, TZ 0.205, NZ 0.162, CA 0.158, KE 0.047, JM 0.015, GB 0.008, HK 0.006, IE 0.004, PH 0.004, PK 0.003, SG 0.003, IN 0.002, US 0.001, BD 0.001, AU 0.000, LK 0.000, ZA 0.000, NG 0.000, MY 0.000, \n",
      "US (2): TZ 0.222, US 0.152, PH 0.102, IN 0.070, HK 0.066, GB 0.054, JM 0.048, CA 0.048, NZ 0.044, AU 0.037, MY 0.025, LK 0.024, BD 0.021, GH 0.019, KE 0.018, SG 0.014, NG 0.011, ZA 0.011, IE 0.010, PK 0.004, \n",
      "PH (1): PH 0.477, TZ 0.140, GH 0.053, IN 0.050, US 0.049, NG 0.034, GB 0.029, SG 0.025, KE 0.025, NZ 0.018, MY 0.017, AU 0.014, ZA 0.014, HK 0.014, CA 0.011, BD 0.010, JM 0.009, LK 0.008, PK 0.002, IE 0.001, \n",
      "CA (1): CA 0.178, US 0.172, TZ 0.136, HK 0.112, IN 0.071, PH 0.062, GB 0.048, MY 0.037, JM 0.027, AU 0.023, KE 0.022, BD 0.021, NZ 0.020, LK 0.019, SG 0.015, ZA 0.011, GH 0.010, NG 0.007, IE 0.006, PK 0.002, \n",
      "LK (1): LK 0.520, TZ 0.122, PH 0.109, BD 0.042, IN 0.041, GB 0.034, JM 0.029, CA 0.020, HK 0.018, GH 0.015, IE 0.010, NZ 0.008, SG 0.006, MY 0.006, ZA 0.004, US 0.004, NG 0.004, KE 0.003, AU 0.003, PK 0.002, \n",
      "MY (5): IN 0.128, HK 0.121, TZ 0.080, NZ 0.078, MY 0.076, CA 0.075, US 0.066, AU 0.063, GB 0.037, KE 0.035, PH 0.034, NG 0.033, SG 0.032, BD 0.031, ZA 0.031, JM 0.029, LK 0.024, GH 0.014, PK 0.008, IE 0.006, \n",
      "PK (19): TZ 0.476, PH 0.171, IN 0.129, HK 0.048, NZ 0.032, GB 0.022, US 0.021, IE 0.014, CA 0.014, JM 0.014, ZA 0.012, BD 0.008, SG 0.008, LK 0.008, MY 0.007, GH 0.006, KE 0.004, AU 0.004, PK 0.003, NG 0.002, \n",
      "\n",
      "TZ 1.8 tries (5 tests)\n",
      "PH 2.5 tries (2 tests)\n",
      "GH 4.0 tries (3 tests)\n",
      "LK 4.0 tries (2 tests)\n",
      "JM 4.6 tries (5 tests)\n",
      "SG 4.7 tries (3 tests)\n",
      "NZ 4.9 tries (7 tests)\n",
      "CA 5.3 tries (14 tests)\n",
      "IN 5.3 tries (9 tests)\n",
      "US 6.0 tries (28 tests)\n",
      "HK 6.2 tries (4 tests)\n",
      "GB 6.4 tries (30 tests)\n",
      "IE 7.4 tries (11 tests)\n",
      "BD 8.0 tries (2 tests)\n",
      "MY 8.0 tries (2 tests)\n",
      "AU 8.2 tries (13 tests)\n",
      "KE 8.5 tries (4 tests)\n",
      "ZA 10.0 tries (6 tests)\n",
      "NG 10.7 tries (7 tests)\n",
      "PK 13.5 tries (8 tests)\n",
      "Average number of tries: 6.499958374958375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Multi-layer Perceptron Model with undersampling\n",
    "mlp_model = MLPClassifier(random_state=3)\n",
    "mlp_model.fit(X_resampled, y_resampled)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(mlp_model)\n",
    "average_tries(mlp_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'TZ': 2.855769230769231, 'NG': 2.475, 'JM': 2.475, 'HK': 2.3951612903225805, 'GH': 2.3951612903225805, 'KE': 2.3203125, 'BD': 2.25, 'ZA': 2.1838235294117645, 'SG': 2.1214285714285714, 'MY': 2.1214285714285714, 'PH': 2.0625, 'PK': 2.0067567567567566, 'LK': 1.9038461538461537, 'NZ': 1.16015625, 'IN': 0.9769736842105263, 'IE': 0.9642857142857143, 'CA': 0.720873786407767, 'AU': 0.6346153846153846, 'GB': 0.24029126213592233, 'US': 0.23951612903225805})\n"
     ]
    }
   ],
   "source": [
    "# Calculate weights based on document number (default \"balanced\" weights)\n",
    "total_docs = sum(Counter(y_train).values())\n",
    "weights_dict_doc = Counter({country_code:0 for country_code in country_set})\n",
    "\n",
    "for country_code in weights_dict_doc:\n",
    "    weights_dict_doc[country_code] = total_docs/(len(country_set) * Counter(y_train)[country_code])\n",
    "\n",
    "print(weights_dict_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'PH': 3.910410016719866, 'JM': 3.8743401980347127, 'GH': 3.3747532220509626, 'ZA': 2.622619654416637, 'BD': 2.469495572460464, 'KE': 2.4500928549320253, 'SG': 2.444912093133761, 'NG': 2.262075749582344, 'MY': 2.1723032593092966, 'HK': 2.0897236211518155, 'TZ': 2.0851083014203797, 'NZ': 1.4886779214767236, 'LK': 1.3360817179730204, 'IN': 0.9784297599239363, 'IE': 0.9313634718074034, 'PK': 0.7469376719338613, 'CA': 0.6877617376775271, 'AU': 0.4746887497866569, 'GB': 0.3223559981455493, 'US': 0.23762114120382663})\n",
      "2058127\n"
     ]
    }
   ],
   "source": [
    "# Calculate weights based on word count\n",
    "total_words = sum(country_word_counts.values())\n",
    "weights_dict_word = Counter({country_code:0 for country_code in country_set})\n",
    "\n",
    "for country_code in weights_dict_word:\n",
    "    weights_dict_word[country_code] = total_words/(len(country_set) * country_word_counts[country_code])\n",
    "\n",
    "print(weights_dict_word)\n",
    "print(total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28\n",
      "KE (10): US 0.424, GB 0.249, CA 0.110, GH 0.075, JM 0.056, AU 0.024, NG 0.021, IE 0.008, NZ 0.007, KE 0.005, LK 0.004, ZA 0.003, PK 0.003, BD 0.003, MY 0.002, TZ 0.002, PH 0.002, IN 0.001, HK 0.001, SG 0.001, \n",
      "NG (2): GB 0.209, NG 0.156, CA 0.114, NZ 0.101, KE 0.099, GH 0.078, TZ 0.067, US 0.042, PK 0.030, IE 0.018, AU 0.016, IN 0.014, ZA 0.013, HK 0.011, PH 0.009, JM 0.008, SG 0.007, LK 0.003, MY 0.003, BD 0.002, \n",
      "ZA (5): GB 0.179, CA 0.160, NG 0.146, AU 0.117, ZA 0.103, IE 0.086, US 0.059, MY 0.043, NZ 0.037, BD 0.011, PK 0.010, SG 0.009, TZ 0.009, LK 0.008, IN 0.007, GH 0.006, HK 0.004, JM 0.003, KE 0.003, PH 0.001, \n",
      "IE (1): IE 0.320, GB 0.175, US 0.142, PH 0.098, LK 0.070, IN 0.038, CA 0.032, TZ 0.029, ZA 0.028, AU 0.016, NZ 0.009, JM 0.009, PK 0.005, KE 0.005, HK 0.004, BD 0.004, GH 0.004, NG 0.004, SG 0.004, MY 0.003, \n",
      "HK (1): HK 0.298, GB 0.170, JM 0.158, MY 0.153, AU 0.097, CA 0.042, US 0.030, SG 0.012, PH 0.007, NZ 0.007, IE 0.007, LK 0.005, IN 0.005, NG 0.004, KE 0.002, TZ 0.002, BD 0.001, PK 0.001, ZA 0.000, GH 0.000, \n",
      "GB (1): GB 0.371, US 0.167, CA 0.102, AU 0.067, PH 0.060, LK 0.044, IE 0.043, IN 0.026, NZ 0.021, TZ 0.016, ZA 0.015, JM 0.013, MY 0.009, SG 0.008, NG 0.008, PK 0.007, HK 0.007, GH 0.006, KE 0.005, BD 0.005, \n",
      "AU (1): AU 0.255, GB 0.242, US 0.142, JM 0.087, ZA 0.058, CA 0.045, TZ 0.030, IN 0.027, NZ 0.019, PH 0.016, IE 0.015, GH 0.014, NG 0.009, SG 0.009, LK 0.008, MY 0.006, KE 0.005, HK 0.005, PK 0.004, BD 0.003, \n",
      "NZ (1): NZ 0.260, GB 0.236, AU 0.132, IN 0.128, US 0.082, PH 0.029, ZA 0.029, CA 0.022, IE 0.015, TZ 0.009, KE 0.009, HK 0.009, PK 0.009, GH 0.006, MY 0.006, LK 0.005, JM 0.005, SG 0.003, BD 0.003, NG 0.003, \n",
      "SG (8): GB 0.349, US 0.180, CA 0.120, IN 0.061, AU 0.051, PH 0.046, ZA 0.028, SG 0.024, IE 0.022, JM 0.020, LK 0.019, NG 0.013, MY 0.013, NZ 0.012, BD 0.010, TZ 0.008, GH 0.007, PK 0.007, HK 0.007, KE 0.005, \n",
      "BD (3): GB 0.735, IN 0.071, BD 0.043, US 0.042, KE 0.038, PH 0.027, NZ 0.024, CA 0.009, IE 0.005, GH 0.002, AU 0.001, SG 0.001, JM 0.001, HK 0.001, LK 0.001, TZ 0.000, MY 0.000, ZA 0.000, PK 0.000, NG 0.000, \n",
      "TZ (18): GB 0.279, US 0.243, IN 0.104, AU 0.084, PH 0.061, ZA 0.036, NZ 0.028, CA 0.028, KE 0.022, JM 0.021, IE 0.019, GH 0.017, NG 0.013, LK 0.011, HK 0.009, BD 0.008, MY 0.006, TZ 0.005, SG 0.003, PK 0.002, \n",
      "IN (2): GB 0.269, IN 0.118, KE 0.112, PH 0.110, IE 0.098, US 0.070, LK 0.055, CA 0.046, JM 0.039, AU 0.023, TZ 0.023, MY 0.008, GH 0.008, ZA 0.007, NZ 0.005, BD 0.003, NG 0.002, SG 0.002, HK 0.002, PK 0.001, \n",
      "JM (5): GB 0.179, AU 0.175, ZA 0.139, NZ 0.134, JM 0.086, US 0.069, KE 0.046, LK 0.024, NG 0.024, IE 0.023, PK 0.021, CA 0.020, GH 0.013, HK 0.007, TZ 0.007, IN 0.007, BD 0.006, PH 0.006, MY 0.006, SG 0.006, \n",
      "GH (1): GH 0.201, IN 0.181, SG 0.149, KE 0.108, US 0.075, IE 0.068, GB 0.065, ZA 0.038, PK 0.033, TZ 0.023, JM 0.016, HK 0.012, CA 0.009, AU 0.007, NG 0.005, LK 0.005, MY 0.002, NZ 0.002, PH 0.001, BD 0.001, \n",
      "US (1): US 0.368, GB 0.247, AU 0.082, CA 0.058, IE 0.048, LK 0.047, NZ 0.035, JM 0.032, IN 0.018, PH 0.010, PK 0.010, SG 0.007, BD 0.007, MY 0.006, HK 0.006, ZA 0.005, NG 0.004, GH 0.004, KE 0.003, TZ 0.001, \n",
      "PH (19): GB 0.823, CA 0.093, KE 0.043, SG 0.009, NG 0.009, IN 0.004, JM 0.003, HK 0.003, MY 0.002, GH 0.002, AU 0.002, TZ 0.002, NZ 0.001, LK 0.001, US 0.001, BD 0.001, ZA 0.000, IE 0.000, PH 0.000, PK 0.000, \n",
      "CA (3): US 0.256, GB 0.200, CA 0.178, PH 0.073, LK 0.066, ZA 0.055, AU 0.040, IN 0.025, IE 0.024, BD 0.016, HK 0.012, SG 0.011, MY 0.008, JM 0.007, KE 0.006, GH 0.006, NZ 0.005, PK 0.004, NG 0.003, TZ 0.003, \n",
      "LK (1): LK 0.583, AU 0.086, PH 0.065, US 0.060, IE 0.028, NG 0.026, GB 0.025, GH 0.023, TZ 0.023, IN 0.018, NZ 0.017, MY 0.010, ZA 0.008, SG 0.007, PK 0.006, HK 0.005, BD 0.004, CA 0.002, JM 0.002, KE 0.001, \n",
      "MY (15): GB 0.362, US 0.351, CA 0.068, NG 0.043, AU 0.038, SG 0.032, IN 0.021, ZA 0.016, LK 0.011, PH 0.010, IE 0.009, HK 0.007, PK 0.005, NZ 0.005, MY 0.005, KE 0.004, TZ 0.003, JM 0.003, BD 0.003, GH 0.001, \n",
      "PK (3): US 0.239, IN 0.188, PK 0.127, NG 0.114, AU 0.114, CA 0.049, PH 0.040, IE 0.034, MY 0.032, GB 0.029, GH 0.015, HK 0.005, NZ 0.004, LK 0.003, JM 0.002, SG 0.002, BD 0.002, TZ 0.001, ZA 0.001, KE 0.000, \n",
      "\n",
      "LK 1.0 tries (2 tests)\n",
      "GB 2.2 tries (30 tests)\n",
      "US 3.1 tries (28 tests)\n",
      "IE 3.4 tries (11 tests)\n",
      "NZ 3.4 tries (7 tests)\n",
      "CA 4.0 tries (14 tests)\n",
      "AU 4.4 tries (13 tests)\n",
      "BD 4.5 tries (2 tests)\n",
      "GH 4.7 tries (3 tests)\n",
      "ZA 7.2 tries (6 tests)\n",
      "HK 7.2 tries (4 tests)\n",
      "PK 8.9 tries (8 tests)\n",
      "NG 9.1 tries (7 tests)\n",
      "IN 9.3 tries (9 tests)\n",
      "SG 9.7 tries (3 tests)\n",
      "JM 10.2 tries (5 tests)\n",
      "KE 13.5 tries (4 tests)\n",
      "TZ 13.8 tries (5 tests)\n",
      "MY 14.0 tries (2 tests)\n",
      "PH 17.0 tries (2 tests)\n",
      "Average number of tries: 7.532710206460206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model with class weights balanced by word count\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=500, class_weight=weights_dict_word)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(model)\n",
    "average_tries(model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.31\n",
      "KE (3): GB 0.255, US 0.185, KE 0.077, CA 0.057, AU 0.048, NZ 0.045, NG 0.035, TZ 0.035, IE 0.033, IN 0.032, GH 0.030, ZA 0.030, BD 0.028, SG 0.025, PK 0.022, LK 0.018, JM 0.015, HK 0.013, MY 0.013, PH 0.005, \n",
      "NG (3): GB 0.186, US 0.183, NG 0.161, CA 0.063, AU 0.054, IN 0.043, IE 0.040, NZ 0.033, GH 0.031, ZA 0.029, KE 0.021, MY 0.021, LK 0.021, HK 0.020, JM 0.019, PK 0.017, BD 0.016, TZ 0.016, SG 0.014, PH 0.011, \n",
      "ZA (11): GB 0.258, US 0.230, AU 0.098, CA 0.070, IN 0.037, SG 0.035, IE 0.028, PH 0.027, KE 0.025, MY 0.025, ZA 0.023, PK 0.023, TZ 0.023, NG 0.020, GH 0.020, NZ 0.015, HK 0.013, BD 0.012, LK 0.010, JM 0.007, \n",
      "IE (3): GB 0.241, US 0.202, IE 0.156, AU 0.085, CA 0.060, NZ 0.033, IN 0.032, PK 0.021, MY 0.020, HK 0.017, SG 0.017, ZA 0.016, TZ 0.015, GH 0.015, LK 0.015, PH 0.015, KE 0.012, BD 0.012, JM 0.010, NG 0.007, \n",
      "HK (3): GB 0.215, US 0.212, HK 0.073, AU 0.070, CA 0.068, NZ 0.048, IE 0.043, SG 0.043, JM 0.040, IN 0.038, MY 0.028, BD 0.020, PK 0.020, ZA 0.018, PH 0.017, NG 0.013, TZ 0.013, KE 0.010, LK 0.010, GH 0.005, \n",
      "GB (1): GB 0.307, US 0.279, AU 0.073, CA 0.060, IE 0.050, NZ 0.034, IN 0.028, PH 0.019, SG 0.018, KE 0.016, ZA 0.015, GH 0.014, PK 0.013, JM 0.013, TZ 0.012, BD 0.011, MY 0.011, HK 0.010, NG 0.008, LK 0.008, \n",
      "AU (3): GB 0.245, US 0.231, AU 0.126, CA 0.046, NZ 0.045, IE 0.034, IN 0.028, ZA 0.025, SG 0.025, PK 0.025, KE 0.024, JM 0.024, GH 0.019, BD 0.018, HK 0.017, PH 0.017, LK 0.017, NG 0.013, TZ 0.011, MY 0.010, \n",
      "NZ (3): GB 0.236, US 0.223, NZ 0.169, AU 0.077, IN 0.057, IE 0.046, CA 0.046, KE 0.021, GH 0.014, NG 0.013, TZ 0.013, BD 0.011, PH 0.011, PK 0.011, SG 0.010, ZA 0.009, HK 0.009, JM 0.009, MY 0.009, LK 0.007, \n",
      "SG (5): GB 0.343, US 0.333, AU 0.093, CA 0.050, SG 0.033, IN 0.030, NZ 0.023, IE 0.020, JM 0.017, HK 0.010, KE 0.007, NG 0.007, ZA 0.007, TZ 0.007, GH 0.007, LK 0.007, MY 0.003, PK 0.003, BD 0.000, PH 0.000, \n",
      "BD (2): GB 0.270, BD 0.150, US 0.145, NZ 0.085, CA 0.075, AU 0.055, IE 0.040, JM 0.025, PH 0.025, HK 0.020, IN 0.020, GH 0.020, MY 0.015, KE 0.010, ZA 0.010, SG 0.010, TZ 0.010, PK 0.010, LK 0.005, NG 0.000, \n",
      "TZ (5): US 0.202, GB 0.196, ZA 0.086, AU 0.068, TZ 0.064, CA 0.060, GH 0.044, KE 0.034, IE 0.034, IN 0.032, LK 0.032, NZ 0.026, SG 0.026, NG 0.024, PH 0.020, PK 0.016, HK 0.014, JM 0.014, BD 0.008, MY 0.000, \n",
      "IN (3): GB 0.264, US 0.180, IN 0.094, AU 0.073, CA 0.048, IE 0.040, NZ 0.034, LK 0.034, MY 0.027, PK 0.024, BD 0.023, JM 0.023, SG 0.022, KE 0.022, HK 0.021, PH 0.019, ZA 0.018, GH 0.014, TZ 0.009, NG 0.008, \n",
      "JM (3): GB 0.242, US 0.218, JM 0.072, AU 0.066, CA 0.060, IN 0.050, IE 0.048, NZ 0.046, HK 0.040, TZ 0.026, NG 0.018, MY 0.018, PK 0.018, GH 0.016, ZA 0.014, BD 0.014, KE 0.010, PH 0.010, SG 0.008, LK 0.006, \n",
      "GH (3): GB 0.183, US 0.180, GH 0.113, KE 0.090, AU 0.077, TZ 0.047, IN 0.047, IE 0.040, CA 0.040, PH 0.033, SG 0.030, LK 0.023, ZA 0.020, HK 0.020, NZ 0.017, PK 0.013, NG 0.010, JM 0.010, MY 0.007, BD 0.000, \n",
      "US (1): US 0.264, GB 0.244, CA 0.074, AU 0.069, IN 0.040, NZ 0.039, IE 0.036, JM 0.025, GH 0.022, PH 0.021, ZA 0.019, BD 0.019, PK 0.018, HK 0.018, SG 0.018, LK 0.016, TZ 0.016, NG 0.016, KE 0.015, MY 0.014, \n",
      "PH (14): GB 0.335, US 0.195, IN 0.095, AU 0.065, NZ 0.065, IE 0.040, KE 0.030, CA 0.030, TZ 0.020, JM 0.020, ZA 0.015, HK 0.015, BD 0.015, PH 0.015, MY 0.015, PK 0.015, SG 0.005, GH 0.005, LK 0.005, NG 0.000, \n",
      "CA (3): GB 0.256, US 0.245, CA 0.097, AU 0.090, NZ 0.045, IE 0.038, IN 0.033, SG 0.024, GH 0.019, PH 0.019, TZ 0.018, MY 0.016, JM 0.016, NG 0.015, HK 0.014, KE 0.014, PK 0.011, BD 0.011, LK 0.010, ZA 0.009, \n",
      "LK (1): LK 0.285, US 0.160, GB 0.145, BD 0.070, IN 0.060, IE 0.055, AU 0.045, NZ 0.045, CA 0.035, PH 0.030, PK 0.015, KE 0.010, ZA 0.010, GH 0.010, NG 0.005, SG 0.005, TZ 0.005, JM 0.005, MY 0.005, HK 0.000, \n",
      "MY (8): US 0.285, GB 0.270, CA 0.080, AU 0.075, IE 0.055, NZ 0.045, SG 0.035, MY 0.030, KE 0.025, HK 0.020, ZA 0.015, IN 0.015, JM 0.010, GH 0.010, PK 0.010, NG 0.005, BD 0.005, TZ 0.005, PH 0.005, LK 0.000, \n",
      "PK (4): GB 0.224, US 0.197, CA 0.083, PK 0.082, AU 0.066, NZ 0.051, IN 0.036, IE 0.030, SG 0.029, NG 0.024, PH 0.022, GH 0.021, MY 0.021, BD 0.020, HK 0.019, LK 0.019, KE 0.016, ZA 0.016, JM 0.013, TZ 0.010, \n",
      "\n",
      "LK 1.0 tries (2 tests)\n",
      "GB 1.4 tries (30 tests)\n",
      "US 1.5 tries (28 tests)\n",
      "NZ 2.9 tries (7 tests)\n",
      "AU 2.9 tries (13 tests)\n",
      "IE 3.6 tries (11 tests)\n",
      "CA 4.4 tries (14 tests)\n",
      "BD 4.5 tries (2 tests)\n",
      "IN 6.4 tries (9 tests)\n",
      "TZ 6.6 tries (5 tests)\n",
      "GH 6.7 tries (3 tests)\n",
      "KE 6.8 tries (4 tests)\n",
      "NG 7.3 tries (7 tests)\n",
      "PK 8.0 tries (8 tests)\n",
      "HK 8.2 tries (4 tests)\n",
      "SG 9.3 tries (3 tests)\n",
      "JM 10.0 tries (5 tests)\n",
      "MY 10.0 tries (2 tests)\n",
      "PH 12.5 tries (2 tests)\n",
      "ZA 14.7 tries (6 tests)\n",
      "Average number of tries: 6.438765678765679\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier with class weights balanced by word count\n",
    "rf_model = RandomForestClassifier(random_state=3, class_weight=weights_dict_word)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(rf_model)\n",
    "average_tries(rf_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models with weight balancing and normal sampling for the 25% dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_25, y, test_size=0.1, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25\n",
      "KE (13): US 0.480, GB 0.211, CA 0.135, JM 0.077, AU 0.036, GH 0.019, NG 0.015, IE 0.006, NZ 0.004, PK 0.004, MY 0.003, IN 0.002, KE 0.002, LK 0.002, BD 0.001, SG 0.001, ZA 0.001, PH 0.001, TZ 0.000, HK 0.000, \n",
      "NG (2): GB 0.248, NG 0.148, US 0.139, CA 0.121, NZ 0.085, KE 0.077, GH 0.027, TZ 0.026, PK 0.024, IE 0.019, AU 0.016, JM 0.016, IN 0.013, ZA 0.010, HK 0.009, SG 0.006, PH 0.006, MY 0.004, LK 0.003, BD 0.002, \n",
      "ZA (8): GB 0.191, NG 0.158, CA 0.155, IE 0.138, AU 0.099, US 0.082, NZ 0.043, ZA 0.040, MY 0.038, TZ 0.011, SG 0.009, BD 0.008, LK 0.006, PK 0.005, IN 0.004, KE 0.004, HK 0.003, GH 0.003, JM 0.002, PH 0.001, \n",
      "IE (1): IE 0.237, GB 0.232, US 0.110, PH 0.097, TZ 0.075, ZA 0.049, LK 0.045, IN 0.042, CA 0.025, AU 0.021, JM 0.010, NZ 0.009, KE 0.008, PK 0.007, SG 0.006, BD 0.006, GH 0.006, NG 0.005, MY 0.005, HK 0.004, \n",
      "HK (1): HK 0.239, AU 0.219, GB 0.149, MY 0.101, JM 0.100, CA 0.072, US 0.021, SG 0.021, IE 0.020, NZ 0.020, IN 0.015, PH 0.007, LK 0.006, NG 0.004, KE 0.002, PK 0.002, TZ 0.002, BD 0.001, ZA 0.000, GH 0.000, \n",
      "GB (1): GB 0.358, US 0.173, CA 0.116, AU 0.066, IE 0.061, LK 0.041, IN 0.025, PH 0.025, NZ 0.025, ZA 0.016, TZ 0.015, MY 0.011, NG 0.011, PK 0.011, JM 0.010, BD 0.009, GH 0.008, HK 0.007, KE 0.007, SG 0.006, \n",
      "AU (1): AU 0.275, GB 0.210, JM 0.175, US 0.139, ZA 0.051, CA 0.042, IN 0.028, SG 0.013, IE 0.010, NZ 0.009, NG 0.008, MY 0.008, LK 0.006, KE 0.005, PK 0.005, PH 0.005, HK 0.003, GH 0.002, TZ 0.002, BD 0.002, \n",
      "NZ (1): NZ 0.281, GB 0.200, AU 0.146, IN 0.146, US 0.081, PH 0.033, ZA 0.020, CA 0.020, IE 0.012, PK 0.011, TZ 0.009, KE 0.007, HK 0.007, GH 0.007, MY 0.005, LK 0.005, JM 0.004, SG 0.003, BD 0.002, NG 0.002, \n",
      "SG (7): GB 0.298, US 0.173, CA 0.117, PH 0.088, IN 0.052, AU 0.039, SG 0.027, ZA 0.024, LK 0.022, JM 0.021, IE 0.020, NG 0.018, BD 0.016, MY 0.016, NZ 0.015, TZ 0.013, PK 0.012, GH 0.011, HK 0.010, KE 0.007, \n",
      "BD (4): GB 0.696, IN 0.137, US 0.038, BD 0.034, NZ 0.027, PH 0.021, CA 0.021, KE 0.018, IE 0.003, GH 0.001, AU 0.001, SG 0.001, LK 0.000, HK 0.000, PK 0.000, JM 0.000, TZ 0.000, ZA 0.000, MY 0.000, NG 0.000, \n",
      "TZ (18): GB 0.328, US 0.240, IN 0.104, AU 0.082, ZA 0.040, KE 0.034, NZ 0.026, JM 0.022, CA 0.022, IE 0.017, PH 0.017, GH 0.014, LK 0.012, HK 0.010, NG 0.010, BD 0.007, MY 0.006, TZ 0.005, SG 0.002, PK 0.002, \n",
      "IN (2): GB 0.227, IN 0.119, KE 0.111, PH 0.104, IE 0.097, US 0.082, JM 0.063, CA 0.051, LK 0.039, TZ 0.039, AU 0.025, MY 0.009, ZA 0.009, GH 0.008, NZ 0.005, BD 0.004, HK 0.002, NG 0.002, SG 0.002, PK 0.001, \n",
      "JM (7): AU 0.200, GB 0.185, US 0.149, NZ 0.137, ZA 0.062, KE 0.060, JM 0.041, LK 0.033, IE 0.024, NG 0.024, PK 0.017, TZ 0.014, CA 0.013, GH 0.010, PH 0.007, BD 0.007, IN 0.006, HK 0.005, SG 0.004, MY 0.003, \n",
      "GH (2): GB 0.261, GH 0.220, IE 0.165, JM 0.123, IN 0.075, KE 0.034, SG 0.032, ZA 0.027, US 0.020, TZ 0.014, HK 0.010, PK 0.007, LK 0.004, CA 0.003, AU 0.003, NG 0.002, NZ 0.001, MY 0.001, PH 0.000, BD 0.000, \n",
      "US (1): US 0.353, GB 0.245, AU 0.094, CA 0.050, IE 0.048, JM 0.038, NZ 0.035, LK 0.029, IN 0.029, PK 0.012, SG 0.012, HK 0.009, MY 0.009, ZA 0.008, BD 0.008, PH 0.007, NG 0.006, GH 0.003, KE 0.003, TZ 0.001, \n",
      "PH (18): GB 0.683, CA 0.226, KE 0.054, SG 0.008, NG 0.007, GH 0.006, IN 0.004, HK 0.002, TZ 0.002, BD 0.002, LK 0.001, MY 0.001, NZ 0.001, AU 0.001, JM 0.001, US 0.000, ZA 0.000, PH 0.000, PK 0.000, IE 0.000, \n",
      "CA (3): US 0.248, GB 0.228, CA 0.194, PH 0.077, IN 0.056, AU 0.042, LK 0.041, IE 0.017, SG 0.017, ZA 0.014, BD 0.010, MY 0.010, JM 0.010, KE 0.008, HK 0.006, GH 0.006, PK 0.005, NZ 0.005, NG 0.004, TZ 0.003, \n",
      "LK (3): US 0.506, AU 0.180, LK 0.074, PH 0.063, NG 0.033, TZ 0.031, IE 0.024, GB 0.021, GH 0.016, IN 0.014, NZ 0.008, MY 0.007, ZA 0.005, BD 0.005, PK 0.004, JM 0.003, SG 0.002, HK 0.001, CA 0.001, KE 0.001, \n",
      "MY (10): GB 0.339, US 0.330, SG 0.125, CA 0.087, AU 0.028, NG 0.024, IN 0.022, IE 0.007, LK 0.007, MY 0.006, PK 0.005, ZA 0.004, JM 0.003, PH 0.003, BD 0.003, KE 0.002, TZ 0.002, HK 0.001, NZ 0.001, GH 0.000, \n",
      "PK (12): IN 0.353, US 0.205, CA 0.125, NG 0.097, AU 0.043, GH 0.037, IE 0.033, GB 0.027, NZ 0.023, HK 0.018, MY 0.012, PK 0.007, LK 0.006, PH 0.005, JM 0.003, BD 0.002, SG 0.002, TZ 0.002, ZA 0.001, KE 0.000, \n",
      "\n",
      "GB 2.4 tries (30 tests)\n",
      "US 3.2 tries (28 tests)\n",
      "LK 3.5 tries (2 tests)\n",
      "NZ 3.9 tries (7 tests)\n",
      "IE 3.9 tries (11 tests)\n",
      "BD 4.0 tries (2 tests)\n",
      "CA 4.0 tries (14 tests)\n",
      "AU 4.3 tries (13 tests)\n",
      "GH 6.3 tries (3 tests)\n",
      "ZA 7.5 tries (6 tests)\n",
      "PK 7.6 tries (8 tests)\n",
      "HK 8.2 tries (4 tests)\n",
      "IN 8.9 tries (9 tests)\n",
      "JM 10.2 tries (5 tests)\n",
      "SG 10.7 tries (3 tests)\n",
      "NG 10.7 tries (7 tests)\n",
      "MY 12.0 tries (2 tests)\n",
      "KE 13.0 tries (4 tests)\n",
      "TZ 13.6 tries (5 tests)\n",
      "PH 16.5 tries (2 tests)\n",
      "Average number of tries: 7.721533605283605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model removing words that appear in less than 25% of countries\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=500, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(model)\n",
    "average_tries(model, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34\n",
      "KE (7): GB 0.237, US 0.195, CA 0.085, AU 0.062, GH 0.048, ZA 0.048, KE 0.045, NG 0.030, IE 0.030, NZ 0.028, TZ 0.028, LK 0.025, IN 0.022, PH 0.022, MY 0.020, PK 0.020, BD 0.017, SG 0.015, JM 0.013, HK 0.010, \n",
      "NG (3): GB 0.213, US 0.176, NG 0.100, CA 0.066, AU 0.059, IN 0.046, HK 0.041, NZ 0.034, GH 0.029, KE 0.027, ZA 0.027, LK 0.027, IE 0.027, MY 0.026, TZ 0.026, SG 0.019, JM 0.019, BD 0.017, PH 0.013, PK 0.010, \n",
      "ZA (9): GB 0.232, US 0.208, AU 0.092, CA 0.082, IN 0.048, SG 0.040, NZ 0.035, KE 0.033, ZA 0.033, IE 0.032, MY 0.030, NG 0.028, JM 0.018, PH 0.018, HK 0.013, GH 0.013, BD 0.012, PK 0.012, TZ 0.010, LK 0.010, \n",
      "IE (2): GB 0.218, IE 0.197, US 0.175, AU 0.099, CA 0.050, NZ 0.030, ZA 0.028, SG 0.023, JM 0.023, IN 0.021, MY 0.016, TZ 0.016, NG 0.015, KE 0.015, BD 0.014, LK 0.014, GH 0.013, HK 0.012, PH 0.012, PK 0.009, \n",
      "HK (5): GB 0.240, US 0.195, AU 0.068, IE 0.058, HK 0.058, IN 0.055, CA 0.052, NZ 0.043, SG 0.033, MY 0.028, TZ 0.022, KE 0.020, JM 0.020, GH 0.020, PH 0.020, PK 0.020, LK 0.017, ZA 0.013, BD 0.013, NG 0.007, \n",
      "GB (1): GB 0.281, US 0.266, AU 0.070, CA 0.070, IE 0.042, IN 0.038, NZ 0.030, PH 0.021, PK 0.021, JM 0.019, LK 0.018, ZA 0.018, MY 0.016, NG 0.014, KE 0.014, GH 0.014, SG 0.013, TZ 0.012, HK 0.012, BD 0.011, \n",
      "AU (3): GB 0.218, US 0.199, AU 0.154, CA 0.055, NZ 0.047, IE 0.038, IN 0.035, PH 0.027, LK 0.025, KE 0.025, SG 0.022, PK 0.021, GH 0.021, NG 0.020, BD 0.018, JM 0.018, MY 0.017, ZA 0.016, HK 0.015, TZ 0.010, \n",
      "NZ (1): NZ 0.230, US 0.194, GB 0.184, AU 0.073, IE 0.056, CA 0.049, IN 0.036, KE 0.023, PH 0.023, TZ 0.020, ZA 0.017, BD 0.014, NG 0.013, JM 0.013, GH 0.011, SG 0.010, LK 0.010, HK 0.009, MY 0.009, PK 0.007, \n",
      "SG (8): GB 0.327, US 0.283, AU 0.073, CA 0.050, PH 0.043, IN 0.033, IE 0.027, SG 0.027, HK 0.020, LK 0.020, NZ 0.017, JM 0.017, GH 0.013, MY 0.013, ZA 0.010, PK 0.010, NG 0.007, TZ 0.007, BD 0.003, KE 0.000, \n",
      "BD (2): GB 0.295, BD 0.180, US 0.145, CA 0.050, IE 0.045, NZ 0.045, MY 0.030, PH 0.030, NG 0.025, PK 0.025, AU 0.020, SG 0.020, IN 0.020, HK 0.015, LK 0.015, KE 0.010, TZ 0.010, GH 0.010, ZA 0.005, JM 0.005, \n",
      "TZ (4): GB 0.186, US 0.158, ZA 0.124, TZ 0.064, CA 0.060, AU 0.054, GH 0.054, NG 0.042, IN 0.038, NZ 0.038, JM 0.034, IE 0.028, KE 0.026, LK 0.024, PH 0.018, SG 0.016, MY 0.012, PK 0.010, BD 0.008, HK 0.006, \n",
      "IN (3): GB 0.203, US 0.200, IN 0.114, CA 0.066, AU 0.064, MY 0.047, LK 0.038, NZ 0.033, JM 0.033, IE 0.026, BD 0.024, PH 0.024, SG 0.020, GH 0.020, NG 0.017, PK 0.016, ZA 0.014, HK 0.014, TZ 0.013, KE 0.012, \n",
      "JM (10): GB 0.230, US 0.220, HK 0.066, AU 0.052, IE 0.048, IN 0.048, CA 0.046, NZ 0.044, GH 0.032, JM 0.030, LK 0.024, MY 0.022, KE 0.022, ZA 0.020, SG 0.020, TZ 0.020, PH 0.020, NG 0.014, PK 0.012, BD 0.010, \n",
      "GH (3): GB 0.187, US 0.157, GH 0.147, KE 0.087, AU 0.047, LK 0.043, IE 0.040, PK 0.040, TZ 0.037, CA 0.037, ZA 0.030, JM 0.030, NZ 0.023, NG 0.020, IN 0.020, MY 0.020, HK 0.017, BD 0.010, PH 0.007, SG 0.003, \n",
      "US (1): US 0.245, GB 0.210, CA 0.087, AU 0.059, IN 0.045, IE 0.044, NZ 0.035, JM 0.030, SG 0.024, PK 0.023, GH 0.022, ZA 0.022, MY 0.021, PH 0.021, KE 0.020, TZ 0.019, BD 0.019, NG 0.019, LK 0.017, HK 0.016, \n",
      "PH (18): US 0.240, GB 0.230, AU 0.120, CA 0.070, TZ 0.045, IE 0.040, NZ 0.035, KE 0.030, SG 0.030, ZA 0.025, IN 0.025, HK 0.020, GH 0.015, LK 0.015, MY 0.015, NG 0.010, BD 0.010, PH 0.010, PK 0.010, JM 0.005, \n",
      "CA (3): GB 0.227, US 0.227, CA 0.116, AU 0.086, IN 0.046, IE 0.029, SG 0.028, HK 0.026, KE 0.022, PH 0.021, JM 0.021, NZ 0.019, GH 0.019, PK 0.019, NG 0.018, ZA 0.018, TZ 0.018, BD 0.015, MY 0.014, LK 0.012, \n",
      "LK (1): LK 0.290, GB 0.110, US 0.110, IN 0.100, BD 0.090, CA 0.045, PK 0.045, AU 0.040, GH 0.025, PH 0.025, IE 0.020, TZ 0.020, NZ 0.015, JM 0.015, KE 0.010, NG 0.010, SG 0.010, MY 0.010, ZA 0.005, HK 0.005, \n",
      "MY (17): US 0.280, GB 0.220, AU 0.080, CA 0.055, IE 0.045, ZA 0.035, HK 0.035, NZ 0.035, JM 0.035, IN 0.030, NG 0.020, BD 0.020, GH 0.020, PH 0.020, PK 0.020, SG 0.015, MY 0.015, KE 0.010, TZ 0.005, LK 0.005, \n",
      "PK (3): GB 0.211, US 0.188, PK 0.128, CA 0.074, NZ 0.049, AU 0.041, IN 0.041, BD 0.034, SG 0.030, NG 0.026, PH 0.024, TZ 0.023, HK 0.022, MY 0.021, KE 0.020, IE 0.020, GH 0.019, JM 0.013, ZA 0.011, LK 0.006, \n",
      "\n",
      "LK 1.0 tries (2 tests)\n",
      "GB 1.4 tries (30 tests)\n",
      "US 1.5 tries (28 tests)\n",
      "AU 2.5 tries (13 tests)\n",
      "NZ 3.0 tries (7 tests)\n",
      "CA 3.3 tries (14 tests)\n",
      "BD 4.0 tries (2 tests)\n",
      "IE 4.3 tries (11 tests)\n",
      "GH 5.7 tries (3 tests)\n",
      "IN 6.3 tries (9 tests)\n",
      "PK 6.8 tries (8 tests)\n",
      "NG 7.6 tries (7 tests)\n",
      "HK 8.8 tries (4 tests)\n",
      "KE 10.2 tries (4 tests)\n",
      "SG 11.0 tries (3 tests)\n",
      "TZ 11.2 tries (5 tests)\n",
      "JM 11.8 tries (5 tests)\n",
      "ZA 11.8 tries (6 tests)\n",
      "MY 14.0 tries (2 tests)\n",
      "PH 17.5 tries (2 tests)\n",
      "Average number of tries: 7.186035631035631\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier removing words that appear in less than 25% of countries\n",
    "rf_model = RandomForestClassifier(random_state=3, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(rf_model)\n",
    "average_tries(rf_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.30\n",
      "KE (9): US 0.557, GB 0.389, CA 0.034, AU 0.017, IE 0.002, IN 0.000, HK 0.000, BD 0.000, KE 0.000, NZ 0.000, NG 0.000, MY 0.000, LK 0.000, GH 0.000, ZA 0.000, SG 0.000, JM 0.000, PK 0.000, PH 0.000, TZ 0.000, \n",
      "NG (10): US 0.291, GB 0.270, CA 0.251, NZ 0.080, IN 0.037, AU 0.011, JM 0.010, MY 0.008, HK 0.008, NG 0.006, PH 0.006, IE 0.005, SG 0.004, BD 0.003, ZA 0.003, GH 0.002, KE 0.001, PK 0.001, LK 0.001, TZ 0.001, \n",
      "ZA (7): CA 0.279, US 0.258, AU 0.241, GB 0.127, NG 0.028, MY 0.020, ZA 0.012, NZ 0.012, JM 0.007, KE 0.005, LK 0.002, BD 0.002, HK 0.002, PH 0.001, TZ 0.001, IN 0.001, IE 0.001, SG 0.000, GH 0.000, PK 0.000, \n",
      "IE (3): GB 0.299, US 0.284, IE 0.203, AU 0.055, CA 0.053, TZ 0.019, IN 0.012, ZA 0.010, LK 0.008, PH 0.007, BD 0.007, JM 0.007, MY 0.006, HK 0.006, PK 0.006, KE 0.005, NG 0.005, NZ 0.003, GH 0.003, SG 0.003, \n",
      "HK (4): US 0.258, AU 0.254, GB 0.171, HK 0.154, MY 0.070, CA 0.039, ZA 0.036, NZ 0.006, PH 0.003, IE 0.002, NG 0.002, JM 0.001, IN 0.001, SG 0.001, LK 0.001, BD 0.001, KE 0.000, PK 0.000, TZ 0.000, GH 0.000, \n",
      "GB (1): GB 0.489, US 0.239, CA 0.106, AU 0.040, PH 0.015, IE 0.014, LK 0.013, IN 0.012, MY 0.009, NZ 0.008, PK 0.008, HK 0.007, TZ 0.006, NG 0.006, ZA 0.005, GH 0.005, KE 0.005, JM 0.004, BD 0.004, SG 0.003, \n",
      "AU (3): GB 0.417, US 0.228, AU 0.220, CA 0.104, ZA 0.008, PK 0.003, NG 0.003, IN 0.003, NZ 0.003, JM 0.003, MY 0.002, PH 0.001, BD 0.001, KE 0.001, HK 0.001, LK 0.001, IE 0.001, TZ 0.000, SG 0.000, GH 0.000, \n",
      "NZ (3): GB 0.493, US 0.212, NZ 0.127, AU 0.068, CA 0.026, ZA 0.023, IN 0.015, HK 0.007, GH 0.004, NG 0.004, KE 0.004, PH 0.003, IE 0.003, TZ 0.003, MY 0.002, BD 0.001, SG 0.001, JM 0.001, PK 0.001, LK 0.001, \n",
      "SG (17): JM 0.253, GB 0.161, CA 0.094, US 0.083, AU 0.073, HK 0.045, ZA 0.034, IN 0.034, IE 0.030, NG 0.024, LK 0.024, PH 0.020, MY 0.018, NZ 0.017, BD 0.017, TZ 0.017, SG 0.016, GH 0.014, KE 0.014, PK 0.013, \n",
      "BD (4): GB 0.668, CA 0.310, NZ 0.012, BD 0.005, US 0.002, PH 0.001, AU 0.001, IN 0.000, JM 0.000, GH 0.000, PK 0.000, MY 0.000, HK 0.000, LK 0.000, TZ 0.000, NG 0.000, ZA 0.000, IE 0.000, SG 0.000, KE 0.000, \n",
      "TZ (8): US 0.605, GB 0.351, CA 0.014, IE 0.008, HK 0.007, AU 0.004, PH 0.003, TZ 0.001, IN 0.001, BD 0.001, NZ 0.001, KE 0.001, ZA 0.001, NG 0.001, SG 0.001, JM 0.001, MY 0.000, PK 0.000, LK 0.000, GH 0.000, \n",
      "IN (4): GB 0.231, CA 0.187, US 0.130, IN 0.121, AU 0.118, JM 0.109, PK 0.028, BD 0.013, GH 0.011, TZ 0.009, NG 0.009, ZA 0.008, NZ 0.007, MY 0.006, LK 0.004, IE 0.004, HK 0.002, PH 0.001, SG 0.001, KE 0.000, \n",
      "JM (9): GB 0.316, AU 0.142, ZA 0.137, US 0.125, PK 0.045, NZ 0.044, GH 0.040, CA 0.033, JM 0.027, NG 0.023, HK 0.019, MY 0.014, LK 0.011, BD 0.007, SG 0.006, TZ 0.004, IN 0.004, PH 0.001, IE 0.001, KE 0.001, \n",
      "GH (3): GB 0.334, CA 0.332, GH 0.322, IN 0.006, US 0.003, ZA 0.001, HK 0.000, BD 0.000, TZ 0.000, IE 0.000, NG 0.000, KE 0.000, JM 0.000, PK 0.000, AU 0.000, PH 0.000, NZ 0.000, MY 0.000, LK 0.000, SG 0.000, \n",
      "US (1): US 0.443, GB 0.279, CA 0.094, AU 0.054, IE 0.048, IN 0.028, NZ 0.009, PH 0.007, JM 0.006, MY 0.005, HK 0.005, KE 0.004, NG 0.004, SG 0.003, PK 0.002, ZA 0.002, BD 0.002, LK 0.001, TZ 0.001, GH 0.001, \n",
      "PH (11): GB 0.701, CA 0.118, MY 0.069, GH 0.032, ZA 0.018, BD 0.015, LK 0.012, NG 0.007, US 0.006, AU 0.004, PH 0.003, NZ 0.003, PK 0.003, JM 0.002, TZ 0.002, KE 0.002, IN 0.002, HK 0.001, IE 0.001, SG 0.000, \n",
      "CA (1): CA 0.382, US 0.299, GB 0.212, AU 0.043, HK 0.020, ZA 0.011, IE 0.005, MY 0.004, TZ 0.004, BD 0.003, JM 0.003, LK 0.003, NG 0.002, PH 0.002, IN 0.002, KE 0.002, GH 0.001, PK 0.001, SG 0.001, NZ 0.001, \n",
      "LK (2): AU 0.334, LK 0.329, US 0.108, BD 0.096, GB 0.084, CA 0.021, NG 0.007, PH 0.005, MY 0.005, PK 0.005, TZ 0.002, GH 0.001, HK 0.001, JM 0.001, ZA 0.001, IE 0.001, IN 0.000, SG 0.000, KE 0.000, NZ 0.000, \n",
      "MY (10): US 0.527, GB 0.346, CA 0.047, IN 0.041, AU 0.010, HK 0.007, NG 0.004, SG 0.003, KE 0.002, MY 0.002, BD 0.002, IE 0.002, NZ 0.002, ZA 0.001, JM 0.001, PK 0.001, PH 0.001, TZ 0.001, LK 0.000, GH 0.000, \n",
      "PK (1): PK 0.241, US 0.210, CA 0.202, IN 0.137, IE 0.122, GB 0.024, AU 0.018, MY 0.011, NZ 0.008, HK 0.004, KE 0.004, ZA 0.004, PH 0.004, GH 0.003, LK 0.002, NG 0.002, BD 0.001, JM 0.001, TZ 0.001, SG 0.001, \n",
      "\n",
      "GB 1.6 tries (30 tests)\n",
      "US 2.2 tries (28 tests)\n",
      "CA 2.3 tries (14 tests)\n",
      "LK 2.5 tries (2 tests)\n",
      "AU 3.3 tries (13 tests)\n",
      "IE 4.7 tries (11 tests)\n",
      "NZ 5.6 tries (7 tests)\n",
      "JM 5.8 tries (5 tests)\n",
      "NG 5.9 tries (7 tests)\n",
      "BD 6.5 tries (2 tests)\n",
      "ZA 7.0 tries (6 tests)\n",
      "HK 7.0 tries (4 tests)\n",
      "PK 7.1 tries (8 tests)\n",
      "GH 9.7 tries (3 tests)\n",
      "TZ 9.8 tries (5 tests)\n",
      "IN 10.1 tries (9 tests)\n",
      "KE 11.5 tries (4 tests)\n",
      "PH 12.0 tries (2 tests)\n",
      "SG 13.3 tries (3 tests)\n",
      "MY 13.5 tries (2 tests)\n",
      "Average number of tries: 7.071768093018093\n"
     ]
    }
   ],
   "source": [
    "# Multi-layer Perceptron Model removing words that appear in less than 25% of countries\n",
    "mlp_model = MLPClassifier(random_state=3)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(mlp_model)\n",
    "average_tries(mlp_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models with weight balancing and normal sampling for the 50% dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_50, y, test_size=0.1, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26\n",
      "KE (9): US 0.461, CA 0.229, JM 0.141, GB 0.074, GH 0.049, LK 0.015, NG 0.010, AU 0.006, KE 0.004, NZ 0.003, MY 0.002, IE 0.002, BD 0.001, PK 0.001, IN 0.000, SG 0.000, PH 0.000, ZA 0.000, TZ 0.000, HK 0.000, \n",
      "NG (3): GB 0.175, CA 0.164, NG 0.143, NZ 0.124, KE 0.107, US 0.092, GH 0.076, TZ 0.060, IE 0.012, LK 0.009, AU 0.007, JM 0.006, IN 0.006, HK 0.004, PH 0.004, ZA 0.004, SG 0.003, MY 0.002, BD 0.001, PK 0.001, \n",
      "ZA (8): NG 0.202, CA 0.178, GB 0.169, IE 0.146, NZ 0.122, US 0.076, AU 0.029, ZA 0.023, LK 0.012, MY 0.012, BD 0.011, TZ 0.006, IN 0.004, SG 0.004, GH 0.003, HK 0.002, KE 0.001, PK 0.001, PH 0.000, JM 0.000, \n",
      "IE (2): GB 0.236, IE 0.171, TZ 0.098, PH 0.094, LK 0.094, US 0.085, ZA 0.079, IN 0.067, CA 0.021, AU 0.021, JM 0.006, BD 0.004, NZ 0.004, NG 0.004, PK 0.003, GH 0.003, KE 0.003, MY 0.002, SG 0.002, HK 0.002, \n",
      "HK (1): HK 0.375, AU 0.194, GB 0.160, JM 0.076, MY 0.065, SG 0.030, CA 0.027, US 0.022, IE 0.021, NZ 0.020, PH 0.004, LK 0.002, KE 0.002, NG 0.002, IN 0.001, TZ 0.000, PK 0.000, BD 0.000, ZA 0.000, GH 0.000, \n",
      "GB (1): GB 0.382, CA 0.134, US 0.129, IE 0.074, AU 0.056, LK 0.047, NZ 0.040, PH 0.026, ZA 0.026, IN 0.025, MY 0.009, GH 0.008, HK 0.007, PK 0.007, NG 0.007, TZ 0.006, JM 0.005, KE 0.005, BD 0.005, SG 0.004, \n",
      "AU (1): AU 0.281, GB 0.236, JM 0.181, US 0.133, CA 0.029, SG 0.026, ZA 0.022, NG 0.018, IN 0.018, IE 0.014, MY 0.012, NZ 0.009, LK 0.007, PH 0.005, KE 0.003, PK 0.002, BD 0.001, HK 0.001, TZ 0.001, GH 0.000, \n",
      "NZ (1): NZ 0.323, GB 0.191, IN 0.154, AU 0.108, PH 0.066, US 0.057, PK 0.028, ZA 0.018, IE 0.012, CA 0.009, TZ 0.005, GH 0.005, KE 0.005, MY 0.004, HK 0.004, LK 0.004, JM 0.003, BD 0.002, NG 0.001, SG 0.001, \n",
      "SG (7): GB 0.290, PH 0.194, CA 0.115, US 0.092, IN 0.048, AU 0.033, SG 0.030, LK 0.029, BD 0.026, ZA 0.023, NZ 0.019, IE 0.018, GH 0.015, JM 0.014, NG 0.013, MY 0.012, TZ 0.009, HK 0.008, PK 0.007, KE 0.004, \n",
      "BD (8): GB 0.992, PH 0.004, CA 0.002, IN 0.001, IE 0.000, US 0.000, GH 0.000, BD 0.000, AU 0.000, NZ 0.000, LK 0.000, TZ 0.000, ZA 0.000, JM 0.000, SG 0.000, KE 0.000, MY 0.000, HK 0.000, NG 0.000, PK 0.000, \n",
      "TZ (19): GB 0.305, US 0.259, IN 0.121, AU 0.112, KE 0.038, ZA 0.033, JM 0.021, NZ 0.020, CA 0.019, IE 0.014, GH 0.013, NG 0.013, PH 0.012, HK 0.006, LK 0.006, MY 0.004, SG 0.002, BD 0.001, TZ 0.001, PK 0.000, \n",
      "IN (3): GB 0.229, LK 0.120, IN 0.113, PH 0.111, IE 0.095, JM 0.093, CA 0.081, US 0.053, KE 0.030, TZ 0.023, NZ 0.013, ZA 0.013, AU 0.011, GH 0.006, MY 0.004, NG 0.002, BD 0.002, HK 0.001, PK 0.000, SG 0.000, \n",
      "JM (9): AU 0.270, ZA 0.176, GB 0.170, KE 0.103, US 0.065, LK 0.062, IE 0.046, NZ 0.027, JM 0.018, NG 0.015, PK 0.014, CA 0.009, BD 0.008, GH 0.005, TZ 0.004, PH 0.002, SG 0.002, IN 0.001, HK 0.001, MY 0.000, \n",
      "GH (1): GH 0.269, GB 0.209, KE 0.164, JM 0.126, US 0.086, IN 0.065, IE 0.021, ZA 0.020, SG 0.019, TZ 0.013, LK 0.003, HK 0.002, CA 0.001, PK 0.001, NG 0.001, AU 0.000, MY 0.000, NZ 0.000, PH 0.000, BD 0.000, \n",
      "US (1): US 0.332, GB 0.250, AU 0.090, CA 0.077, LK 0.045, IE 0.045, NZ 0.040, IN 0.025, JM 0.019, SG 0.016, ZA 0.011, MY 0.010, BD 0.008, NG 0.006, HK 0.006, GH 0.006, PH 0.005, PK 0.004, KE 0.003, TZ 0.001, \n",
      "PH (16): GB 0.667, CA 0.319, KE 0.003, SG 0.003, NG 0.003, GH 0.002, IN 0.001, TZ 0.001, BD 0.000, MY 0.000, LK 0.000, NZ 0.000, HK 0.000, AU 0.000, JM 0.000, PH 0.000, ZA 0.000, US 0.000, IE 0.000, PK 0.000, \n",
      "CA (3): GB 0.243, US 0.182, CA 0.177, PH 0.110, IN 0.075, LK 0.068, SG 0.033, IE 0.030, AU 0.020, BD 0.016, JM 0.011, MY 0.010, GH 0.005, KE 0.005, ZA 0.005, NZ 0.003, HK 0.003, NG 0.002, TZ 0.001, PK 0.001, \n",
      "LK (14): US 0.564, GB 0.117, AU 0.092, NG 0.060, PH 0.039, GH 0.027, IE 0.022, MY 0.020, TZ 0.020, NZ 0.008, IN 0.007, JM 0.006, SG 0.004, LK 0.004, ZA 0.004, CA 0.004, HK 0.001, PK 0.001, KE 0.000, BD 0.000, \n",
      "MY (7): US 0.640, CA 0.121, GB 0.106, SG 0.076, IN 0.017, AU 0.014, MY 0.006, LK 0.005, IE 0.003, KE 0.002, PH 0.002, PK 0.002, NG 0.002, BD 0.001, JM 0.001, ZA 0.001, TZ 0.000, GH 0.000, NZ 0.000, HK 0.000, \n",
      "PK (19): US 0.190, IN 0.163, IE 0.160, NG 0.127, AU 0.121, LK 0.108, GB 0.034, CA 0.028, NZ 0.025, MY 0.016, GH 0.011, PH 0.005, HK 0.004, JM 0.002, SG 0.002, BD 0.002, TZ 0.001, ZA 0.001, PK 0.001, KE 0.000, \n",
      "\n",
      "GB 2.9 tries (30 tests)\n",
      "US 3.0 tries (28 tests)\n",
      "NZ 3.4 tries (7 tests)\n",
      "IE 3.6 tries (11 tests)\n",
      "CA 3.8 tries (14 tests)\n",
      "AU 4.0 tries (13 tests)\n",
      "BD 7.0 tries (2 tests)\n",
      "MY 7.5 tries (2 tests)\n",
      "GH 7.7 tries (3 tests)\n",
      "HK 7.8 tries (4 tests)\n",
      "ZA 8.7 tries (6 tests)\n",
      "IN 8.8 tries (9 tests)\n",
      "SG 9.7 tries (3 tests)\n",
      "PK 10.6 tries (8 tests)\n",
      "JM 11.6 tries (5 tests)\n",
      "LK 12.5 tries (2 tests)\n",
      "NG 12.7 tries (7 tests)\n",
      "KE 12.8 tries (4 tests)\n",
      "TZ 13.4 tries (5 tests)\n",
      "PH 16.5 tries (2 tests)\n",
      "Average number of tries: 8.393266594516595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model removing words that appear in less than 50% of countries\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=500, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(model)\n",
    "average_tries(model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.31\n",
      "KE (6): GB 0.230, US 0.180, AU 0.077, CA 0.065, NZ 0.048, KE 0.040, ZA 0.040, LK 0.040, GH 0.038, IE 0.037, PK 0.035, BD 0.033, IN 0.033, JM 0.033, NG 0.022, MY 0.018, PH 0.013, TZ 0.010, SG 0.007, HK 0.003, \n",
      "NG (14): US 0.219, GB 0.193, IN 0.069, CA 0.064, AU 0.056, NZ 0.046, KE 0.040, HK 0.034, IE 0.033, MY 0.031, JM 0.027, GH 0.027, LK 0.027, NG 0.026, PK 0.026, ZA 0.024, PH 0.020, SG 0.014, BD 0.014, TZ 0.010, \n",
      "ZA (13): US 0.220, GB 0.208, AU 0.088, CA 0.060, IN 0.047, IE 0.042, NG 0.038, NZ 0.032, TZ 0.032, PH 0.030, KE 0.025, GH 0.025, ZA 0.023, HK 0.022, MY 0.022, SG 0.020, BD 0.020, PK 0.020, LK 0.015, JM 0.012, \n",
      "IE (1): IE 0.195, GB 0.195, US 0.168, AU 0.083, CA 0.043, IN 0.040, NZ 0.035, JM 0.025, LK 0.025, SG 0.024, TZ 0.023, BD 0.020, GH 0.019, NG 0.018, ZA 0.016, PH 0.016, PK 0.015, MY 0.014, KE 0.013, HK 0.013, \n",
      "HK (3): US 0.178, GB 0.170, HK 0.088, AU 0.068, CA 0.065, IN 0.053, IE 0.050, SG 0.045, NZ 0.040, LK 0.035, KE 0.028, BD 0.025, JM 0.023, TZ 0.022, GH 0.022, MY 0.022, PH 0.020, NG 0.018, PK 0.015, ZA 0.015, \n",
      "GB (2): US 0.256, GB 0.254, AU 0.073, CA 0.065, IN 0.049, IE 0.045, NZ 0.034, PH 0.023, LK 0.022, JM 0.019, PK 0.019, SG 0.019, MY 0.018, NG 0.016, ZA 0.016, GH 0.016, KE 0.015, BD 0.014, HK 0.014, TZ 0.012, \n",
      "AU (3): GB 0.196, US 0.181, AU 0.172, CA 0.062, NZ 0.048, IE 0.044, IN 0.038, LK 0.031, BD 0.028, SG 0.028, PH 0.025, NG 0.022, PK 0.021, KE 0.020, GH 0.019, MY 0.018, JM 0.017, ZA 0.016, HK 0.009, TZ 0.005, \n",
      "NZ (1): NZ 0.207, GB 0.189, US 0.169, IE 0.076, AU 0.076, IN 0.057, CA 0.044, MY 0.021, KE 0.020, JM 0.016, LK 0.016, NG 0.016, GH 0.016, SG 0.014, PK 0.013, ZA 0.011, TZ 0.011, PH 0.010, HK 0.010, BD 0.009, \n",
      "SG (6): US 0.310, GB 0.273, CA 0.077, AU 0.070, IE 0.053, SG 0.033, IN 0.030, JM 0.027, TZ 0.023, NG 0.017, HK 0.017, PK 0.017, NZ 0.013, PH 0.010, LK 0.010, ZA 0.007, MY 0.007, BD 0.003, GH 0.003, KE 0.000, \n",
      "BD (9): GB 0.245, US 0.150, NZ 0.075, PH 0.070, CA 0.070, AU 0.065, IN 0.045, IE 0.040, BD 0.035, MY 0.030, NG 0.025, SG 0.025, JM 0.025, PK 0.025, KE 0.020, LK 0.020, ZA 0.015, GH 0.010, HK 0.005, TZ 0.005, \n",
      "TZ (10): GB 0.164, US 0.144, ZA 0.110, GH 0.074, AU 0.068, CA 0.048, KE 0.046, IN 0.044, LK 0.044, TZ 0.042, NZ 0.034, IE 0.032, BD 0.032, NG 0.026, PH 0.026, JM 0.022, MY 0.020, SG 0.014, PK 0.010, HK 0.000, \n",
      "IN (3): US 0.200, GB 0.197, IN 0.102, CA 0.052, AU 0.052, LK 0.048, NZ 0.042, IE 0.034, BD 0.032, JM 0.029, KE 0.027, MY 0.027, PH 0.026, GH 0.026, SG 0.026, PK 0.020, ZA 0.018, NG 0.016, HK 0.014, TZ 0.013, \n",
      "JM (7): US 0.214, GB 0.204, HK 0.060, AU 0.058, IE 0.056, NZ 0.048, JM 0.046, ZA 0.040, CA 0.038, IN 0.030, LK 0.030, KE 0.028, GH 0.028, MY 0.024, SG 0.022, PH 0.022, NG 0.018, BD 0.012, PK 0.012, TZ 0.010, \n",
      "GH (5): US 0.240, GB 0.180, AU 0.077, ZA 0.063, GH 0.050, IE 0.047, IN 0.047, KE 0.043, CA 0.040, NZ 0.037, LK 0.037, PH 0.023, SG 0.023, MY 0.023, TZ 0.017, JM 0.017, NG 0.013, HK 0.013, PK 0.007, BD 0.003, \n",
      "US (1): US 0.248, GB 0.213, CA 0.073, AU 0.070, IN 0.046, IE 0.041, NZ 0.033, PH 0.029, PK 0.027, TZ 0.026, BD 0.024, JM 0.023, MY 0.022, LK 0.021, GH 0.021, NG 0.019, KE 0.017, ZA 0.017, SG 0.016, HK 0.014, \n",
      "PH (10): GB 0.295, US 0.135, AU 0.095, IN 0.075, NZ 0.045, SG 0.045, CA 0.045, TZ 0.030, IE 0.030, PH 0.030, LK 0.030, MY 0.030, NG 0.025, KE 0.020, BD 0.020, GH 0.020, HK 0.015, ZA 0.005, JM 0.005, PK 0.005, \n",
      "CA (3): GB 0.217, US 0.216, CA 0.106, AU 0.081, NZ 0.038, IN 0.037, IE 0.033, LK 0.029, JM 0.027, GH 0.027, SG 0.026, PH 0.022, NG 0.021, MY 0.020, KE 0.019, ZA 0.019, HK 0.017, PK 0.016, BD 0.015, TZ 0.013, \n",
      "LK (9): US 0.260, GB 0.215, CA 0.090, PH 0.045, PK 0.045, IE 0.040, NZ 0.040, AU 0.035, LK 0.035, MY 0.030, IN 0.025, JM 0.025, GH 0.025, ZA 0.020, BD 0.020, NG 0.015, SG 0.015, TZ 0.015, HK 0.005, KE 0.000, \n",
      "MY (14): US 0.250, GB 0.220, CA 0.080, AU 0.075, IE 0.055, NG 0.050, NZ 0.040, PH 0.040, ZA 0.030, PK 0.030, BD 0.025, IN 0.025, LK 0.020, MY 0.020, HK 0.015, SG 0.010, JM 0.010, TZ 0.005, KE 0.000, GH 0.000, \n",
      "PK (3): US 0.202, GB 0.193, PK 0.104, CA 0.070, IN 0.060, AU 0.055, NZ 0.049, NG 0.033, IE 0.033, KE 0.028, PH 0.024, BD 0.023, MY 0.023, GH 0.020, LK 0.019, HK 0.017, JM 0.016, SG 0.015, TZ 0.011, ZA 0.007, \n",
      "\n",
      "US 1.4 tries (28 tests)\n",
      "GB 1.5 tries (30 tests)\n",
      "AU 2.5 tries (13 tests)\n",
      "IE 4.3 tries (11 tests)\n",
      "NZ 4.4 tries (7 tests)\n",
      "CA 4.5 tries (14 tests)\n",
      "IN 6.4 tries (9 tests)\n",
      "JM 8.0 tries (5 tests)\n",
      "PK 8.0 tries (8 tests)\n",
      "HK 8.5 tries (4 tests)\n",
      "KE 9.0 tries (4 tests)\n",
      "BD 9.5 tries (2 tests)\n",
      "TZ 9.6 tries (5 tests)\n",
      "SG 10.0 tries (3 tests)\n",
      "GH 10.3 tries (3 tests)\n",
      "LK 10.5 tries (2 tests)\n",
      "PH 12.0 tries (2 tests)\n",
      "NG 12.6 tries (7 tests)\n",
      "ZA 13.8 tries (6 tests)\n",
      "MY 14.5 tries (2 tests)\n",
      "Average number of tries: 8.070364080364081\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier removing words that appear in less than 50% of countries\n",
    "rf_model = RandomForestClassifier(random_state=3, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")  \n",
    "\n",
    "probability_breakdown(rf_model)\n",
    "average_tries(rf_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28\n",
      "KE (17): US 0.682, GB 0.217, NZ 0.063, CA 0.019, IE 0.005, PH 0.004, ZA 0.003, LK 0.003, PK 0.001, MY 0.001, JM 0.000, NG 0.000, BD 0.000, AU 0.000, IN 0.000, GH 0.000, KE 0.000, HK 0.000, TZ 0.000, SG 0.000, \n",
      "NG (19): CA 0.339, GB 0.247, US 0.208, AU 0.099, IN 0.025, NZ 0.019, IE 0.013, JM 0.013, TZ 0.005, MY 0.005, GH 0.005, HK 0.005, SG 0.004, BD 0.003, PH 0.002, ZA 0.002, PK 0.002, KE 0.002, NG 0.001, LK 0.001, \n",
      "ZA (14): CA 0.278, US 0.220, GB 0.141, AU 0.123, IN 0.046, NG 0.044, MY 0.038, TZ 0.038, IE 0.018, SG 0.014, PK 0.010, KE 0.009, BD 0.005, ZA 0.004, NZ 0.004, HK 0.002, JM 0.002, PH 0.002, GH 0.001, LK 0.001, \n",
      "IE (1): IE 0.339, US 0.155, GB 0.144, CA 0.130, LK 0.060, ZA 0.043, NZ 0.032, AU 0.022, BD 0.021, MY 0.009, TZ 0.007, JM 0.006, NG 0.005, SG 0.005, IN 0.005, KE 0.005, PK 0.004, HK 0.003, GH 0.002, PH 0.002, \n",
      "HK (12): AU 0.454, NZ 0.257, US 0.204, GB 0.032, CA 0.009, LK 0.008, SG 0.007, BD 0.006, IE 0.006, PH 0.003, IN 0.002, HK 0.002, PK 0.002, MY 0.002, GH 0.002, KE 0.002, NG 0.002, JM 0.001, TZ 0.001, ZA 0.000, \n",
      "GB (1): GB 0.434, US 0.212, AU 0.092, CA 0.082, IE 0.043, IN 0.035, NZ 0.014, PH 0.011, TZ 0.009, LK 0.009, NG 0.008, MY 0.008, ZA 0.007, PK 0.007, GH 0.007, BD 0.006, SG 0.005, KE 0.005, JM 0.003, HK 0.003, \n",
      "AU (2): GB 0.357, AU 0.319, US 0.171, CA 0.087, IE 0.017, IN 0.011, PK 0.007, PH 0.006, JM 0.004, NZ 0.004, MY 0.003, BD 0.003, LK 0.002, ZA 0.002, HK 0.002, NG 0.001, GH 0.001, KE 0.001, SG 0.001, TZ 0.000, \n",
      "NZ (2): GB 0.396, NZ 0.165, US 0.145, IN 0.097, AU 0.074, CA 0.036, ZA 0.036, PH 0.021, TZ 0.006, IE 0.004, PK 0.004, BD 0.003, HK 0.003, GH 0.002, KE 0.002, MY 0.001, LK 0.001, SG 0.001, NG 0.001, JM 0.001, \n",
      "SG (7): JM 0.190, CA 0.158, GB 0.139, AU 0.098, IE 0.087, US 0.074, SG 0.032, TZ 0.030, NZ 0.020, ZA 0.020, IN 0.020, LK 0.019, BD 0.018, KE 0.017, MY 0.016, NG 0.015, GH 0.013, HK 0.012, PK 0.012, PH 0.009, \n",
      "BD (10): GB 0.562, CA 0.296, NZ 0.119, US 0.018, AU 0.002, IN 0.001, IE 0.001, PH 0.001, GH 0.000, BD 0.000, ZA 0.000, KE 0.000, PK 0.000, TZ 0.000, HK 0.000, MY 0.000, NG 0.000, SG 0.000, JM 0.000, LK 0.000, \n",
      "TZ (10): US 0.616, GB 0.175, IE 0.069, ZA 0.048, IN 0.048, NZ 0.012, PH 0.004, AU 0.004, MY 0.004, TZ 0.004, PK 0.004, CA 0.003, KE 0.003, GH 0.002, JM 0.002, HK 0.001, BD 0.001, SG 0.001, LK 0.001, NG 0.000, \n",
      "IN (4): GB 0.321, US 0.279, CA 0.166, IN 0.112, ZA 0.048, NZ 0.014, IE 0.012, LK 0.008, JM 0.008, TZ 0.007, MY 0.005, PK 0.005, PH 0.003, AU 0.003, BD 0.002, GH 0.002, NG 0.002, SG 0.001, HK 0.001, KE 0.000, \n",
      "JM (12): GB 0.312, NZ 0.296, ZA 0.098, PK 0.082, US 0.050, GH 0.037, AU 0.037, CA 0.018, IE 0.014, IN 0.014, MY 0.008, JM 0.008, KE 0.008, TZ 0.007, HK 0.006, NG 0.003, BD 0.001, SG 0.001, LK 0.001, PH 0.000, \n",
      "GH (2): GB 0.380, GH 0.190, IN 0.167, KE 0.156, SG 0.038, JM 0.030, PK 0.024, ZA 0.007, US 0.003, TZ 0.003, AU 0.002, CA 0.001, HK 0.000, IE 0.000, NZ 0.000, NG 0.000, PH 0.000, MY 0.000, LK 0.000, BD 0.000, \n",
      "US (1): US 0.467, GB 0.240, CA 0.098, IE 0.064, AU 0.049, IN 0.018, NZ 0.011, JM 0.009, LK 0.006, MY 0.005, GH 0.005, SG 0.005, HK 0.004, PK 0.004, PH 0.004, ZA 0.004, NG 0.002, TZ 0.002, BD 0.002, KE 0.001, \n",
      "PH (17): GB 0.967, KE 0.020, CA 0.009, NZ 0.003, SG 0.000, AU 0.000, US 0.000, IN 0.000, GH 0.000, BD 0.000, MY 0.000, TZ 0.000, ZA 0.000, HK 0.000, NG 0.000, PK 0.000, PH 0.000, LK 0.000, JM 0.000, IE 0.000, \n",
      "CA (2): US 0.355, CA 0.345, GB 0.213, AU 0.020, IE 0.016, LK 0.007, IN 0.006, JM 0.005, ZA 0.004, KE 0.004, NZ 0.004, SG 0.004, PK 0.004, PH 0.002, MY 0.002, BD 0.002, HK 0.002, TZ 0.002, NG 0.002, GH 0.001, \n",
      "LK (3): AU 0.422, US 0.344, LK 0.126, GH 0.021, GB 0.017, NZ 0.015, MY 0.013, PK 0.010, IE 0.010, NG 0.007, TZ 0.005, CA 0.002, IN 0.002, HK 0.002, PH 0.001, KE 0.001, BD 0.001, SG 0.000, ZA 0.000, JM 0.000, \n",
      "MY (6): US 0.742, GB 0.052, AU 0.043, CA 0.041, PK 0.027, MY 0.019, SG 0.016, KE 0.012, HK 0.011, IN 0.010, NG 0.009, GH 0.005, LK 0.004, IE 0.003, PH 0.002, JM 0.002, BD 0.001, NZ 0.001, TZ 0.001, ZA 0.000, \n",
      "PK (3): IE 0.278, US 0.225, PK 0.131, IN 0.130, AU 0.081, GB 0.067, NZ 0.027, ZA 0.017, CA 0.011, NG 0.010, JM 0.005, LK 0.004, MY 0.003, HK 0.003, PH 0.003, BD 0.002, TZ 0.001, GH 0.001, SG 0.001, KE 0.001, \n",
      "\n",
      "GB 2.5 tries (30 tests)\n",
      "CA 2.6 tries (14 tests)\n",
      "US 3.0 tries (28 tests)\n",
      "NZ 3.4 tries (7 tests)\n",
      "AU 3.5 tries (13 tests)\n",
      "IE 4.3 tries (11 tests)\n",
      "GH 6.7 tries (3 tests)\n",
      "HK 7.0 tries (4 tests)\n",
      "PK 7.2 tries (8 tests)\n",
      "MY 8.0 tries (2 tests)\n",
      "SG 8.7 tries (3 tests)\n",
      "BD 9.0 tries (2 tests)\n",
      "IN 10.0 tries (9 tests)\n",
      "JM 10.8 tries (5 tests)\n",
      "TZ 11.0 tries (5 tests)\n",
      "LK 11.0 tries (2 tests)\n",
      "ZA 11.2 tries (6 tests)\n",
      "NG 11.4 tries (7 tests)\n",
      "KE 15.0 tries (4 tests)\n",
      "PH 16.0 tries (2 tests)\n",
      "Average number of tries: 8.114332334332335\n"
     ]
    }
   ],
   "source": [
    "# Multi-layer Perceptron Model removing words that appear in less than 50% of countries\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_50, y, test_size=0.1, random_state=3)\n",
    "mlp_model = MLPClassifier(random_state=3)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "probability_breakdown(mlp_model)\n",
    "average_tries(mlp_model,X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
